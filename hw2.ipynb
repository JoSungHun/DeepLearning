{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoSungHun/Deeplearning/blob/master/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6vFXgflvZ6Ga",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Assignment"
      ]
    },
    {
      "metadata": {
        "id": "UevAU0CYVhiu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Due date: **2018/04/09 00:00** (will not accept late submission)\n",
        "* Submittion format: notebook file which can be executed in Colab environment\n",
        "\n",
        "* We want to build a multi-class classification model using Reuters dataset."
      ]
    },
    {
      "metadata": {
        "id": "KxrhNNhWbfHD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> ### Loading and preprocessing data"
      ]
    },
    {
      "metadata": {
        "id": "8E-2WLHgcAit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import reuters\n",
        "\n",
        "# Like IMDB, the argument num_words restricts the data to \n",
        "# the 10,000 most frequently occurring words \n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yrjD9AerdW4X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NmLRa5GNeSOi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UzoKfo9PUArN",
        "colab_type": "code",
        "outputId": "ca2f2442-4fd2-4df3-f704-8f89127e5f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(x_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8982"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "metadata": {
        "id": "yWfgUJiPeicE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> ### Building the network"
      ]
    },
    {
      "metadata": {
        "id": "Edwc4GbneqKA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(46, activation='softmax'))\n",
        "  model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j0TUwiHLgI_-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> ### Validation"
      ]
    },
    {
      "metadata": {
        "id": "KFe4FOkfgUII",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* We employ *k-fold cross validation* for monitoring the performance of trained model.\n",
        "* Write a code in the below to perform *10-fold cross validation*.\n",
        "* **For each fold, save a model at every epoch in your Google Drive.**"
      ]
    },
    {
      "metadata": {
        "id": "ZKOrz-PXN1nf",
        "colab_type": "code",
        "outputId": "b7521264-97f0-48e1-e2d0-e9e01394079c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive#구글드라이브 마운트 및 파일경로 설정\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "drive.mount('/content/gdrive')\n",
        "# filepath = '/content/gdrive/My Drive/hw2_callbacks/hw2_model.(폴드).{에폭}.hdf5'\n",
        "# for 문을 통해 경로 설정을 하고 싶었지만 이상하게 오류가 뜨며 안되서 스태틱하게 배열로 저장\n",
        "filepath = ['/content/gdrive/My Drive/hw2_callbacks/hw2_model.0.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.1.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.2.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.3.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.4.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.5.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.6.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.7.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.8.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.9.{epoch:02d}.hdf5']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bri7RrUs3Nf5",
        "colab_type": "code",
        "outputId": "f63e9cce-6000-408b-e061-56a66dfaf91b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "%cd hw2_callbacks"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/hw2_callbacks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jPWjg2s4WYfm",
        "colab_type": "code",
        "outputId": "58c0bb22-f28b-447b-d99c-923fb2476ca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36377
        }
      },
      "cell_type": "code",
      "source": [
        "# write a code for 10-fold cross validation here\n",
        "import numpy as np\n",
        "k = 10\n",
        "num_val_samples = len(x_train) // k\n",
        "num_epochs = 100\n",
        "all_train_acc = []\n",
        "all_val_acc = []\n",
        "for i in range(k):\n",
        "  print('처리중인 폴드 #', i)\n",
        "  \n",
        "  modelckpt = ModelCheckpoint(filepath=filepath[i]) \n",
        "  # \n",
        "  val_data = x_train[i * num_val_samples: (i+1) * num_val_samples]\n",
        "  val_labels = one_hot_train_labels[i * num_val_samples: (i+1) * num_val_samples]\n",
        "  \n",
        "  partial_train_data = np.concatenate([x_train[:i * num_val_samples], x_train[(i + 1) * num_val_samples:]], axis=0)\n",
        "  partial_train_labels = np.concatenate([one_hot_train_labels[:i * num_val_samples], one_hot_train_labels[(i + 1) * num_val_samples:]], axis=0)\n",
        "    \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data,\n",
        "                      partial_train_labels,\n",
        "                      epochs=num_epochs,\n",
        "                      batch_size=512,\n",
        "                      validation_data=(val_data, val_labels),\n",
        "                      callbacks=[modelckpt])\n",
        "  \n",
        "  train_acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  \n",
        "  all_train_acc.append(train_acc)\n",
        "  all_val_acc.append(val_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "처리중인 폴드 # 0\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 3s 344us/step - loss: 2.5658 - acc: 0.5391 - val_loss: 1.6928 - val_acc: 0.6470\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 1.3745 - acc: 0.7165 - val_loss: 1.3046 - val_acc: 0.7249\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 1.0312 - acc: 0.7785 - val_loss: 1.1599 - val_acc: 0.7528\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.8158 - acc: 0.8251 - val_loss: 1.0615 - val_acc: 0.7795\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.6507 - acc: 0.8637 - val_loss: 0.9983 - val_acc: 0.7817\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.5234 - acc: 0.8919 - val_loss: 0.9626 - val_acc: 0.8073\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.4192 - acc: 0.9111 - val_loss: 0.9404 - val_acc: 0.8085\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.3398 - acc: 0.9264 - val_loss: 0.9376 - val_acc: 0.8085\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.2818 - acc: 0.9380 - val_loss: 0.9524 - val_acc: 0.8051\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.2395 - acc: 0.9420 - val_loss: 0.9708 - val_acc: 0.8062\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.2067 - acc: 0.9498 - val_loss: 0.9830 - val_acc: 0.8073\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1837 - acc: 0.9509 - val_loss: 0.9934 - val_acc: 0.8007\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1633 - acc: 0.9521 - val_loss: 1.0079 - val_acc: 0.8018\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1500 - acc: 0.9544 - val_loss: 1.0439 - val_acc: 0.8129\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.1405 - acc: 0.9551 - val_loss: 1.0757 - val_acc: 0.7929\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1299 - acc: 0.9567 - val_loss: 1.0431 - val_acc: 0.7973\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.1237 - acc: 0.9578 - val_loss: 1.0534 - val_acc: 0.8018\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1180 - acc: 0.9573 - val_loss: 1.0740 - val_acc: 0.7951\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1139 - acc: 0.9582 - val_loss: 1.1109 - val_acc: 0.8018\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1100 - acc: 0.9587 - val_loss: 1.1030 - val_acc: 0.7996\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.1115 - acc: 0.9565 - val_loss: 1.1171 - val_acc: 0.8051\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.1016 - acc: 0.9581 - val_loss: 1.1314 - val_acc: 0.7996\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.1058 - acc: 0.9567 - val_loss: 1.1793 - val_acc: 0.7962\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.1042 - acc: 0.9588 - val_loss: 1.1609 - val_acc: 0.8007\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.1005 - acc: 0.9589 - val_loss: 1.1899 - val_acc: 0.7840\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0988 - acc: 0.9557 - val_loss: 1.1906 - val_acc: 0.7940\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0967 - acc: 0.9582 - val_loss: 1.2497 - val_acc: 0.7762\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0984 - acc: 0.9576 - val_loss: 1.2066 - val_acc: 0.8007\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0961 - acc: 0.9594 - val_loss: 1.2368 - val_acc: 0.7862\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0968 - acc: 0.9592 - val_loss: 1.2494 - val_acc: 0.7784\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0930 - acc: 0.9592 - val_loss: 1.2389 - val_acc: 0.7906\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0956 - acc: 0.9586 - val_loss: 1.2154 - val_acc: 0.7996\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0907 - acc: 0.9572 - val_loss: 1.2799 - val_acc: 0.7851\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0913 - acc: 0.9587 - val_loss: 1.2969 - val_acc: 0.7873\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0924 - acc: 0.9572 - val_loss: 1.2533 - val_acc: 0.7996\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0915 - acc: 0.9578 - val_loss: 1.2770 - val_acc: 0.7884\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0855 - acc: 0.9598 - val_loss: 1.3141 - val_acc: 0.7929\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0920 - acc: 0.9568 - val_loss: 1.3232 - val_acc: 0.7784\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0866 - acc: 0.9586 - val_loss: 1.3411 - val_acc: 0.7829\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0899 - acc: 0.9579 - val_loss: 1.3265 - val_acc: 0.7817\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0887 - acc: 0.9577 - val_loss: 1.3497 - val_acc: 0.7840\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0823 - acc: 0.9618 - val_loss: 1.3344 - val_acc: 0.7817\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0873 - acc: 0.9576 - val_loss: 1.3819 - val_acc: 0.7806\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0856 - acc: 0.9587 - val_loss: 1.3128 - val_acc: 0.7918\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0882 - acc: 0.9563 - val_loss: 1.3590 - val_acc: 0.7784\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0836 - acc: 0.9591 - val_loss: 1.3641 - val_acc: 0.7851\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0840 - acc: 0.9570 - val_loss: 1.3709 - val_acc: 0.7840\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0817 - acc: 0.9600 - val_loss: 1.4702 - val_acc: 0.7628\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0852 - acc: 0.9594 - val_loss: 1.3770 - val_acc: 0.7895\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0825 - acc: 0.9578 - val_loss: 1.4058 - val_acc: 0.7784\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0818 - acc: 0.9574 - val_loss: 1.4001 - val_acc: 0.7918\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0811 - acc: 0.9584 - val_loss: 1.3758 - val_acc: 0.7884\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0791 - acc: 0.9589 - val_loss: 1.4053 - val_acc: 0.7829\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0813 - acc: 0.9592 - val_loss: 1.3911 - val_acc: 0.7851\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0781 - acc: 0.9595 - val_loss: 1.4664 - val_acc: 0.7784\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0796 - acc: 0.9584 - val_loss: 1.4402 - val_acc: 0.7706\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0796 - acc: 0.9587 - val_loss: 1.4712 - val_acc: 0.7717\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0773 - acc: 0.9582 - val_loss: 1.4384 - val_acc: 0.7840\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0789 - acc: 0.9579 - val_loss: 1.5241 - val_acc: 0.7617\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0774 - acc: 0.9572 - val_loss: 1.4894 - val_acc: 0.7661\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0766 - acc: 0.9583 - val_loss: 1.4435 - val_acc: 0.7784\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0759 - acc: 0.9566 - val_loss: 1.4826 - val_acc: 0.7706\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0771 - acc: 0.9582 - val_loss: 1.5336 - val_acc: 0.7606\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0774 - acc: 0.9561 - val_loss: 1.5130 - val_acc: 0.7728\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0768 - acc: 0.9566 - val_loss: 1.4905 - val_acc: 0.7784\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0735 - acc: 0.9593 - val_loss: 1.5106 - val_acc: 0.7695\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0735 - acc: 0.9599 - val_loss: 1.4693 - val_acc: 0.7806\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0744 - acc: 0.9592 - val_loss: 1.5204 - val_acc: 0.7673\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0722 - acc: 0.9607 - val_loss: 1.5526 - val_acc: 0.7684\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0737 - acc: 0.9592 - val_loss: 1.5394 - val_acc: 0.7628\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0726 - acc: 0.9579 - val_loss: 1.5683 - val_acc: 0.7695\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0724 - acc: 0.9598 - val_loss: 1.5831 - val_acc: 0.7639\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0726 - acc: 0.9583 - val_loss: 1.6030 - val_acc: 0.7550\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0735 - acc: 0.9581 - val_loss: 1.5811 - val_acc: 0.7695\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0709 - acc: 0.9568 - val_loss: 1.6060 - val_acc: 0.7673\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0723 - acc: 0.9571 - val_loss: 1.6056 - val_acc: 0.7584\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0713 - acc: 0.9595 - val_loss: 1.6137 - val_acc: 0.7606\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0713 - acc: 0.9591 - val_loss: 1.5545 - val_acc: 0.7762\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0716 - acc: 0.9591 - val_loss: 1.5919 - val_acc: 0.7628\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0703 - acc: 0.9582 - val_loss: 1.6344 - val_acc: 0.7595\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0711 - acc: 0.9577 - val_loss: 1.5918 - val_acc: 0.7706\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0689 - acc: 0.9599 - val_loss: 1.6180 - val_acc: 0.7561\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0713 - acc: 0.9578 - val_loss: 1.5944 - val_acc: 0.7628\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0688 - acc: 0.9584 - val_loss: 1.6581 - val_acc: 0.7661\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0702 - acc: 0.9582 - val_loss: 1.6047 - val_acc: 0.7650\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0691 - acc: 0.9597 - val_loss: 1.6760 - val_acc: 0.7461\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0683 - acc: 0.9595 - val_loss: 1.6929 - val_acc: 0.7483\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0685 - acc: 0.9570 - val_loss: 1.6613 - val_acc: 0.7572\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0697 - acc: 0.9572 - val_loss: 1.6372 - val_acc: 0.7673\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0672 - acc: 0.9607 - val_loss: 1.7048 - val_acc: 0.7528\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0684 - acc: 0.9582 - val_loss: 1.6891 - val_acc: 0.7584\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0661 - acc: 0.9598 - val_loss: 1.6560 - val_acc: 0.7650\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0670 - acc: 0.9584 - val_loss: 1.6782 - val_acc: 0.7561\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0658 - acc: 0.9587 - val_loss: 1.6917 - val_acc: 0.7528\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0664 - acc: 0.9588 - val_loss: 1.7392 - val_acc: 0.7528\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0658 - acc: 0.9602 - val_loss: 1.7106 - val_acc: 0.7539\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0656 - acc: 0.9593 - val_loss: 1.7321 - val_acc: 0.7528\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0673 - acc: 0.9578 - val_loss: 1.7288 - val_acc: 0.7561\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0650 - acc: 0.9599 - val_loss: 1.7199 - val_acc: 0.7561\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0654 - acc: 0.9599 - val_loss: 1.7289 - val_acc: 0.7561\n",
            "처리중인 폴드 # 1\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 3s 323us/step - loss: 2.5901 - acc: 0.5275 - val_loss: 1.7982 - val_acc: 0.6214\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 1.3976 - acc: 0.7074 - val_loss: 1.3797 - val_acc: 0.7127\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 1.0308 - acc: 0.7789 - val_loss: 1.2138 - val_acc: 0.7361\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.8088 - acc: 0.8321 - val_loss: 1.1482 - val_acc: 0.7528\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.6485 - acc: 0.8647 - val_loss: 1.0999 - val_acc: 0.7483\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.5195 - acc: 0.8923 - val_loss: 1.0181 - val_acc: 0.7728\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.4199 - acc: 0.9144 - val_loss: 0.9724 - val_acc: 0.7973\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.3430 - acc: 0.9306 - val_loss: 0.9849 - val_acc: 0.7962\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.2851 - acc: 0.9390 - val_loss: 1.0173 - val_acc: 0.7929\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.2411 - acc: 0.9438 - val_loss: 0.9482 - val_acc: 0.8029\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.2104 - acc: 0.9489 - val_loss: 0.9779 - val_acc: 0.7951\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.1834 - acc: 0.9526 - val_loss: 0.9864 - val_acc: 0.8051\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1635 - acc: 0.9537 - val_loss: 1.0312 - val_acc: 0.7962\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.1574 - acc: 0.9562 - val_loss: 1.0508 - val_acc: 0.7895\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.1373 - acc: 0.9562 - val_loss: 1.0749 - val_acc: 0.7951\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1338 - acc: 0.9571 - val_loss: 1.0991 - val_acc: 0.7906\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1234 - acc: 0.9571 - val_loss: 1.0912 - val_acc: 0.7962\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.1198 - acc: 0.9565 - val_loss: 1.1519 - val_acc: 0.7895\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1155 - acc: 0.9579 - val_loss: 1.1052 - val_acc: 0.7951\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1093 - acc: 0.9577 - val_loss: 1.1718 - val_acc: 0.7873\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.1106 - acc: 0.9572 - val_loss: 1.1086 - val_acc: 0.8073\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1066 - acc: 0.9581 - val_loss: 1.1907 - val_acc: 0.7918\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1052 - acc: 0.9574 - val_loss: 1.1620 - val_acc: 0.7951\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1034 - acc: 0.9573 - val_loss: 1.1992 - val_acc: 0.7895\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0994 - acc: 0.9594 - val_loss: 1.1847 - val_acc: 0.7973\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1004 - acc: 0.9588 - val_loss: 1.2220 - val_acc: 0.7873\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0963 - acc: 0.9586 - val_loss: 1.2543 - val_acc: 0.7895\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0977 - acc: 0.9583 - val_loss: 1.2143 - val_acc: 0.7895\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0981 - acc: 0.9579 - val_loss: 1.2603 - val_acc: 0.7873\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0947 - acc: 0.9578 - val_loss: 1.2485 - val_acc: 0.7817\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0940 - acc: 0.9579 - val_loss: 1.3463 - val_acc: 0.7784\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0978 - acc: 0.9563 - val_loss: 1.2997 - val_acc: 0.7862\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 71us/step - loss: 0.0927 - acc: 0.9584 - val_loss: 1.2325 - val_acc: 0.7973\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0915 - acc: 0.9592 - val_loss: 1.3089 - val_acc: 0.7940\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0906 - acc: 0.9591 - val_loss: 1.3397 - val_acc: 0.7840\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0927 - acc: 0.9566 - val_loss: 1.3343 - val_acc: 0.7762\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0922 - acc: 0.9584 - val_loss: 1.3330 - val_acc: 0.7806\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0891 - acc: 0.9587 - val_loss: 1.3577 - val_acc: 0.7840\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0917 - acc: 0.9591 - val_loss: 1.3076 - val_acc: 0.7795\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0882 - acc: 0.9589 - val_loss: 1.3964 - val_acc: 0.7762\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0872 - acc: 0.9572 - val_loss: 1.3449 - val_acc: 0.7762\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0899 - acc: 0.9567 - val_loss: 1.3956 - val_acc: 0.7717\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0884 - acc: 0.9584 - val_loss: 1.3301 - val_acc: 0.7862\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0863 - acc: 0.9591 - val_loss: 1.4113 - val_acc: 0.7817\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0878 - acc: 0.9576 - val_loss: 1.3516 - val_acc: 0.7829\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0846 - acc: 0.9595 - val_loss: 1.3485 - val_acc: 0.7862\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0871 - acc: 0.9563 - val_loss: 1.3779 - val_acc: 0.7728\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0814 - acc: 0.9615 - val_loss: 1.3641 - val_acc: 0.7840\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0829 - acc: 0.9593 - val_loss: 1.3927 - val_acc: 0.7829\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0860 - acc: 0.9565 - val_loss: 1.3613 - val_acc: 0.7784\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0842 - acc: 0.9566 - val_loss: 1.4450 - val_acc: 0.7717\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0819 - acc: 0.9577 - val_loss: 1.4090 - val_acc: 0.7784\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0808 - acc: 0.9600 - val_loss: 1.4109 - val_acc: 0.7650\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0810 - acc: 0.9581 - val_loss: 1.4474 - val_acc: 0.7751\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0818 - acc: 0.9586 - val_loss: 1.3822 - val_acc: 0.7795\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0840 - acc: 0.9592 - val_loss: 1.4411 - val_acc: 0.7773\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0795 - acc: 0.9582 - val_loss: 1.4582 - val_acc: 0.7717\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0788 - acc: 0.9589 - val_loss: 1.4299 - val_acc: 0.7762\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0814 - acc: 0.9573 - val_loss: 1.4065 - val_acc: 0.7795\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0772 - acc: 0.9589 - val_loss: 1.4663 - val_acc: 0.7695\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0780 - acc: 0.9594 - val_loss: 1.4528 - val_acc: 0.7728\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0758 - acc: 0.9597 - val_loss: 1.4742 - val_acc: 0.7661\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0788 - acc: 0.9586 - val_loss: 1.4907 - val_acc: 0.7762\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0764 - acc: 0.9597 - val_loss: 1.4779 - val_acc: 0.7739\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0789 - acc: 0.9579 - val_loss: 1.4630 - val_acc: 0.7751\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0770 - acc: 0.9573 - val_loss: 1.5434 - val_acc: 0.7628\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0780 - acc: 0.9589 - val_loss: 1.4772 - val_acc: 0.7673\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0760 - acc: 0.9586 - val_loss: 1.5212 - val_acc: 0.7639\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0766 - acc: 0.9573 - val_loss: 1.5255 - val_acc: 0.7751\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0751 - acc: 0.9586 - val_loss: 1.6338 - val_acc: 0.7539\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0751 - acc: 0.9595 - val_loss: 1.5764 - val_acc: 0.7595\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0755 - acc: 0.9598 - val_loss: 1.4994 - val_acc: 0.7751\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0748 - acc: 0.9584 - val_loss: 1.5738 - val_acc: 0.7617\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0724 - acc: 0.9577 - val_loss: 1.5255 - val_acc: 0.7684\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0746 - acc: 0.9589 - val_loss: 1.5716 - val_acc: 0.7673\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0745 - acc: 0.9608 - val_loss: 1.6155 - val_acc: 0.7639\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0720 - acc: 0.9603 - val_loss: 1.5515 - val_acc: 0.7661\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0740 - acc: 0.9589 - val_loss: 1.5323 - val_acc: 0.7695\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0720 - acc: 0.9586 - val_loss: 1.5683 - val_acc: 0.7595\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0728 - acc: 0.9578 - val_loss: 1.5489 - val_acc: 0.7695\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0729 - acc: 0.9598 - val_loss: 1.6192 - val_acc: 0.7595\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0723 - acc: 0.9579 - val_loss: 1.6279 - val_acc: 0.7595\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0715 - acc: 0.9576 - val_loss: 1.6095 - val_acc: 0.7628\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0710 - acc: 0.9597 - val_loss: 1.5758 - val_acc: 0.7639\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0692 - acc: 0.9604 - val_loss: 1.6047 - val_acc: 0.7673\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0717 - acc: 0.9567 - val_loss: 1.5341 - val_acc: 0.7684\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0712 - acc: 0.9579 - val_loss: 1.6065 - val_acc: 0.7695\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0716 - acc: 0.9570 - val_loss: 1.6287 - val_acc: 0.7661\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0699 - acc: 0.9595 - val_loss: 1.6535 - val_acc: 0.7584\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0695 - acc: 0.9595 - val_loss: 1.6496 - val_acc: 0.7673\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0709 - acc: 0.9587 - val_loss: 1.6113 - val_acc: 0.7717\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0702 - acc: 0.9582 - val_loss: 1.6287 - val_acc: 0.7661\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0693 - acc: 0.9582 - val_loss: 1.6126 - val_acc: 0.7650\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0704 - acc: 0.9593 - val_loss: 1.6526 - val_acc: 0.7650\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0685 - acc: 0.9594 - val_loss: 1.7117 - val_acc: 0.7550\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0690 - acc: 0.9583 - val_loss: 1.6931 - val_acc: 0.7617\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0680 - acc: 0.9582 - val_loss: 1.6568 - val_acc: 0.7650\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0679 - acc: 0.9603 - val_loss: 1.6706 - val_acc: 0.7606\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0676 - acc: 0.9577 - val_loss: 1.7117 - val_acc: 0.7606\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0665 - acc: 0.9608 - val_loss: 1.6672 - val_acc: 0.7673\n",
            "처리중인 폴드 # 2\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 3s 333us/step - loss: 2.4463 - acc: 0.5442 - val_loss: 1.6695 - val_acc: 0.6492\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 1.3347 - acc: 0.7118 - val_loss: 1.3072 - val_acc: 0.7105\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 1.0068 - acc: 0.7782 - val_loss: 1.1503 - val_acc: 0.7450\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.7955 - acc: 0.8326 - val_loss: 1.0534 - val_acc: 0.7684\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.6341 - acc: 0.8667 - val_loss: 1.0132 - val_acc: 0.7840\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.5057 - acc: 0.8934 - val_loss: 0.9712 - val_acc: 0.7918\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.4080 - acc: 0.9159 - val_loss: 0.9529 - val_acc: 0.8029\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.3298 - acc: 0.9290 - val_loss: 0.9950 - val_acc: 0.7906\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.2776 - acc: 0.9398 - val_loss: 0.9509 - val_acc: 0.8096\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.2313 - acc: 0.9459 - val_loss: 0.9942 - val_acc: 0.7973\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.2052 - acc: 0.9485 - val_loss: 0.9656 - val_acc: 0.8096\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1757 - acc: 0.9542 - val_loss: 0.9765 - val_acc: 0.8040\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1606 - acc: 0.9544 - val_loss: 1.0032 - val_acc: 0.8051\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.1504 - acc: 0.9544 - val_loss: 1.0370 - val_acc: 0.8062\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1381 - acc: 0.9553 - val_loss: 1.0629 - val_acc: 0.8018\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1290 - acc: 0.9562 - val_loss: 1.0597 - val_acc: 0.8040\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1213 - acc: 0.9574 - val_loss: 1.1204 - val_acc: 0.7895\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1129 - acc: 0.9591 - val_loss: 1.1068 - val_acc: 0.7973\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1139 - acc: 0.9586 - val_loss: 1.1271 - val_acc: 0.7962\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.1090 - acc: 0.9552 - val_loss: 1.1527 - val_acc: 0.7906\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1087 - acc: 0.9572 - val_loss: 1.1603 - val_acc: 0.7962\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1051 - acc: 0.9573 - val_loss: 1.1498 - val_acc: 0.7929\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1053 - acc: 0.9584 - val_loss: 1.2328 - val_acc: 0.7918\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1012 - acc: 0.9603 - val_loss: 1.1637 - val_acc: 0.7962\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1005 - acc: 0.9586 - val_loss: 1.1914 - val_acc: 0.7918\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0958 - acc: 0.9591 - val_loss: 1.2144 - val_acc: 0.7884\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0942 - acc: 0.9607 - val_loss: 1.2812 - val_acc: 0.7817\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0958 - acc: 0.9602 - val_loss: 1.2365 - val_acc: 0.7895\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0969 - acc: 0.9582 - val_loss: 1.2779 - val_acc: 0.7862\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0917 - acc: 0.9607 - val_loss: 1.2752 - val_acc: 0.7851\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0913 - acc: 0.9599 - val_loss: 1.3030 - val_acc: 0.7829\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0905 - acc: 0.9600 - val_loss: 1.3233 - val_acc: 0.7851\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0934 - acc: 0.9581 - val_loss: 1.3012 - val_acc: 0.7840\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0901 - acc: 0.9599 - val_loss: 1.3259 - val_acc: 0.7873\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0901 - acc: 0.9608 - val_loss: 1.3447 - val_acc: 0.7840\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0866 - acc: 0.9609 - val_loss: 1.4276 - val_acc: 0.7784\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0888 - acc: 0.9598 - val_loss: 1.3607 - val_acc: 0.7851\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0885 - acc: 0.9603 - val_loss: 1.4135 - val_acc: 0.7851\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0859 - acc: 0.9613 - val_loss: 1.3386 - val_acc: 0.7873\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0869 - acc: 0.9594 - val_loss: 1.3717 - val_acc: 0.7829\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0849 - acc: 0.9608 - val_loss: 1.4050 - val_acc: 0.7773\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0853 - acc: 0.9595 - val_loss: 1.4922 - val_acc: 0.7728\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0866 - acc: 0.9592 - val_loss: 1.3708 - val_acc: 0.7840\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0848 - acc: 0.9609 - val_loss: 1.4234 - val_acc: 0.7862\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0853 - acc: 0.9609 - val_loss: 1.4582 - val_acc: 0.7806\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0824 - acc: 0.9597 - val_loss: 1.4811 - val_acc: 0.7773\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0823 - acc: 0.9614 - val_loss: 1.4227 - val_acc: 0.7840\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0826 - acc: 0.9602 - val_loss: 1.5059 - val_acc: 0.7795\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0810 - acc: 0.9613 - val_loss: 1.5035 - val_acc: 0.7784\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0813 - acc: 0.9608 - val_loss: 1.4385 - val_acc: 0.7840\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0812 - acc: 0.9602 - val_loss: 1.4842 - val_acc: 0.7762\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0810 - acc: 0.9592 - val_loss: 1.4676 - val_acc: 0.7739\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0836 - acc: 0.9587 - val_loss: 1.4621 - val_acc: 0.7829\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 71us/step - loss: 0.0767 - acc: 0.9605 - val_loss: 1.5238 - val_acc: 0.7840\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0789 - acc: 0.9612 - val_loss: 1.5151 - val_acc: 0.7784\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0787 - acc: 0.9594 - val_loss: 1.4974 - val_acc: 0.7795\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0806 - acc: 0.9597 - val_loss: 1.4867 - val_acc: 0.7873\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0777 - acc: 0.9598 - val_loss: 1.5674 - val_acc: 0.7762\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0781 - acc: 0.9599 - val_loss: 1.5409 - val_acc: 0.7695\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0779 - acc: 0.9599 - val_loss: 1.5691 - val_acc: 0.7751\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0770 - acc: 0.9599 - val_loss: 1.5900 - val_acc: 0.7728\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 71us/step - loss: 0.0742 - acc: 0.9623 - val_loss: 1.5547 - val_acc: 0.7684\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0759 - acc: 0.9617 - val_loss: 1.6065 - val_acc: 0.7706\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0747 - acc: 0.9605 - val_loss: 1.5823 - val_acc: 0.7706\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 71us/step - loss: 0.0775 - acc: 0.9594 - val_loss: 1.5769 - val_acc: 0.7695\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0749 - acc: 0.9603 - val_loss: 1.6253 - val_acc: 0.7717\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0749 - acc: 0.9591 - val_loss: 1.5899 - val_acc: 0.7739\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0744 - acc: 0.9617 - val_loss: 1.5640 - val_acc: 0.7728\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0713 - acc: 0.9612 - val_loss: 1.6351 - val_acc: 0.7739\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0747 - acc: 0.9603 - val_loss: 1.5755 - val_acc: 0.7706\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0724 - acc: 0.9624 - val_loss: 1.5778 - val_acc: 0.7795\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0721 - acc: 0.9605 - val_loss: 1.6376 - val_acc: 0.7762\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0720 - acc: 0.9603 - val_loss: 1.6707 - val_acc: 0.7661\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0726 - acc: 0.9600 - val_loss: 1.6124 - val_acc: 0.7751\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0722 - acc: 0.9604 - val_loss: 1.7052 - val_acc: 0.7717\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0724 - acc: 0.9592 - val_loss: 1.6646 - val_acc: 0.7695\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0696 - acc: 0.9624 - val_loss: 1.7249 - val_acc: 0.7706\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0694 - acc: 0.9610 - val_loss: 1.7291 - val_acc: 0.7650\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0699 - acc: 0.9602 - val_loss: 1.6553 - val_acc: 0.7684\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0690 - acc: 0.9614 - val_loss: 1.7481 - val_acc: 0.7595\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0704 - acc: 0.9584 - val_loss: 1.7241 - val_acc: 0.7684\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0678 - acc: 0.9614 - val_loss: 1.7397 - val_acc: 0.7606\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0707 - acc: 0.9583 - val_loss: 1.7594 - val_acc: 0.7572\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0687 - acc: 0.9613 - val_loss: 1.7899 - val_acc: 0.7628\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0677 - acc: 0.9608 - val_loss: 1.7310 - val_acc: 0.7661\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0685 - acc: 0.9608 - val_loss: 1.7644 - val_acc: 0.7673\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0667 - acc: 0.9608 - val_loss: 1.7505 - val_acc: 0.7661\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0678 - acc: 0.9623 - val_loss: 1.7906 - val_acc: 0.7595\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0669 - acc: 0.9598 - val_loss: 1.8176 - val_acc: 0.7673\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0674 - acc: 0.9593 - val_loss: 1.8658 - val_acc: 0.7650\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0671 - acc: 0.9613 - val_loss: 1.8601 - val_acc: 0.7561\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0667 - acc: 0.9600 - val_loss: 1.8209 - val_acc: 0.7628\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0668 - acc: 0.9610 - val_loss: 1.8272 - val_acc: 0.7595\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0665 - acc: 0.9600 - val_loss: 1.8645 - val_acc: 0.7606\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0660 - acc: 0.9613 - val_loss: 1.8229 - val_acc: 0.7650\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0651 - acc: 0.9605 - val_loss: 1.8907 - val_acc: 0.7606\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0659 - acc: 0.9603 - val_loss: 1.8317 - val_acc: 0.7684\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0642 - acc: 0.9620 - val_loss: 1.8898 - val_acc: 0.7639\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0646 - acc: 0.9629 - val_loss: 1.9348 - val_acc: 0.7628\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0661 - acc: 0.9582 - val_loss: 1.9746 - val_acc: 0.7595\n",
            "처리중인 폴드 # 3\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 3s 343us/step - loss: 2.6670 - acc: 0.5212 - val_loss: 1.8519 - val_acc: 0.6269\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 1.4957 - acc: 0.6904 - val_loss: 1.3684 - val_acc: 0.6993\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 1.0868 - acc: 0.7690 - val_loss: 1.1632 - val_acc: 0.7506\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.8455 - acc: 0.8237 - val_loss: 1.0693 - val_acc: 0.7684\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.6736 - acc: 0.8608 - val_loss: 0.9924 - val_acc: 0.7962\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.5407 - acc: 0.8910 - val_loss: 0.9562 - val_acc: 0.7962\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.4385 - acc: 0.9112 - val_loss: 0.9389 - val_acc: 0.7984\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.3587 - acc: 0.9250 - val_loss: 0.9062 - val_acc: 0.8007\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.2932 - acc: 0.9375 - val_loss: 0.9281 - val_acc: 0.8029\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.2473 - acc: 0.9448 - val_loss: 0.9427 - val_acc: 0.8040\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.2114 - acc: 0.9516 - val_loss: 0.9543 - val_acc: 0.7996\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1865 - acc: 0.9511 - val_loss: 0.9436 - val_acc: 0.8040\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.1642 - acc: 0.9550 - val_loss: 0.9820 - val_acc: 0.7984\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1553 - acc: 0.9556 - val_loss: 1.0031 - val_acc: 0.8062\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1442 - acc: 0.9553 - val_loss: 0.9752 - val_acc: 0.8073\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1293 - acc: 0.9592 - val_loss: 1.0258 - val_acc: 0.8029\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.1220 - acc: 0.9588 - val_loss: 1.1377 - val_acc: 0.7739\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.1201 - acc: 0.9588 - val_loss: 1.1024 - val_acc: 0.7895\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.1169 - acc: 0.9593 - val_loss: 1.0906 - val_acc: 0.7951\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.1105 - acc: 0.9594 - val_loss: 1.1578 - val_acc: 0.7862\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1076 - acc: 0.9588 - val_loss: 1.1622 - val_acc: 0.7884\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1054 - acc: 0.9602 - val_loss: 1.2243 - val_acc: 0.7751\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1055 - acc: 0.9587 - val_loss: 1.1628 - val_acc: 0.7929\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0994 - acc: 0.9597 - val_loss: 1.2706 - val_acc: 0.7706\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0986 - acc: 0.9603 - val_loss: 1.1772 - val_acc: 0.7940\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0985 - acc: 0.9608 - val_loss: 1.1842 - val_acc: 0.7940\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0974 - acc: 0.9604 - val_loss: 1.2301 - val_acc: 0.7884\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0935 - acc: 0.9624 - val_loss: 1.2355 - val_acc: 0.7795\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0929 - acc: 0.9607 - val_loss: 1.2831 - val_acc: 0.7739\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0913 - acc: 0.9605 - val_loss: 1.3177 - val_acc: 0.7840\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0945 - acc: 0.9598 - val_loss: 1.3043 - val_acc: 0.7817\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0933 - acc: 0.9609 - val_loss: 1.3236 - val_acc: 0.7817\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0889 - acc: 0.9614 - val_loss: 1.2995 - val_acc: 0.7851\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0905 - acc: 0.9610 - val_loss: 1.3450 - val_acc: 0.7706\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0915 - acc: 0.9609 - val_loss: 1.4027 - val_acc: 0.7728\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0892 - acc: 0.9591 - val_loss: 1.3422 - val_acc: 0.7817\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0884 - acc: 0.9592 - val_loss: 1.3463 - val_acc: 0.7784\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0866 - acc: 0.9609 - val_loss: 1.3050 - val_acc: 0.7906\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0887 - acc: 0.9613 - val_loss: 1.3792 - val_acc: 0.7773\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0839 - acc: 0.9619 - val_loss: 1.3837 - val_acc: 0.7650\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0851 - acc: 0.9619 - val_loss: 1.3902 - val_acc: 0.7817\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0850 - acc: 0.9618 - val_loss: 1.4274 - val_acc: 0.7751\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0853 - acc: 0.9618 - val_loss: 1.3649 - val_acc: 0.7840\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0883 - acc: 0.9581 - val_loss: 1.4095 - val_acc: 0.7795\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0827 - acc: 0.9604 - val_loss: 1.4237 - val_acc: 0.7773\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0825 - acc: 0.9605 - val_loss: 1.3876 - val_acc: 0.7795\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0831 - acc: 0.9612 - val_loss: 1.5012 - val_acc: 0.7684\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0835 - acc: 0.9599 - val_loss: 1.4820 - val_acc: 0.7795\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0803 - acc: 0.9613 - val_loss: 1.4028 - val_acc: 0.7895\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0793 - acc: 0.9609 - val_loss: 1.4751 - val_acc: 0.7762\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0798 - acc: 0.9617 - val_loss: 1.5211 - val_acc: 0.7717\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0815 - acc: 0.9612 - val_loss: 1.4291 - val_acc: 0.7806\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0786 - acc: 0.9612 - val_loss: 1.5274 - val_acc: 0.7661\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0786 - acc: 0.9624 - val_loss: 1.4258 - val_acc: 0.7762\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0789 - acc: 0.9620 - val_loss: 1.4704 - val_acc: 0.7829\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0783 - acc: 0.9600 - val_loss: 1.4973 - val_acc: 0.7728\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0753 - acc: 0.9619 - val_loss: 1.5252 - val_acc: 0.7728\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0803 - acc: 0.9597 - val_loss: 1.4979 - val_acc: 0.7650\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0735 - acc: 0.9619 - val_loss: 1.5118 - val_acc: 0.7739\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0787 - acc: 0.9600 - val_loss: 1.5787 - val_acc: 0.7684\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0780 - acc: 0.9592 - val_loss: 1.5282 - val_acc: 0.7717\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0729 - acc: 0.9614 - val_loss: 1.5120 - val_acc: 0.7739\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0745 - acc: 0.9625 - val_loss: 1.5418 - val_acc: 0.7728\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0731 - acc: 0.9620 - val_loss: 1.5810 - val_acc: 0.7695\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0764 - acc: 0.9604 - val_loss: 1.5328 - val_acc: 0.7684\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0719 - acc: 0.9618 - val_loss: 1.5960 - val_acc: 0.7628\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0745 - acc: 0.9624 - val_loss: 1.6062 - val_acc: 0.7684\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0717 - acc: 0.9607 - val_loss: 1.6217 - val_acc: 0.7650\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0731 - acc: 0.9600 - val_loss: 1.5600 - val_acc: 0.7695\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0726 - acc: 0.9609 - val_loss: 1.6529 - val_acc: 0.7661\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0723 - acc: 0.9605 - val_loss: 1.6203 - val_acc: 0.7673\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0724 - acc: 0.9614 - val_loss: 1.6081 - val_acc: 0.7650\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0707 - acc: 0.9615 - val_loss: 1.6338 - val_acc: 0.7639\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0709 - acc: 0.9621 - val_loss: 1.6756 - val_acc: 0.7606\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0700 - acc: 0.9629 - val_loss: 1.6216 - val_acc: 0.7628\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0720 - acc: 0.9612 - val_loss: 1.6139 - val_acc: 0.7639\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0691 - acc: 0.9607 - val_loss: 1.6442 - val_acc: 0.7628\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0706 - acc: 0.9609 - val_loss: 1.7076 - val_acc: 0.7595\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0686 - acc: 0.9608 - val_loss: 1.7151 - val_acc: 0.7572\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0696 - acc: 0.9609 - val_loss: 1.6287 - val_acc: 0.7650\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0695 - acc: 0.9612 - val_loss: 1.6694 - val_acc: 0.7661\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0689 - acc: 0.9613 - val_loss: 1.6699 - val_acc: 0.7628\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0672 - acc: 0.9618 - val_loss: 1.7315 - val_acc: 0.7639\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0705 - acc: 0.9604 - val_loss: 1.7023 - val_acc: 0.7572\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0671 - acc: 0.9625 - val_loss: 1.7285 - val_acc: 0.7595\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0694 - acc: 0.9608 - val_loss: 1.6792 - val_acc: 0.7661\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0687 - acc: 0.9621 - val_loss: 1.7263 - val_acc: 0.7584\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0658 - acc: 0.9609 - val_loss: 1.6768 - val_acc: 0.7673\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0675 - acc: 0.9599 - val_loss: 1.7220 - val_acc: 0.7606\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0659 - acc: 0.9621 - val_loss: 1.7376 - val_acc: 0.7617\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0674 - acc: 0.9600 - val_loss: 1.7184 - val_acc: 0.7661\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0670 - acc: 0.9604 - val_loss: 1.7460 - val_acc: 0.7595\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0672 - acc: 0.9594 - val_loss: 1.7536 - val_acc: 0.7584\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0663 - acc: 0.9604 - val_loss: 1.7376 - val_acc: 0.7584\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0669 - acc: 0.9605 - val_loss: 1.8057 - val_acc: 0.7572\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0653 - acc: 0.9614 - val_loss: 1.8345 - val_acc: 0.7550\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0656 - acc: 0.9615 - val_loss: 1.7835 - val_acc: 0.7650\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0661 - acc: 0.9602 - val_loss: 1.7572 - val_acc: 0.7661\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0645 - acc: 0.9603 - val_loss: 1.7821 - val_acc: 0.7517\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0648 - acc: 0.9600 - val_loss: 1.8354 - val_acc: 0.7561\n",
            "처리중인 폴드 # 4\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 3s 345us/step - loss: 2.7249 - acc: 0.4405 - val_loss: 1.8474 - val_acc: 0.6281\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 1.4710 - acc: 0.7030 - val_loss: 1.3897 - val_acc: 0.7060\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 1.0718 - acc: 0.7770 - val_loss: 1.2318 - val_acc: 0.7394\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.8329 - acc: 0.8315 - val_loss: 1.1097 - val_acc: 0.7650\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.6567 - acc: 0.8690 - val_loss: 1.0384 - val_acc: 0.7817\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.5286 - acc: 0.8970 - val_loss: 1.0134 - val_acc: 0.7784\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.4247 - acc: 0.9172 - val_loss: 0.9781 - val_acc: 0.7940\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.3443 - acc: 0.9284 - val_loss: 0.9758 - val_acc: 0.7996\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.2883 - acc: 0.9383 - val_loss: 1.0054 - val_acc: 0.7873\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.2454 - acc: 0.9442 - val_loss: 0.9676 - val_acc: 0.7996\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.2059 - acc: 0.9513 - val_loss: 1.0042 - val_acc: 0.7996\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1797 - acc: 0.9541 - val_loss: 1.0438 - val_acc: 0.7873\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1681 - acc: 0.9553 - val_loss: 1.0079 - val_acc: 0.7951\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1514 - acc: 0.9572 - val_loss: 1.0810 - val_acc: 0.7762\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1395 - acc: 0.9578 - val_loss: 1.0596 - val_acc: 0.7918\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.1290 - acc: 0.9560 - val_loss: 1.0846 - val_acc: 0.8018\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1227 - acc: 0.9579 - val_loss: 1.1081 - val_acc: 0.7873\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1181 - acc: 0.9599 - val_loss: 1.1645 - val_acc: 0.7751\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1128 - acc: 0.9592 - val_loss: 1.1719 - val_acc: 0.7795\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.1104 - acc: 0.9595 - val_loss: 1.1639 - val_acc: 0.7940\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1065 - acc: 0.9593 - val_loss: 1.2155 - val_acc: 0.7884\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.1068 - acc: 0.9591 - val_loss: 1.2505 - val_acc: 0.7762\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1018 - acc: 0.9588 - val_loss: 1.1898 - val_acc: 0.7929\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.1047 - acc: 0.9586 - val_loss: 1.2219 - val_acc: 0.7884\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0993 - acc: 0.9589 - val_loss: 1.2389 - val_acc: 0.7906\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0985 - acc: 0.9595 - val_loss: 1.2574 - val_acc: 0.7884\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0982 - acc: 0.9577 - val_loss: 1.2847 - val_acc: 0.7851\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0939 - acc: 0.9587 - val_loss: 1.3157 - val_acc: 0.7773\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0944 - acc: 0.9614 - val_loss: 1.2596 - val_acc: 0.7929\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0940 - acc: 0.9618 - val_loss: 1.3164 - val_acc: 0.7806\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0927 - acc: 0.9599 - val_loss: 1.3497 - val_acc: 0.7784\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0918 - acc: 0.9598 - val_loss: 1.3315 - val_acc: 0.7795\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0924 - acc: 0.9583 - val_loss: 1.3578 - val_acc: 0.7773\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0881 - acc: 0.9613 - val_loss: 1.4010 - val_acc: 0.7728\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0890 - acc: 0.9607 - val_loss: 1.3500 - val_acc: 0.7795\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0901 - acc: 0.9598 - val_loss: 1.3781 - val_acc: 0.7739\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0858 - acc: 0.9629 - val_loss: 1.4474 - val_acc: 0.7717\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0902 - acc: 0.9592 - val_loss: 1.3386 - val_acc: 0.7817\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0867 - acc: 0.9597 - val_loss: 1.4125 - val_acc: 0.7873\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0855 - acc: 0.9607 - val_loss: 1.4231 - val_acc: 0.7695\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0852 - acc: 0.9615 - val_loss: 1.4277 - val_acc: 0.7739\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0864 - acc: 0.9617 - val_loss: 1.4005 - val_acc: 0.7829\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0837 - acc: 0.9615 - val_loss: 1.4386 - val_acc: 0.7773\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0839 - acc: 0.9615 - val_loss: 1.4203 - val_acc: 0.7829\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0852 - acc: 0.9583 - val_loss: 1.4335 - val_acc: 0.7873\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0812 - acc: 0.9604 - val_loss: 1.4409 - val_acc: 0.7717\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0835 - acc: 0.9617 - val_loss: 1.4552 - val_acc: 0.7795\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0794 - acc: 0.9624 - val_loss: 1.4950 - val_acc: 0.7661\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0820 - acc: 0.9623 - val_loss: 1.4669 - val_acc: 0.7661\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0793 - acc: 0.9618 - val_loss: 1.4659 - val_acc: 0.7784\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0830 - acc: 0.9591 - val_loss: 1.4381 - val_acc: 0.7840\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0799 - acc: 0.9603 - val_loss: 1.5025 - val_acc: 0.7739\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0814 - acc: 0.9604 - val_loss: 1.4817 - val_acc: 0.7773\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0789 - acc: 0.9604 - val_loss: 1.5701 - val_acc: 0.7606\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 71us/step - loss: 0.0804 - acc: 0.9597 - val_loss: 1.5359 - val_acc: 0.7751\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0776 - acc: 0.9618 - val_loss: 1.5292 - val_acc: 0.7751\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0778 - acc: 0.9605 - val_loss: 1.5489 - val_acc: 0.7728\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0764 - acc: 0.9609 - val_loss: 1.5383 - val_acc: 0.7762\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0788 - acc: 0.9615 - val_loss: 1.5483 - val_acc: 0.7728\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0782 - acc: 0.9594 - val_loss: 1.5334 - val_acc: 0.7784\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0763 - acc: 0.9612 - val_loss: 1.6510 - val_acc: 0.7561\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0769 - acc: 0.9591 - val_loss: 1.5670 - val_acc: 0.7773\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0757 - acc: 0.9593 - val_loss: 1.5512 - val_acc: 0.7751\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0767 - acc: 0.9614 - val_loss: 1.5968 - val_acc: 0.7728\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0764 - acc: 0.9609 - val_loss: 1.5897 - val_acc: 0.7762\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0754 - acc: 0.9605 - val_loss: 1.5663 - val_acc: 0.7773\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0732 - acc: 0.9605 - val_loss: 1.5682 - val_acc: 0.7751\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0755 - acc: 0.9598 - val_loss: 1.6223 - val_acc: 0.7762\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0740 - acc: 0.9607 - val_loss: 1.6193 - val_acc: 0.7661\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0750 - acc: 0.9597 - val_loss: 1.6163 - val_acc: 0.7717\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0720 - acc: 0.9623 - val_loss: 1.6125 - val_acc: 0.7739\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0722 - acc: 0.9615 - val_loss: 1.6183 - val_acc: 0.7773\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0737 - acc: 0.9591 - val_loss: 1.6582 - val_acc: 0.7706\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0716 - acc: 0.9599 - val_loss: 1.6562 - val_acc: 0.7695\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0723 - acc: 0.9603 - val_loss: 1.6721 - val_acc: 0.7673\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0707 - acc: 0.9605 - val_loss: 1.6543 - val_acc: 0.7751\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0703 - acc: 0.9603 - val_loss: 1.6812 - val_acc: 0.7806\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0717 - acc: 0.9605 - val_loss: 1.6770 - val_acc: 0.7739\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0708 - acc: 0.9626 - val_loss: 1.7285 - val_acc: 0.7695\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0718 - acc: 0.9581 - val_loss: 1.7624 - val_acc: 0.7572\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0683 - acc: 0.9613 - val_loss: 1.6673 - val_acc: 0.7684\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0706 - acc: 0.9613 - val_loss: 1.7314 - val_acc: 0.7684\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0681 - acc: 0.9626 - val_loss: 1.7327 - val_acc: 0.7650\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0678 - acc: 0.9600 - val_loss: 1.8052 - val_acc: 0.7628\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0711 - acc: 0.9597 - val_loss: 1.7464 - val_acc: 0.7795\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0670 - acc: 0.9614 - val_loss: 1.8312 - val_acc: 0.7506\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0681 - acc: 0.9613 - val_loss: 1.7256 - val_acc: 0.7695\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0681 - acc: 0.9612 - val_loss: 1.8084 - val_acc: 0.7617\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0668 - acc: 0.9625 - val_loss: 1.7641 - val_acc: 0.7628\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0677 - acc: 0.9604 - val_loss: 1.7727 - val_acc: 0.7739\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0668 - acc: 0.9599 - val_loss: 1.8189 - val_acc: 0.7606\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0668 - acc: 0.9615 - val_loss: 1.7704 - val_acc: 0.7661\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0682 - acc: 0.9583 - val_loss: 1.7796 - val_acc: 0.7706\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0644 - acc: 0.9619 - val_loss: 1.7943 - val_acc: 0.7684\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0680 - acc: 0.9608 - val_loss: 1.8147 - val_acc: 0.7695\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 70us/step - loss: 0.0658 - acc: 0.9608 - val_loss: 1.8444 - val_acc: 0.7606\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0670 - acc: 0.9604 - val_loss: 1.8061 - val_acc: 0.7673\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0659 - acc: 0.9604 - val_loss: 1.8571 - val_acc: 0.7684\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0660 - acc: 0.9610 - val_loss: 1.8747 - val_acc: 0.7661\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0647 - acc: 0.9614 - val_loss: 1.8804 - val_acc: 0.7528\n",
            "처리중인 폴드 # 5\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 3s 341us/step - loss: 2.6478 - acc: 0.5189 - val_loss: 1.7783 - val_acc: 0.6459\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 1.4280 - acc: 0.7123 - val_loss: 1.3972 - val_acc: 0.7027\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 1.0603 - acc: 0.7775 - val_loss: 1.2055 - val_acc: 0.7439\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.8414 - acc: 0.8220 - val_loss: 1.0778 - val_acc: 0.7684\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.6694 - acc: 0.8572 - val_loss: 1.0253 - val_acc: 0.7795\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 71us/step - loss: 0.5347 - acc: 0.8867 - val_loss: 0.9890 - val_acc: 0.7873\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.4310 - acc: 0.9107 - val_loss: 0.9403 - val_acc: 0.8040\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.3540 - acc: 0.9242 - val_loss: 0.9191 - val_acc: 0.8018\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.2921 - acc: 0.9364 - val_loss: 0.9249 - val_acc: 0.8051\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.2487 - acc: 0.9437 - val_loss: 0.9138 - val_acc: 0.8196\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.2127 - acc: 0.9485 - val_loss: 0.9353 - val_acc: 0.8007\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.1913 - acc: 0.9499 - val_loss: 1.0253 - val_acc: 0.7973\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1723 - acc: 0.9524 - val_loss: 0.9592 - val_acc: 0.8062\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1602 - acc: 0.9536 - val_loss: 1.0060 - val_acc: 0.8118\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1460 - acc: 0.9526 - val_loss: 1.0061 - val_acc: 0.8129\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.1333 - acc: 0.9570 - val_loss: 1.0468 - val_acc: 0.8018\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.1329 - acc: 0.9547 - val_loss: 1.0954 - val_acc: 0.7996\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1218 - acc: 0.9560 - val_loss: 1.0646 - val_acc: 0.8062\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.1191 - acc: 0.9555 - val_loss: 1.0749 - val_acc: 0.8051\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1176 - acc: 0.9565 - val_loss: 1.0886 - val_acc: 0.8129\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1145 - acc: 0.9548 - val_loss: 1.1396 - val_acc: 0.8062\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.1120 - acc: 0.9555 - val_loss: 1.1548 - val_acc: 0.8040\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.1104 - acc: 0.9556 - val_loss: 1.1643 - val_acc: 0.8018\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1072 - acc: 0.9576 - val_loss: 1.2125 - val_acc: 0.7951\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1094 - acc: 0.9558 - val_loss: 1.1696 - val_acc: 0.8107\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.1051 - acc: 0.9547 - val_loss: 1.2120 - val_acc: 0.8118\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1054 - acc: 0.9550 - val_loss: 1.1877 - val_acc: 0.8085\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1032 - acc: 0.9570 - val_loss: 1.2130 - val_acc: 0.8062\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0995 - acc: 0.9570 - val_loss: 1.1835 - val_acc: 0.8007\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1016 - acc: 0.9561 - val_loss: 1.2444 - val_acc: 0.8018\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0951 - acc: 0.9571 - val_loss: 1.2460 - val_acc: 0.8007\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0999 - acc: 0.9562 - val_loss: 1.2207 - val_acc: 0.8073\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0959 - acc: 0.9582 - val_loss: 1.2627 - val_acc: 0.8096\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0963 - acc: 0.9560 - val_loss: 1.2975 - val_acc: 0.7973\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0945 - acc: 0.9579 - val_loss: 1.2837 - val_acc: 0.7996\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0972 - acc: 0.9567 - val_loss: 1.2554 - val_acc: 0.8085\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0904 - acc: 0.9574 - val_loss: 1.3043 - val_acc: 0.7996\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0934 - acc: 0.9581 - val_loss: 1.3693 - val_acc: 0.7906\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0944 - acc: 0.9562 - val_loss: 1.3386 - val_acc: 0.7962\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0882 - acc: 0.9597 - val_loss: 1.2612 - val_acc: 0.7984\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0903 - acc: 0.9594 - val_loss: 1.3198 - val_acc: 0.7951\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0921 - acc: 0.9579 - val_loss: 1.4374 - val_acc: 0.7739\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0901 - acc: 0.9594 - val_loss: 1.3334 - val_acc: 0.7984\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0890 - acc: 0.9579 - val_loss: 1.3504 - val_acc: 0.8018\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0898 - acc: 0.9556 - val_loss: 1.3453 - val_acc: 0.8029\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0874 - acc: 0.9581 - val_loss: 1.3651 - val_acc: 0.7906\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0886 - acc: 0.9581 - val_loss: 1.3563 - val_acc: 0.8007\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0882 - acc: 0.9553 - val_loss: 1.4418 - val_acc: 0.7829\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0855 - acc: 0.9581 - val_loss: 1.3930 - val_acc: 0.7973\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0841 - acc: 0.9584 - val_loss: 1.3489 - val_acc: 0.7962\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0861 - acc: 0.9573 - val_loss: 1.3920 - val_acc: 0.7973\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0855 - acc: 0.9553 - val_loss: 1.4162 - val_acc: 0.7884\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0830 - acc: 0.9571 - val_loss: 1.4294 - val_acc: 0.7884\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0832 - acc: 0.9577 - val_loss: 1.4252 - val_acc: 0.7973\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0811 - acc: 0.9582 - val_loss: 1.4434 - val_acc: 0.7873\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 71us/step - loss: 0.0831 - acc: 0.9561 - val_loss: 1.4424 - val_acc: 0.7895\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0823 - acc: 0.9566 - val_loss: 1.4691 - val_acc: 0.7817\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0822 - acc: 0.9576 - val_loss: 1.4740 - val_acc: 0.7862\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0812 - acc: 0.9566 - val_loss: 1.4127 - val_acc: 0.7906\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0814 - acc: 0.9577 - val_loss: 1.4154 - val_acc: 0.7951\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0786 - acc: 0.9578 - val_loss: 1.4326 - val_acc: 0.7951\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0814 - acc: 0.9589 - val_loss: 1.4439 - val_acc: 0.7906\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0814 - acc: 0.9560 - val_loss: 1.4375 - val_acc: 0.7940\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0788 - acc: 0.9582 - val_loss: 1.4827 - val_acc: 0.7873\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0783 - acc: 0.9587 - val_loss: 1.4204 - val_acc: 0.7929\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0786 - acc: 0.9581 - val_loss: 1.4711 - val_acc: 0.7840\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0794 - acc: 0.9572 - val_loss: 1.4646 - val_acc: 0.7884\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0774 - acc: 0.9570 - val_loss: 1.4453 - val_acc: 0.7884\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 71us/step - loss: 0.0778 - acc: 0.9572 - val_loss: 1.4782 - val_acc: 0.7895\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0781 - acc: 0.9581 - val_loss: 1.4929 - val_acc: 0.7895\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0771 - acc: 0.9574 - val_loss: 1.5135 - val_acc: 0.7906\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0768 - acc: 0.9567 - val_loss: 1.5501 - val_acc: 0.7751\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0768 - acc: 0.9570 - val_loss: 1.5190 - val_acc: 0.7862\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0748 - acc: 0.9583 - val_loss: 1.5292 - val_acc: 0.7784\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0743 - acc: 0.9584 - val_loss: 1.6352 - val_acc: 0.7684\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0756 - acc: 0.9578 - val_loss: 1.5548 - val_acc: 0.7851\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0735 - acc: 0.9581 - val_loss: 1.5265 - val_acc: 0.7829\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 71us/step - loss: 0.0748 - acc: 0.9579 - val_loss: 1.5527 - val_acc: 0.7829\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0766 - acc: 0.9556 - val_loss: 1.5802 - val_acc: 0.7840\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0742 - acc: 0.9567 - val_loss: 1.5581 - val_acc: 0.7817\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0731 - acc: 0.9584 - val_loss: 1.5481 - val_acc: 0.7829\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0757 - acc: 0.9573 - val_loss: 1.5218 - val_acc: 0.7884\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0728 - acc: 0.9584 - val_loss: 1.5660 - val_acc: 0.7851\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0734 - acc: 0.9565 - val_loss: 1.5848 - val_acc: 0.7784\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0740 - acc: 0.9578 - val_loss: 1.5928 - val_acc: 0.7795\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0725 - acc: 0.9581 - val_loss: 1.5894 - val_acc: 0.7817\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0713 - acc: 0.9581 - val_loss: 1.5891 - val_acc: 0.7762\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0735 - acc: 0.9592 - val_loss: 1.6138 - val_acc: 0.7840\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0731 - acc: 0.9563 - val_loss: 1.6040 - val_acc: 0.7795\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0721 - acc: 0.9577 - val_loss: 1.5922 - val_acc: 0.7817\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0717 - acc: 0.9573 - val_loss: 1.6335 - val_acc: 0.7728\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0712 - acc: 0.9586 - val_loss: 1.6046 - val_acc: 0.7751\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0723 - acc: 0.9568 - val_loss: 1.6606 - val_acc: 0.7728\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0717 - acc: 0.9571 - val_loss: 1.6283 - val_acc: 0.7784\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0707 - acc: 0.9589 - val_loss: 1.6501 - val_acc: 0.7829\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0704 - acc: 0.9561 - val_loss: 1.6405 - val_acc: 0.7806\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0706 - acc: 0.9574 - val_loss: 1.6198 - val_acc: 0.7806\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 71us/step - loss: 0.0697 - acc: 0.9570 - val_loss: 1.6597 - val_acc: 0.7762\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0698 - acc: 0.9570 - val_loss: 1.6212 - val_acc: 0.7817\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0700 - acc: 0.9577 - val_loss: 1.7114 - val_acc: 0.7773\n",
            "처리중인 폴드 # 6\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 3s 358us/step - loss: 2.6827 - acc: 0.5098 - val_loss: 1.7458 - val_acc: 0.6425\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 1.4619 - acc: 0.6930 - val_loss: 1.2825 - val_acc: 0.7305\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 1.0703 - acc: 0.7716 - val_loss: 1.0874 - val_acc: 0.7584\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.8344 - acc: 0.8245 - val_loss: 0.9773 - val_acc: 0.7751\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.6585 - acc: 0.8639 - val_loss: 0.8923 - val_acc: 0.8051\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.5246 - acc: 0.8945 - val_loss: 0.8362 - val_acc: 0.8151\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.4187 - acc: 0.9146 - val_loss: 0.8335 - val_acc: 0.8073\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.3440 - acc: 0.9281 - val_loss: 0.8000 - val_acc: 0.8218\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.2849 - acc: 0.9386 - val_loss: 0.8233 - val_acc: 0.8163\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.2421 - acc: 0.9421 - val_loss: 0.7850 - val_acc: 0.8252\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.2095 - acc: 0.9484 - val_loss: 0.8082 - val_acc: 0.8330\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1841 - acc: 0.9503 - val_loss: 0.8536 - val_acc: 0.8163\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1693 - acc: 0.9526 - val_loss: 0.8327 - val_acc: 0.8296\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1535 - acc: 0.9542 - val_loss: 0.8108 - val_acc: 0.8330\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1403 - acc: 0.9556 - val_loss: 0.8368 - val_acc: 0.8330\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1333 - acc: 0.9556 - val_loss: 0.8746 - val_acc: 0.8263\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 71us/step - loss: 0.1301 - acc: 0.9545 - val_loss: 0.8668 - val_acc: 0.8241\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 70us/step - loss: 0.1215 - acc: 0.9584 - val_loss: 0.9007 - val_acc: 0.8241\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.1140 - acc: 0.9574 - val_loss: 0.8997 - val_acc: 0.8229\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1142 - acc: 0.9576 - val_loss: 0.8931 - val_acc: 0.8241\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1139 - acc: 0.9567 - val_loss: 0.9244 - val_acc: 0.8207\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1086 - acc: 0.9588 - val_loss: 0.9211 - val_acc: 0.8174\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1085 - acc: 0.9556 - val_loss: 0.9226 - val_acc: 0.8196\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.1055 - acc: 0.9589 - val_loss: 0.9495 - val_acc: 0.8185\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1016 - acc: 0.9579 - val_loss: 0.9676 - val_acc: 0.8096\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1037 - acc: 0.9571 - val_loss: 0.9596 - val_acc: 0.8263\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1000 - acc: 0.9593 - val_loss: 1.0012 - val_acc: 0.8151\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.1012 - acc: 0.9578 - val_loss: 1.0540 - val_acc: 0.8073\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0996 - acc: 0.9581 - val_loss: 0.9967 - val_acc: 0.8174\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0980 - acc: 0.9586 - val_loss: 0.9642 - val_acc: 0.8185\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0978 - acc: 0.9587 - val_loss: 0.9661 - val_acc: 0.8174\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0949 - acc: 0.9582 - val_loss: 0.9886 - val_acc: 0.8174\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0957 - acc: 0.9577 - val_loss: 0.9815 - val_acc: 0.8185\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0969 - acc: 0.9583 - val_loss: 1.0232 - val_acc: 0.8107\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0920 - acc: 0.9578 - val_loss: 1.0343 - val_acc: 0.8151\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0973 - acc: 0.9562 - val_loss: 1.0209 - val_acc: 0.8107\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0903 - acc: 0.9586 - val_loss: 1.0321 - val_acc: 0.8129\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0953 - acc: 0.9557 - val_loss: 1.0116 - val_acc: 0.8118\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0895 - acc: 0.9594 - val_loss: 1.0150 - val_acc: 0.8163\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0917 - acc: 0.9579 - val_loss: 1.0116 - val_acc: 0.8196\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0902 - acc: 0.9574 - val_loss: 1.0315 - val_acc: 0.8207\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0910 - acc: 0.9572 - val_loss: 1.0923 - val_acc: 0.8118\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0892 - acc: 0.9566 - val_loss: 1.0325 - val_acc: 0.8107\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0877 - acc: 0.9607 - val_loss: 1.0549 - val_acc: 0.8151\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0867 - acc: 0.9583 - val_loss: 1.1110 - val_acc: 0.8096\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0898 - acc: 0.9579 - val_loss: 1.1229 - val_acc: 0.8029\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0862 - acc: 0.9577 - val_loss: 1.0455 - val_acc: 0.8107\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0878 - acc: 0.9577 - val_loss: 1.1065 - val_acc: 0.8085\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0863 - acc: 0.9591 - val_loss: 1.1066 - val_acc: 0.8085\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0851 - acc: 0.9566 - val_loss: 1.0971 - val_acc: 0.7996\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0874 - acc: 0.9570 - val_loss: 1.0824 - val_acc: 0.8140\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0831 - acc: 0.9587 - val_loss: 1.1562 - val_acc: 0.7996\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0835 - acc: 0.9594 - val_loss: 1.1194 - val_acc: 0.8062\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0865 - acc: 0.9574 - val_loss: 1.0945 - val_acc: 0.8174\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0845 - acc: 0.9594 - val_loss: 1.1280 - val_acc: 0.8040\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0830 - acc: 0.9591 - val_loss: 1.1205 - val_acc: 0.7962\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0818 - acc: 0.9598 - val_loss: 1.1453 - val_acc: 0.8096\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0809 - acc: 0.9594 - val_loss: 1.0955 - val_acc: 0.8051\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0800 - acc: 0.9595 - val_loss: 1.1239 - val_acc: 0.8073\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0807 - acc: 0.9591 - val_loss: 1.1222 - val_acc: 0.8040\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0815 - acc: 0.9593 - val_loss: 1.1493 - val_acc: 0.7984\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0802 - acc: 0.9584 - val_loss: 1.1209 - val_acc: 0.8051\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0789 - acc: 0.9583 - val_loss: 1.1311 - val_acc: 0.7984\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0783 - acc: 0.9583 - val_loss: 1.1290 - val_acc: 0.8051\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0806 - acc: 0.9589 - val_loss: 1.1407 - val_acc: 0.7984\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0763 - acc: 0.9591 - val_loss: 1.1604 - val_acc: 0.8018\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0780 - acc: 0.9592 - val_loss: 1.1812 - val_acc: 0.7918\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0771 - acc: 0.9578 - val_loss: 1.2187 - val_acc: 0.8007\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0773 - acc: 0.9571 - val_loss: 1.1482 - val_acc: 0.8085\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0765 - acc: 0.9587 - val_loss: 1.1692 - val_acc: 0.8007\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0778 - acc: 0.9599 - val_loss: 1.1697 - val_acc: 0.7951\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0757 - acc: 0.9599 - val_loss: 1.2076 - val_acc: 0.7996\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0750 - acc: 0.9584 - val_loss: 1.1691 - val_acc: 0.8051\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0747 - acc: 0.9594 - val_loss: 1.1865 - val_acc: 0.7996\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0762 - acc: 0.9576 - val_loss: 1.2090 - val_acc: 0.8040\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0757 - acc: 0.9572 - val_loss: 1.1943 - val_acc: 0.7984\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0734 - acc: 0.9572 - val_loss: 1.2400 - val_acc: 0.7873\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0742 - acc: 0.9576 - val_loss: 1.2440 - val_acc: 0.7918\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0755 - acc: 0.9583 - val_loss: 1.2494 - val_acc: 0.7951\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0738 - acc: 0.9591 - val_loss: 1.2348 - val_acc: 0.7984\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0742 - acc: 0.9579 - val_loss: 1.2171 - val_acc: 0.8007\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0742 - acc: 0.9578 - val_loss: 1.2487 - val_acc: 0.7984\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0709 - acc: 0.9584 - val_loss: 1.2893 - val_acc: 0.7829\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0717 - acc: 0.9594 - val_loss: 1.2784 - val_acc: 0.7862\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0734 - acc: 0.9566 - val_loss: 1.2503 - val_acc: 0.7984\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0713 - acc: 0.9588 - val_loss: 1.2611 - val_acc: 0.7962\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0732 - acc: 0.9578 - val_loss: 1.2895 - val_acc: 0.7929\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0704 - acc: 0.9610 - val_loss: 1.2855 - val_acc: 0.7873\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0705 - acc: 0.9587 - val_loss: 1.2770 - val_acc: 0.7929\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0706 - acc: 0.9597 - val_loss: 1.2932 - val_acc: 0.7996\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0700 - acc: 0.9600 - val_loss: 1.3415 - val_acc: 0.7817\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0706 - acc: 0.9595 - val_loss: 1.2792 - val_acc: 0.7884\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0695 - acc: 0.9598 - val_loss: 1.2973 - val_acc: 0.7940\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 71us/step - loss: 0.0693 - acc: 0.9573 - val_loss: 1.3315 - val_acc: 0.7906\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0707 - acc: 0.9587 - val_loss: 1.3071 - val_acc: 0.7895\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0696 - acc: 0.9573 - val_loss: 1.3092 - val_acc: 0.7929\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0685 - acc: 0.9604 - val_loss: 1.3131 - val_acc: 0.7918\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0674 - acc: 0.9609 - val_loss: 1.3642 - val_acc: 0.7929\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0681 - acc: 0.9578 - val_loss: 1.3437 - val_acc: 0.7984\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0683 - acc: 0.9599 - val_loss: 1.3950 - val_acc: 0.7906\n",
            "처리중인 폴드 # 7\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 3s 355us/step - loss: 2.5425 - acc: 0.5200 - val_loss: 1.6579 - val_acc: 0.6626\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 1.4067 - acc: 0.6997 - val_loss: 1.2589 - val_acc: 0.7283\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 1.0486 - acc: 0.7751 - val_loss: 1.1004 - val_acc: 0.7706\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.8300 - acc: 0.8226 - val_loss: 1.0405 - val_acc: 0.7817\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.6519 - acc: 0.8648 - val_loss: 0.9445 - val_acc: 0.7996\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.5227 - acc: 0.8910 - val_loss: 0.9086 - val_acc: 0.8085\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.4115 - acc: 0.9139 - val_loss: 0.9161 - val_acc: 0.8007\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.3341 - acc: 0.9280 - val_loss: 0.9117 - val_acc: 0.8018\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.2692 - acc: 0.9424 - val_loss: 0.9121 - val_acc: 0.8073\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.2307 - acc: 0.9463 - val_loss: 0.9127 - val_acc: 0.8085\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1944 - acc: 0.9519 - val_loss: 1.0003 - val_acc: 0.7984\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1756 - acc: 0.9541 - val_loss: 0.9473 - val_acc: 0.8207\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.1546 - acc: 0.9572 - val_loss: 1.0003 - val_acc: 0.8096\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.1431 - acc: 0.9568 - val_loss: 1.0209 - val_acc: 0.8085\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1263 - acc: 0.9588 - val_loss: 1.0418 - val_acc: 0.8163\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 71us/step - loss: 0.1219 - acc: 0.9598 - val_loss: 1.0644 - val_acc: 0.8062\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1169 - acc: 0.9592 - val_loss: 1.1167 - val_acc: 0.8018\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1128 - acc: 0.9597 - val_loss: 1.0782 - val_acc: 0.8140\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1057 - acc: 0.9609 - val_loss: 1.1180 - val_acc: 0.7962\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.1043 - acc: 0.9597 - val_loss: 1.1732 - val_acc: 0.7996\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1033 - acc: 0.9599 - val_loss: 1.1670 - val_acc: 0.8007\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.1014 - acc: 0.9612 - val_loss: 1.1749 - val_acc: 0.8062\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0958 - acc: 0.9617 - val_loss: 1.1854 - val_acc: 0.8018\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0953 - acc: 0.9612 - val_loss: 1.1950 - val_acc: 0.7996\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0938 - acc: 0.9614 - val_loss: 1.2098 - val_acc: 0.8040\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0910 - acc: 0.9615 - val_loss: 1.2033 - val_acc: 0.7996\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0901 - acc: 0.9617 - val_loss: 1.2393 - val_acc: 0.8029\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0922 - acc: 0.9603 - val_loss: 1.2570 - val_acc: 0.7940\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0905 - acc: 0.9614 - val_loss: 1.2436 - val_acc: 0.8007\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0906 - acc: 0.9595 - val_loss: 1.2590 - val_acc: 0.7996\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0905 - acc: 0.9603 - val_loss: 1.2623 - val_acc: 0.7984\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0864 - acc: 0.9614 - val_loss: 1.3440 - val_acc: 0.7973\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0866 - acc: 0.9614 - val_loss: 1.3101 - val_acc: 0.7940\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0844 - acc: 0.9618 - val_loss: 1.3245 - val_acc: 0.7940\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0845 - acc: 0.9625 - val_loss: 1.3228 - val_acc: 0.7962\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0848 - acc: 0.9617 - val_loss: 1.3690 - val_acc: 0.7996\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0826 - acc: 0.9620 - val_loss: 1.3237 - val_acc: 0.8007\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0845 - acc: 0.9610 - val_loss: 1.3772 - val_acc: 0.7918\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0831 - acc: 0.9623 - val_loss: 1.3355 - val_acc: 0.7951\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0801 - acc: 0.9620 - val_loss: 1.3607 - val_acc: 0.7973\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0802 - acc: 0.9635 - val_loss: 1.3577 - val_acc: 0.7918\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0789 - acc: 0.9626 - val_loss: 1.3671 - val_acc: 0.7973\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0805 - acc: 0.9600 - val_loss: 1.3740 - val_acc: 0.7951\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0776 - acc: 0.9626 - val_loss: 1.4191 - val_acc: 0.7895\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0792 - acc: 0.9620 - val_loss: 1.4342 - val_acc: 0.7929\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0772 - acc: 0.9631 - val_loss: 1.4003 - val_acc: 0.8018\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0783 - acc: 0.9618 - val_loss: 1.3970 - val_acc: 0.7962\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0779 - acc: 0.9607 - val_loss: 1.4504 - val_acc: 0.7840\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0790 - acc: 0.9605 - val_loss: 1.3903 - val_acc: 0.8007\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0767 - acc: 0.9617 - val_loss: 1.4044 - val_acc: 0.7951\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0772 - acc: 0.9625 - val_loss: 1.4354 - val_acc: 0.7906\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0787 - acc: 0.9600 - val_loss: 1.4280 - val_acc: 0.7962\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0755 - acc: 0.9610 - val_loss: 1.4666 - val_acc: 0.7973\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0737 - acc: 0.9623 - val_loss: 1.4819 - val_acc: 0.7906\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0773 - acc: 0.9603 - val_loss: 1.4544 - val_acc: 0.7940\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0735 - acc: 0.9631 - val_loss: 1.4479 - val_acc: 0.7962\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0760 - acc: 0.9609 - val_loss: 1.4788 - val_acc: 0.7906\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0744 - acc: 0.9605 - val_loss: 1.4945 - val_acc: 0.7962\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0726 - acc: 0.9618 - val_loss: 1.4731 - val_acc: 0.7895\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0726 - acc: 0.9633 - val_loss: 1.5062 - val_acc: 0.7895\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0728 - acc: 0.9617 - val_loss: 1.4915 - val_acc: 0.7895\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0719 - acc: 0.9619 - val_loss: 1.4848 - val_acc: 0.7851\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0720 - acc: 0.9617 - val_loss: 1.5262 - val_acc: 0.7862\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0699 - acc: 0.9624 - val_loss: 1.5316 - val_acc: 0.7829\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0716 - acc: 0.9610 - val_loss: 1.5003 - val_acc: 0.7873\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0705 - acc: 0.9593 - val_loss: 1.4890 - val_acc: 0.7851\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0686 - acc: 0.9612 - val_loss: 1.5626 - val_acc: 0.7873\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0705 - acc: 0.9599 - val_loss: 1.6283 - val_acc: 0.7851\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0695 - acc: 0.9624 - val_loss: 1.5601 - val_acc: 0.7873\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0711 - acc: 0.9600 - val_loss: 1.5419 - val_acc: 0.7906\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0688 - acc: 0.9624 - val_loss: 1.5579 - val_acc: 0.7918\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0688 - acc: 0.9614 - val_loss: 1.5670 - val_acc: 0.7806\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0674 - acc: 0.9628 - val_loss: 1.5732 - val_acc: 0.7918\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0695 - acc: 0.9607 - val_loss: 1.6073 - val_acc: 0.7795\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0674 - acc: 0.9615 - val_loss: 1.5916 - val_acc: 0.7940\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0668 - acc: 0.9623 - val_loss: 1.6077 - val_acc: 0.7762\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0667 - acc: 0.9617 - val_loss: 1.6009 - val_acc: 0.7840\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0666 - acc: 0.9612 - val_loss: 1.6081 - val_acc: 0.7840\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0683 - acc: 0.9605 - val_loss: 1.6264 - val_acc: 0.7762\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0682 - acc: 0.9619 - val_loss: 1.6228 - val_acc: 0.7784\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0638 - acc: 0.9634 - val_loss: 1.6406 - val_acc: 0.7751\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0665 - acc: 0.9614 - val_loss: 1.6554 - val_acc: 0.7784\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0653 - acc: 0.9615 - val_loss: 1.6070 - val_acc: 0.7840\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0652 - acc: 0.9617 - val_loss: 1.6131 - val_acc: 0.7773\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0670 - acc: 0.9624 - val_loss: 1.6313 - val_acc: 0.7795\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0647 - acc: 0.9604 - val_loss: 1.6430 - val_acc: 0.7840\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0650 - acc: 0.9612 - val_loss: 1.6689 - val_acc: 0.7829\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0634 - acc: 0.9605 - val_loss: 1.6661 - val_acc: 0.7751\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0642 - acc: 0.9626 - val_loss: 1.6780 - val_acc: 0.7862\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0636 - acc: 0.9638 - val_loss: 1.6430 - val_acc: 0.7706\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0647 - acc: 0.9631 - val_loss: 1.6860 - val_acc: 0.7717\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0648 - acc: 0.9592 - val_loss: 1.6927 - val_acc: 0.7806\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0632 - acc: 0.9621 - val_loss: 1.6960 - val_acc: 0.7795\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0642 - acc: 0.9602 - val_loss: 1.7240 - val_acc: 0.7751\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0618 - acc: 0.9625 - val_loss: 1.7467 - val_acc: 0.7751\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0625 - acc: 0.9633 - val_loss: 1.6864 - val_acc: 0.7706\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0620 - acc: 0.9620 - val_loss: 1.7313 - val_acc: 0.7795\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0637 - acc: 0.9605 - val_loss: 1.7222 - val_acc: 0.7829\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0609 - acc: 0.9620 - val_loss: 1.6911 - val_acc: 0.7817\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0630 - acc: 0.9612 - val_loss: 1.7565 - val_acc: 0.7684\n",
            "처리중인 폴드 # 8\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 3s 386us/step - loss: 2.6586 - acc: 0.5077 - val_loss: 1.6784 - val_acc: 0.6637\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 1.4537 - acc: 0.6988 - val_loss: 1.2443 - val_acc: 0.7350\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 1.0710 - acc: 0.7702 - val_loss: 1.0631 - val_acc: 0.7706\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.8330 - acc: 0.8236 - val_loss: 0.9672 - val_acc: 0.7862\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.6560 - acc: 0.8597 - val_loss: 0.9366 - val_acc: 0.7940\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.5249 - acc: 0.8902 - val_loss: 0.8807 - val_acc: 0.8107\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.4280 - acc: 0.9119 - val_loss: 0.8517 - val_acc: 0.8051\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.3448 - acc: 0.9289 - val_loss: 0.8622 - val_acc: 0.8118\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.2870 - acc: 0.9381 - val_loss: 0.8401 - val_acc: 0.8140\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.2442 - acc: 0.9441 - val_loss: 0.8389 - val_acc: 0.8185\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.2107 - acc: 0.9484 - val_loss: 0.8501 - val_acc: 0.8196\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.1828 - acc: 0.9525 - val_loss: 0.8564 - val_acc: 0.8151\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1668 - acc: 0.9550 - val_loss: 0.9328 - val_acc: 0.7951\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1531 - acc: 0.9548 - val_loss: 0.8934 - val_acc: 0.8107\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.1397 - acc: 0.9568 - val_loss: 0.8939 - val_acc: 0.8118\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1327 - acc: 0.9567 - val_loss: 0.9384 - val_acc: 0.7973\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1283 - acc: 0.9578 - val_loss: 0.9440 - val_acc: 0.8118\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1201 - acc: 0.9583 - val_loss: 0.9474 - val_acc: 0.8051\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1137 - acc: 0.9579 - val_loss: 0.9785 - val_acc: 0.8073\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1103 - acc: 0.9592 - val_loss: 0.9972 - val_acc: 0.8007\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.1107 - acc: 0.9570 - val_loss: 1.0428 - val_acc: 0.8040\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.1086 - acc: 0.9592 - val_loss: 1.0063 - val_acc: 0.8118\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1023 - acc: 0.9592 - val_loss: 1.0221 - val_acc: 0.8062\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0994 - acc: 0.9617 - val_loss: 1.1103 - val_acc: 0.8029\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1012 - acc: 0.9568 - val_loss: 1.0733 - val_acc: 0.7962\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0981 - acc: 0.9589 - val_loss: 1.1112 - val_acc: 0.7984\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0999 - acc: 0.9594 - val_loss: 1.0920 - val_acc: 0.7973\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0966 - acc: 0.9604 - val_loss: 1.1224 - val_acc: 0.7996\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0969 - acc: 0.9593 - val_loss: 1.1533 - val_acc: 0.8018\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0962 - acc: 0.9587 - val_loss: 1.1182 - val_acc: 0.7984\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0937 - acc: 0.9600 - val_loss: 1.1491 - val_acc: 0.7829\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0974 - acc: 0.9584 - val_loss: 1.1693 - val_acc: 0.7973\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0929 - acc: 0.9592 - val_loss: 1.1633 - val_acc: 0.8018\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0955 - acc: 0.9583 - val_loss: 1.1590 - val_acc: 0.7929\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0939 - acc: 0.9576 - val_loss: 1.1938 - val_acc: 0.8007\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0891 - acc: 0.9602 - val_loss: 1.1922 - val_acc: 0.7951\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0893 - acc: 0.9576 - val_loss: 1.2274 - val_acc: 0.7862\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0906 - acc: 0.9592 - val_loss: 1.1743 - val_acc: 0.7984\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0866 - acc: 0.9594 - val_loss: 1.2289 - val_acc: 0.7895\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0884 - acc: 0.9583 - val_loss: 1.2279 - val_acc: 0.7984\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0889 - acc: 0.9593 - val_loss: 1.2178 - val_acc: 0.7962\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0868 - acc: 0.9599 - val_loss: 1.2705 - val_acc: 0.7929\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0880 - acc: 0.9588 - val_loss: 1.2633 - val_acc: 0.7873\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0844 - acc: 0.9595 - val_loss: 1.3486 - val_acc: 0.7817\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0884 - acc: 0.9595 - val_loss: 1.2904 - val_acc: 0.7840\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0846 - acc: 0.9618 - val_loss: 1.2692 - val_acc: 0.7873\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0824 - acc: 0.9605 - val_loss: 1.3044 - val_acc: 0.7773\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0838 - acc: 0.9595 - val_loss: 1.2596 - val_acc: 0.7806\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0847 - acc: 0.9600 - val_loss: 1.3577 - val_acc: 0.7806\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0810 - acc: 0.9615 - val_loss: 1.2549 - val_acc: 0.7873\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0856 - acc: 0.9581 - val_loss: 1.3161 - val_acc: 0.7851\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0796 - acc: 0.9624 - val_loss: 1.3246 - val_acc: 0.7817\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0832 - acc: 0.9602 - val_loss: 1.3053 - val_acc: 0.7817\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0803 - acc: 0.9607 - val_loss: 1.3087 - val_acc: 0.7829\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0811 - acc: 0.9576 - val_loss: 1.3424 - val_acc: 0.7784\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0817 - acc: 0.9574 - val_loss: 1.3491 - val_acc: 0.7795\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0815 - acc: 0.9595 - val_loss: 1.4113 - val_acc: 0.7739\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0782 - acc: 0.9593 - val_loss: 1.4007 - val_acc: 0.7728\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0801 - acc: 0.9577 - val_loss: 1.3606 - val_acc: 0.7751\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0789 - acc: 0.9593 - val_loss: 1.3902 - val_acc: 0.7873\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0803 - acc: 0.9577 - val_loss: 1.4245 - val_acc: 0.7751\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0757 - acc: 0.9588 - val_loss: 1.3958 - val_acc: 0.7728\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0794 - acc: 0.9576 - val_loss: 1.4517 - val_acc: 0.7695\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0767 - acc: 0.9586 - val_loss: 1.3616 - val_acc: 0.7918\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0761 - acc: 0.9582 - val_loss: 1.4274 - val_acc: 0.7795\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0764 - acc: 0.9597 - val_loss: 1.3827 - val_acc: 0.7806\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0737 - acc: 0.9602 - val_loss: 1.4246 - val_acc: 0.7728\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0790 - acc: 0.9598 - val_loss: 1.4573 - val_acc: 0.7773\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0762 - acc: 0.9588 - val_loss: 1.4582 - val_acc: 0.7739\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0745 - acc: 0.9593 - val_loss: 1.4255 - val_acc: 0.7829\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0721 - acc: 0.9594 - val_loss: 1.4789 - val_acc: 0.7784\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0741 - acc: 0.9599 - val_loss: 1.4573 - val_acc: 0.7739\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0754 - acc: 0.9586 - val_loss: 1.4643 - val_acc: 0.7784\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0727 - acc: 0.9607 - val_loss: 1.4732 - val_acc: 0.7728\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0722 - acc: 0.9589 - val_loss: 1.4589 - val_acc: 0.7728\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0725 - acc: 0.9602 - val_loss: 1.5193 - val_acc: 0.7650\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0717 - acc: 0.9597 - val_loss: 1.4825 - val_acc: 0.7739\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0722 - acc: 0.9586 - val_loss: 1.4761 - val_acc: 0.7795\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0715 - acc: 0.9599 - val_loss: 1.4919 - val_acc: 0.7717\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0701 - acc: 0.9583 - val_loss: 1.4800 - val_acc: 0.7751\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0724 - acc: 0.9610 - val_loss: 1.5192 - val_acc: 0.7706\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0701 - acc: 0.9589 - val_loss: 1.5300 - val_acc: 0.7739\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0701 - acc: 0.9602 - val_loss: 1.4982 - val_acc: 0.7695\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0696 - acc: 0.9588 - val_loss: 1.5661 - val_acc: 0.7751\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0703 - acc: 0.9604 - val_loss: 1.5890 - val_acc: 0.7684\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0705 - acc: 0.9582 - val_loss: 1.5842 - val_acc: 0.7661\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0703 - acc: 0.9570 - val_loss: 1.5212 - val_acc: 0.7762\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0674 - acc: 0.9605 - val_loss: 1.5877 - val_acc: 0.7717\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0696 - acc: 0.9600 - val_loss: 1.5730 - val_acc: 0.7706\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0678 - acc: 0.9598 - val_loss: 1.6008 - val_acc: 0.7706\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0678 - acc: 0.9600 - val_loss: 1.6410 - val_acc: 0.7706\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0671 - acc: 0.9602 - val_loss: 1.6011 - val_acc: 0.7739\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0676 - acc: 0.9595 - val_loss: 1.6110 - val_acc: 0.7684\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0675 - acc: 0.9582 - val_loss: 1.6667 - val_acc: 0.7639\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0673 - acc: 0.9602 - val_loss: 1.6543 - val_acc: 0.7695\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0669 - acc: 0.9588 - val_loss: 1.6651 - val_acc: 0.7728\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0670 - acc: 0.9599 - val_loss: 1.6443 - val_acc: 0.7673\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0670 - acc: 0.9605 - val_loss: 1.7108 - val_acc: 0.7606\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0659 - acc: 0.9592 - val_loss: 1.6684 - val_acc: 0.7628\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0664 - acc: 0.9576 - val_loss: 1.7140 - val_acc: 0.7617\n",
            "처리중인 폴드 # 9\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 3s 375us/step - loss: 2.6670 - acc: 0.5447 - val_loss: 1.8258 - val_acc: 0.6514\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 1.4060 - acc: 0.7130 - val_loss: 1.3916 - val_acc: 0.7038\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 1.0225 - acc: 0.7851 - val_loss: 1.2251 - val_acc: 0.7439\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.7929 - acc: 0.8383 - val_loss: 1.1494 - val_acc: 0.7483\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.6292 - acc: 0.8721 - val_loss: 1.0511 - val_acc: 0.7784\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.4991 - acc: 0.9003 - val_loss: 1.0275 - val_acc: 0.7817\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.4032 - acc: 0.9171 - val_loss: 1.0193 - val_acc: 0.7862\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.3293 - acc: 0.9290 - val_loss: 1.0237 - val_acc: 0.7817\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.2713 - acc: 0.9411 - val_loss: 0.9984 - val_acc: 0.7784\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.2339 - acc: 0.9447 - val_loss: 1.0359 - val_acc: 0.7973\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.1978 - acc: 0.9506 - val_loss: 1.0200 - val_acc: 0.8018\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.1773 - acc: 0.9537 - val_loss: 1.0373 - val_acc: 0.7884\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1593 - acc: 0.9557 - val_loss: 1.0900 - val_acc: 0.7862\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1477 - acc: 0.9568 - val_loss: 1.1589 - val_acc: 0.7851\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.1398 - acc: 0.9565 - val_loss: 1.1154 - val_acc: 0.7906\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1311 - acc: 0.9574 - val_loss: 1.1411 - val_acc: 0.7906\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1227 - acc: 0.9576 - val_loss: 1.1685 - val_acc: 0.7851\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.1108 - acc: 0.9602 - val_loss: 1.1762 - val_acc: 0.7817\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.1118 - acc: 0.9612 - val_loss: 1.2365 - val_acc: 0.7806\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.1084 - acc: 0.9598 - val_loss: 1.2305 - val_acc: 0.7773\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.1067 - acc: 0.9603 - val_loss: 1.1949 - val_acc: 0.7906\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.1027 - acc: 0.9603 - val_loss: 1.2924 - val_acc: 0.7817\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.1055 - acc: 0.9587 - val_loss: 1.2795 - val_acc: 0.7862\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0965 - acc: 0.9600 - val_loss: 1.3983 - val_acc: 0.7650\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.1020 - acc: 0.9586 - val_loss: 1.3244 - val_acc: 0.7829\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0975 - acc: 0.9584 - val_loss: 1.2732 - val_acc: 0.7784\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0932 - acc: 0.9624 - val_loss: 1.3716 - val_acc: 0.7784\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0957 - acc: 0.9600 - val_loss: 1.3632 - val_acc: 0.7784\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0963 - acc: 0.9588 - val_loss: 1.3608 - val_acc: 0.7884\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 71us/step - loss: 0.0930 - acc: 0.9595 - val_loss: 1.4015 - val_acc: 0.7717\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0937 - acc: 0.9612 - val_loss: 1.4011 - val_acc: 0.7762\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0906 - acc: 0.9608 - val_loss: 1.4398 - val_acc: 0.7706\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0912 - acc: 0.9594 - val_loss: 1.4827 - val_acc: 0.7650\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0873 - acc: 0.9600 - val_loss: 1.4663 - val_acc: 0.7751\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0895 - acc: 0.9597 - val_loss: 1.4787 - val_acc: 0.7650\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0874 - acc: 0.9604 - val_loss: 1.4083 - val_acc: 0.7851\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0917 - acc: 0.9588 - val_loss: 1.4204 - val_acc: 0.7751\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0878 - acc: 0.9604 - val_loss: 1.5298 - val_acc: 0.7739\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0861 - acc: 0.9593 - val_loss: 1.5470 - val_acc: 0.7639\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0842 - acc: 0.9597 - val_loss: 1.4951 - val_acc: 0.7650\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0864 - acc: 0.9589 - val_loss: 1.4917 - val_acc: 0.7728\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0839 - acc: 0.9605 - val_loss: 1.4589 - val_acc: 0.7784\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0857 - acc: 0.9591 - val_loss: 1.5421 - val_acc: 0.7717\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0849 - acc: 0.9602 - val_loss: 1.5483 - val_acc: 0.7717\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0818 - acc: 0.9598 - val_loss: 1.5568 - val_acc: 0.7728\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0813 - acc: 0.9593 - val_loss: 1.5993 - val_acc: 0.7650\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0858 - acc: 0.9591 - val_loss: 1.5398 - val_acc: 0.7706\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0807 - acc: 0.9598 - val_loss: 1.6100 - val_acc: 0.7606\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0815 - acc: 0.9583 - val_loss: 1.5873 - val_acc: 0.7572\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0809 - acc: 0.9602 - val_loss: 1.5272 - val_acc: 0.7773\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0799 - acc: 0.9618 - val_loss: 1.6267 - val_acc: 0.7572\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0794 - acc: 0.9608 - val_loss: 1.6674 - val_acc: 0.7450\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0794 - acc: 0.9612 - val_loss: 1.6001 - val_acc: 0.7606\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0790 - acc: 0.9608 - val_loss: 1.5828 - val_acc: 0.7717\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0790 - acc: 0.9623 - val_loss: 1.6163 - val_acc: 0.7639\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0784 - acc: 0.9597 - val_loss: 1.6147 - val_acc: 0.7639\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0779 - acc: 0.9610 - val_loss: 1.6443 - val_acc: 0.7695\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0796 - acc: 0.9592 - val_loss: 1.6641 - val_acc: 0.7550\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0759 - acc: 0.9615 - val_loss: 1.6646 - val_acc: 0.7617\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0765 - acc: 0.9623 - val_loss: 1.6451 - val_acc: 0.7650\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0761 - acc: 0.9618 - val_loss: 1.6821 - val_acc: 0.7639\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0781 - acc: 0.9581 - val_loss: 1.6698 - val_acc: 0.7617\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0740 - acc: 0.9600 - val_loss: 1.6512 - val_acc: 0.7639\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0736 - acc: 0.9598 - val_loss: 1.6800 - val_acc: 0.7617\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0750 - acc: 0.9609 - val_loss: 1.7045 - val_acc: 0.7639\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0730 - acc: 0.9593 - val_loss: 1.7127 - val_acc: 0.7595\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0751 - acc: 0.9597 - val_loss: 1.7285 - val_acc: 0.7639\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0729 - acc: 0.9621 - val_loss: 1.7845 - val_acc: 0.7461\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0725 - acc: 0.9598 - val_loss: 1.7192 - val_acc: 0.7606\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0709 - acc: 0.9605 - val_loss: 1.6642 - val_acc: 0.7617\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0742 - acc: 0.9610 - val_loss: 1.7111 - val_acc: 0.7639\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0716 - acc: 0.9614 - val_loss: 1.7905 - val_acc: 0.7472\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0724 - acc: 0.9578 - val_loss: 1.7503 - val_acc: 0.7595\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0720 - acc: 0.9594 - val_loss: 1.7551 - val_acc: 0.7528\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0710 - acc: 0.9617 - val_loss: 1.8322 - val_acc: 0.7416\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0695 - acc: 0.9625 - val_loss: 1.7578 - val_acc: 0.7572\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 75us/step - loss: 0.0709 - acc: 0.9603 - val_loss: 1.8034 - val_acc: 0.7528\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0706 - acc: 0.9610 - val_loss: 1.8103 - val_acc: 0.7561\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0704 - acc: 0.9583 - val_loss: 1.8033 - val_acc: 0.7506\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0701 - acc: 0.9610 - val_loss: 1.7779 - val_acc: 0.7506\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 0.0678 - acc: 0.9621 - val_loss: 1.7657 - val_acc: 0.7561\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0685 - acc: 0.9630 - val_loss: 1.7952 - val_acc: 0.7517\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0707 - acc: 0.9599 - val_loss: 1.7927 - val_acc: 0.7517\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 76us/step - loss: 0.0687 - acc: 0.9617 - val_loss: 1.8371 - val_acc: 0.7550\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0682 - acc: 0.9595 - val_loss: 1.8179 - val_acc: 0.7517\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0688 - acc: 0.9609 - val_loss: 1.8207 - val_acc: 0.7450\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 78us/step - loss: 0.0675 - acc: 0.9626 - val_loss: 1.8395 - val_acc: 0.7517\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0678 - acc: 0.9600 - val_loss: 1.8203 - val_acc: 0.7572\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0681 - acc: 0.9577 - val_loss: 1.9308 - val_acc: 0.7450\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0667 - acc: 0.9602 - val_loss: 1.8538 - val_acc: 0.7483\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0675 - acc: 0.9603 - val_loss: 1.8502 - val_acc: 0.7494\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0668 - acc: 0.9618 - val_loss: 1.8782 - val_acc: 0.7517\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 73us/step - loss: 0.0664 - acc: 0.9614 - val_loss: 1.8184 - val_acc: 0.7561\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0666 - acc: 0.9599 - val_loss: 1.9320 - val_acc: 0.7472\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0656 - acc: 0.9609 - val_loss: 1.8946 - val_acc: 0.7394\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0673 - acc: 0.9599 - val_loss: 1.8663 - val_acc: 0.7439\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 72us/step - loss: 0.0648 - acc: 0.9612 - val_loss: 1.9693 - val_acc: 0.7372\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0662 - acc: 0.9605 - val_loss: 1.8786 - val_acc: 0.7383\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 74us/step - loss: 0.0654 - acc: 0.9608 - val_loss: 1.9009 - val_acc: 0.7450\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 77us/step - loss: 0.0655 - acc: 0.9604 - val_loss: 1.9277 - val_acc: 0.7405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X8908muCWdnG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Plotting the training and validation accuracy\n",
        "  * To obtain the validation accuracy at the end of every epoch, just average the performances of all folds."
      ]
    },
    {
      "metadata": {
        "id": "FagMBVf8pWyo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6PJb3RyXXZlC",
        "colab_type": "code",
        "outputId": "21205c61-23ca-44db-b0c1-3bf9d0fe4955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# write a code for plotting the training and validation accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 모든 폴드의 val_acc 평균\n",
        "average_val_acc = [\n",
        "    np.mean([x[i] for x in all_val_acc]) for i in range(num_epochs)]\n",
        "#모든 폴드의 train_acc 평균\n",
        "average_train_acc = [\n",
        "    np.mean([x[i] for x in all_train_acc]) for i in range(num_epochs)]\n",
        "\n",
        "plt.plot(range(1, len(average_train_acc) + 1), average_train_acc, 'bo', label = 'Average Training acc')\n",
        "plt.plot(range(1, len(average_val_acc) + 1), average_val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation Average accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.legend()\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd4FNXixvHvliQQEkpCQkcplxYE\nRYqIGAjBEIoaEEEsYLnoRX+CBQuKSIerKKBeRCxXlF4CIhCKgDQvSJFeBAWkBwKRkJDNlt8faxZi\nEhLIbpJN3s/z8JCZ3Zk5e1LeOXPOnDE4HA4HIiIi4jWMBV0AERERuTEKbxERES+j8BYREfEyCm8R\nEREvo/AWERHxMgpvERERL6PwlnwzZMgQOnToQIcOHQgLC6Nt27au5aSkpBvaV4cOHTh37tx13zNu\n3DhmzJiRlyK7XZ8+fZg/f75b9lW3bl1Onz7NihUrePPNN/N0vNmzZ7u+zk3d3qiDBw/StGlTJk2a\n5Nb9ihRX5oIugBQfQ4cOdX0dERHBv//9b5o2bXpT+4qLi8vxPa+88spN7dvbtG/fnvbt29/09vHx\n8Xz++ec8/PDDQO7q9kbFxsbSv39/Zs6cyb/+9S+371+kuFHLWwqNxx9/nA8//JDo6Gi2bdvGuXPn\nePrpp+nQoQMRERF89dVXrvemtzo3bdpEjx49GDduHNHR0URERLB582YA3njjDf7zn/8AzpOFmTNn\n8tBDD3HPPfcwZswY174+/fRTWrZsSbdu3Zg2bRoRERFZlm/OnDlER0dz33338eijj3LixAkA5s+f\nz4svvsigQYOIioqiY8eO/PrrrwD88ccfdO/encjISF555RVsNlum/f7444906dIlw7oHHniAtWvX\nXrcO0s2fP58+ffrkeLwffviBLl26EBUVRdeuXdm3bx8APXv25OTJk3To0AGLxeKqW4CpU6fSsWNH\nOnTowL/+9S8SEhJcdTtx4kSefPJJ2rZty5NPPklKSkqW9Waz2Vi5ciVdu3alYsWK7NixA4BDhw7R\nvHlzrFar6739+vVjxowZWCwWRowYQVRUFBEREXz66aeu90RERPDxxx8TFRXFyZMn+e2333jkkUeI\njo6mffv2fP/99xnqplWrVtx///3Mnz+funXrAuBwOFz7aNu2LSNGjMjye2O32xk6dKirHAMHDiQt\nLQ2AhIQEnnvuOdq1a0eXLl1Yv379ddc//vjjLFy40LXva5fr1q3L5MmTiYqKwmazsX37drp27UqH\nDh3o2LEjGzdudG23YMECoqKiiIqKYuDAgVgsFrp165bhpGv16tU88MADWX4/pGhQeEuhsnv3bhYv\nXkyTJk2YNGkSVatWJS4ujq+//ppx48Zx6tSpTNvs3buXxo0bs3TpUnr16pXtpdmff/6ZWbNmMW/e\nPL799ltOnz7Nr7/+yueff87ChQuZPn16tq3O8+fPM2zYML766iuWL19O9erVXScGAGvXrqVXr14s\nW7aMFi1a8PXXXwPw/vvv07JlS1auXEnv3r3Ztm1bpn23bNmS06dP88cffwDOAD59+jR33313rusg\nXXbHs1qtvPHGGwwfPpxly5YRERHB2LFjARg1ahSVKlUiLi4OX19f175++eUXvvjiC7755hvi4uKo\nXLky48aNc70eFxfHhx9+yIoVK0hISGDFihVZlmndunU0btyYUqVK0aVLFxYsWABA7dq1KV++PFu2\nbAEgJSWF//3vf0RFRTFlyhQOHTrEokWL+P7771m2bBmrV6927fPMmTMsW7aMypUr8+9//5u2bduy\ndOlSRo0axVtvvUVaWhoXL15k6NChfPXVVyxYsMAVogALFy4kLi6OuXPnsmLFCv74448su1hWrFjB\nli1b+P7771m6dCl79uxhyZIlgLNbplatWvzwww+MHTuWV155BYvFku36nDgcDpYtW4bJZOKdd97h\n6aefJi4ujr59+zJkyBAAjh8/ztixY5k6dSpxcXGkpKQwdepUOnfunOGkZcWKFXTq1CnHY4r3UnhL\noRIeHo7R6PyxfPvttxk8eDAA1apVIyQkhOPHj2faplSpUkRGRgIQFhbGyZMns9x3ly5dMJlMVKhQ\ngeDgYE6dOsXPP/9M8+bNCQ0Nxc/Pj27dumW5bXBwMFu3bqVixYoANG3a1BW2ALVq1aJhw4YANGjQ\nwBWwW7ZsoWPHjgA0atSImjVrZtq3r68vbdu2ZdWqVQCsXLmSyMhIzGZzrusgXXbHM5vNbNy4kdtv\nvz3L8mdlzZo1REVFERwcDED37t3ZsGGD6/Xw8HDKli2L2WymTp062Z5UxMbGcv/99wPOS/yrV692\nhVlUVJTrc69bt45GjRoRFBTE6tWr6dWrF76+vvj7+/PAAw+wfPly1z7btGnj+vo///kPTz/9NAB3\n3nknqampxMfHs2PHDm699Vbq1KmD0WjkkUcecW2zevVqunXrRmBgIGazme7du2fYf7qoqCjmzZuH\nj48Pfn5+3Hbbba56+/HHH+ncuTPg/J7/8MMP+Pr6Zrs+J9d+pgULFhAdHe36TOnH3LBhA3fccQcV\nKlTAYDAwbtw4+vTpQ8eOHVm3bh2XLl3CZrOxevVq1/ZSNKnPWwqVMmXKuL7etWuXq6VpNBqJj4/H\nbrdn2iYwMND1tdFozPI9AAEBAa6vTSYTNpuNP//8M8MxK1SokOW2NpuNiRMnsmrVKmw2G5cvX6ZG\njRpZliF93wCJiYkZjlu6dOks9x8VFcXUqVPp3bs3K1eupF+/fjdUB+mud7xvvvmG2NhYLBYLFosF\ng8GQ7X7Aefk3NDQ0w77Onz+f42f+e3nWrFmTIfSvXLnCmjVruO+++4iKiuKFF15g0KBBrFy50nXi\ncenSJUaPHs0HH3wAgMVioVGjRq59XPs9W7duHZMmTeLChQsYDAYcDgd2u/2639tLly7xxRdfMGvW\nLMD5/Q0KCsqyDoYPH87evXsxGAycO3eO3r17A3Dx4sUMdZBe79mtz0nZsmVdXy9atIipU6dy+fJl\n7HY76Y+guHDhQobvqZ+fn+uzNWrUyHVVqEqVKlSrVi1XxxXvpPCWQmvgwIH07t2bRx55BIPBQOvW\nrd1+jICAAJKTk13LZ8+ezfJ9S5YsYdWqVXz77bcEBQUxe/ZsFi1alOP+S5cunWEkfXqf8d+1bt2a\nQYMGceTIEY4cOcJdd90F3HgdZHe8bdu2MWXKFObMmUPVqlXZsGGDq0WfnfLly3Px4kXX8sWLFylf\nvvz1P/DfLF68mAceeIBhw4a51q1YsYLY2Fjuu+8+6tWrh8lkYv/+/axfv941aj40NJSnnnqKtm3b\nXnf/aWlpDBgwgPHjxxMeHp4h5K/3vQ0NDSUiIoLHHnvsuvv/8MMPMZvNLFq0CF9f3wyDIMuWLcuF\nCxeoWrUq4LykXaFChWzX//3EMjExMctjnjlzhrfffps5c+ZQv359jhw5QlRUFADlypVj+/btrvcm\nJSVx5coVypcvT6dOnYiLi+OWW25xnQRJ0aXL5lJonT9/noYNG2IwGIiNjSUlJSXDH2N3aNSoEZs2\nbSIhIQGLxeLqj82qLFWqVCEoKIgLFy6wdOlSLl++nOP+b7/9dldf8LZt2zh27FiW7/P19eWee+7h\nvffeo127dphMJtdxb6QOsjteQkICwcHBVK5cmZSUFGJjY0lOTsbhcGA2m0lOTs4wcAycl3FXrFjB\nhQsXAJg5cybh4eE5fuZrxcbGuro00t1zzz1s3rzZtd+oqCg++ugj6tevT7ly5QBo164dc+bMwWaz\n4XA4+M9//sPatWsz7T+9PtK7LL7++mt8fHxITk4mLCyMAwcOcPToUex2O3PnznVt165dOxYuXOga\nZDdz5kxiY2Mz7f/8+fPUqVMHX19f9u/fz/bt2131HxER4drm0KFDdO3aFZvNlu36kJAQ9u/fD8D2\n7ds5cuRIlnWWkJCAv78/NWvWxGq1uq4OXL58mfDwcLZt28bx48dxOBwMGTLE9bk6dOjA1q1biYuL\n0yXzYkDhLYVW//79ef755+nSpQvJycn06NGDwYMHZxuAN6NRo0bExMQQExPDE088kW1Lr3Pnzly8\neJH27dvzyiuvMGDAAE6fPp1h1HpWBg4cyOrVq4mMjGTatGncfffd2b43KiqKlStXZvjDe6N1kN3x\nWrduTWhoKJGRkTz11FP07t2bwMBAXnzxRerWrUuZMmVo1apVhvECjRo1om/fvjz66KN06NCBS5cu\n8dJLL133817r8OHD/Pbbb66rCOlKlixJ8+bNWbx4cYbP3aFDB9d7evXqReXKlenUqRMdOnTg8OHD\n3HnnnZmOUbp0aZ555hkefPBBHnzwQapXr05kZCTPPfccAQEBvPzyyzzxxBN07949w/aRkZG0bduW\nmJgYOnTowKpVq7jnnnsy7f+pp55i5syZREdHM23aNF5//XXmzJnD0qVLGThwIKdPnyYiIoKXXnqJ\n999/nxIlSmS7/sknn2TNmjVER0ezYMECWrVqlWW91atXj3vvvZeoqCh69OhBREQEt99+O48//jgV\nK1Zk2LBh9O7d29Uaf/LJJwHnlYBmzZpRtWpVKlWqlOvvk3gng57nLcWdw+Fw9f+uWbOG8ePHZ9sC\nF+9y7ff2119/pVevXvz8888FXCrPeffdd/nHP/7Bo48+WtBFEQ9Ty1uKtYSEBO666y5OnDiBw+Fg\n6dKlrhHZ4t2sViutW7d23Ve+ZMmSIv29PXLkCGvXrnWN7JeiTQPWpFgLCgpiwIAB9OnTB4PBQM2a\nNXnttdcKuljiBmazmSFDhvD666/jcDgICQlh5MiRBV0sj5gwYQILFy5k8ODBGUa6S9Gly+YiIiJe\nRpfNRUREvIzCW0RExMt4TZ93fPylPG1frpw/Fy649x7h4kj16B6qR/dQPbqH6tE9PFGPISFZj2Eo\nNi1vs9lU0EUoElSP7qF6dA/Vo3uoHt0jP+ux2IS3iIhIUaHwFhER8TIKbxERES+j8BYREfEyCm8R\nEREvo/AWERHxMgpvERERL6PwFhER8TIKbxGRfBAbayY83J9KlQIID/cnNtac5TrJyBN1VBTq3Wue\nKpbX6VFDQgLzvI/iJDbWzPjxvhw8aKROHTsDBliIibFetx6z2ya711u1srFhg+mm33+jywMGWADc\nus+bXzZRp46tkJdR9eiu5QoVHJw8mbu2UpUqdk6fNqger1Nv19bRjZYxN/vMaz3+/W9ZXmQ3ParC\nu5i4kSC8/g+3Mctf8latbHz+uW822xhy/cfrRt8vIlIYTZ6c4pYAV3gX8fC+XjgrCEVE8leDBjbW\nrMn7Q0oU3kUsvK8Na4WziEjhYjY7OHkyKc/7yS68va+Xvpi6XlifPGkowJKJiMjf1alj9+j+1Vzz\nArGxZp59tiT79pmw2QxqZYt4qapV7ZjNDho0sDF5cgqTJ6fQoIENs9lBlSqe/WPvzdLrzZ115Il9\nXqt/f4tH9pvO9O67777r0SO4SXJy3iqiVCm/PO8jv8TGmnnuuRIMGuTHokVmli83c+lS/reuq1a1\nk5wMlSo5cnX8Z56xcOUKXLhgyHab9H3Wq2fngQesN/3+m1keMSKVLl2s/Pab8ab34b5lI/Xq2Qp5\nGVWP7q638eNTeeUVC336pFG/vp369e306ZPGK69YeO65NGrXthdQmQtvPV5bb1nV0c2UMad95rUe\nR4xIddto81Kl/LJcrz7vQia9le0JVatevRXi7rttbNx4dbT435f7989829aECb6uWyFyen/GbbJ/\nT17e78285eexsFM9uofq0T08UY8asOYlP5zh4f7s22e6qW2vF87uCkJvqcfCTvXoHqpH91A9ukd+\nhrcGrBUC1w5Gs9lyv921YV2UW6kiIpKRwruA5fYyeZUqdsqUcRSLS8oiInJ9Cu8CNn585lnJsvLO\nO+4bACEiIt5N9xwVsIMHs/sWODLcUqLgFhGRdGp5F4Br+7jNZrLs527QwO6WqfVERKToUXjns7/3\ncWc3QM3TN/iLiIj30mXzfJZdH7efny6Ti4hI7qjlnc+y6+O22XDLJPYiIlL0qeWdz7KbrN7Tk9iL\niEjRofDOZwMGZN2XrT5uERHJLYV3PoiNNRMe7k+lSgGMH+/LM89YXE8SUh+3iIjcKPV5e9jfR5fv\n22di3z6TAltERG6aWt4elt3o8gkTcjezmoiIyN8pvD0su9Hl2c+sJiIicn1KEA/T6HIREXE3hbeH\naXS5iIi4m8Lbw2JirEyenKLR5SIi4jYabZ4PYmKsCmsREXEbtbxFRES8jMLbA66dlCU83J/YWF3g\nEBER91GquFlWk7I4l9XPLSIi7qGWt5tpUhYREfE0hbebaVIWERHxNCWKm2lSFhER8TSFt5tpUhYR\nEfE0hbebaVIWERHxNI029wBNyiIiIp7k0Zb3qFGj6NGjBz179mTnzp0ZXlu5ciXdunXjkUce4dtv\nv/VkMURERIoUj4X35s2bOXr0KLNmzWLkyJGMHDnS9Zrdbmf48OFMmTKFadOmsXr1ak6fPu2pooiI\niBQpHgvvn376icjISABq1apFYmIiSUlJAFy4cIHSpUsTFBSE0WjkrrvuYuPGjZ4qioiISJHisT7v\nc+fOERYW5loOCgoiPj6egIAAgoKCuHz5MkeOHKFKlSps2rSJ5s2bX3d/5cr5Yzab8lSmkJDAPG0v\nTqpH91A9uofq0T1Uj+6RX/WYbwPWHA6H62uDwcCYMWMYNGgQgYGBVK1aNcftL1xIztPxQ0ICiY+/\nlKd9iOrRXVSP7qF6dA/Vo3t4oh6zOxnw2GXz0NBQzp0751o+e/YsISEhruXmzZszffp0Jk+eTGBg\nIFWqVPFUUTxODyIREZH85LHwbtWqFcuWLQNgz549hIaGEhAQ4Hr9mWee4fz58yQnJ7N69Wpatmzp\nqaJ4VPqDSPbtM2GzGVwPIlGAi4iIp3gsYZo0aUJYWBg9e/bEYDAwZMgQ5s+fT2BgIO3bt+fhhx/m\nqaeewmAw0LdvX4KCgjxVFI+63oNIdK+3iIh4gsFxbWd0IZbXfgRP9elUqhSAzWbItN5sdnDyZJLb\nj1fQ1DfmHqpH91A9uofq0T2KRJ93caEHkYiISH5TeOeRHkQiIiL5TeGdR3oQiYiI5DcNiXYDPYhE\nRETyk1reIiIiXkbhLSIi4mUU3iIiIl5G4S0iIuJlFN4iIiJeRuEtIiLiZRTeIiIiXkbhLSIi4mUU\n3iIiIl5G4X0TYmPNhIf7U6lSAOHh/np2t4iI5Culzg2KjTXz7LMlXcv79pn+WtZ85iIikj/U8r5B\n48f7Zrl+woSs14uIiLibwvsGHTyYdZVlt15ERMTdlDg3qE4d+w2tFxERcTeF9w0aMMCS5fr+/bNe\nLyIi4m4K7xsUE2Nl8uQUGjSwYTY7aNDAxuTJGqwmIiL5R6PNb0JMjFVhLSIiBUYtbxERES+j8BYR\nEfEyCm8REREvo/AWERHxMgpvERERL6PwFhER8TIKbxERES+j8BYREfEyCm8REREvo/AWERHxMgpv\nERERL6PwFhER8TIKbxERES+j8BYREfEyCm8REREvo/DOhdhYM+Hh/lSqFEB4uD+xsXoMuoiIFByl\nUA5iY808+2xJ1/K+faa/llOIibEWXMFERKTYUss7B+PH+2a5fsKErNeLiIh4msI7BwcPZl1F2a0X\nERHxNCVQDurUsd/QehEREU9TeOdgwABLluv79896vYiIiKcpvHMQE2Nl8uQUGjSwYTY7aNDAxuTJ\nGqwmIiIFR6PNcyEmxqqwFhGRQkMtbxERES+j8BYREfEyCm8REREvo/AWERHxMgpvERERL6PwFhER\n8TI5hveMGTNISkrKj7IUWw4HTJrkQ9OmpXjooZKMGePL8uUmzp0zFHTRRESkEMrxPu8DBw4wZcoU\nmjVrRvfu3WnatGl+lKvYsNthyBA/Jk/2pWRJB8eOmVm79uq3pXVrK716pdGxo5WSJa+zo2s4HHD8\nuIE9e4zUqOGgbl1N5SoiUpTkGN7vvvsudrudTZs28d133/H+++/Trl07Hn74YcqUKXPdbUeNGsWO\nHTswGAwMGjSIRo0auV6bNm0a3333HUajkYYNG/LWW2/l/dN4mdRUePHFEsTG+lC3ro2ZM1MoVcrB\ntm0mtm418eOPJtatM7NunZnSpR3ExKQRGWmlWTMbQUFX93P5MmzZYuKnn0xs22Zi504jCQnOiyoG\ng4MHH7QycGAqtWs7CuiTioiIO+VqhjWj0Uj16tWpWLEiu3fvZs+ePTz66KMMGDCAyMjILLfZvHkz\nR48eZdasWRw+fJhBgwYxa9YsAJKSkvjiiy9Yvnw5ZrOZp556il9++YXbb7/dfZ+sELJaITHRQGIi\nXLhgYNQoP9atM9OihZWpU1MoV875vogIGxERNgYOhMOHDcyc6cPMmT58/bUvX3/tfBRpnTo2br/d\nzpEjBrZvN5GWdvUS+y232LnnnjTq17ezdKmZ2FgfFi4006OHlVdfTaVaNYW4iIg3yzG8FyxYwLx5\n87h48SLdu3fnq6++okyZMvz555889thj2Yb3Tz/95HqtVq1aJCYmkpSUREBAAD4+Pvj4+JCcnIy/\nvz8pKSk5tuK9zaFDBlasMHPokJHffjNy+LCR06czDzHo0CGNyZOvZHtJvFYtB2+9ZeH11y389JOJ\n//3PxKZNJrZsMXHwoAmj0UHjxnbuvtvG3XdbadrU5joJAHj5ZQtLlpgZO9aXGTN8+O47M6NHX6FH\nDyuGPHap795tZPFiM088kUalSjohEBHJLzmG9/r16+nfv3+mvu7SpUvTu3fvbLc7d+4cYWFhruWg\noCDi4+MJCAjAz8+P559/nsjISPz8/OjUqRM1atTIw8coXBYsMNO/fwlSUpzpaDA4qFrVQatWVsqW\ndVC2rIMyZaBmTTu9eqVhzsX1D7MZWre20bq1DXC24g8dMlKlip3AwOy3MxigUycrHTpYmT3bzNtv\nl+DFF0uyalUa7713hZs5ZzpyxMDYsX7Mn2/G4TDw9dc+fPrpFe6915bpvQkJUK4ceT5REBGRq3KM\njWeffZbvvvvOFd5vvvkmTz75JHXq1KFbt265PpDDcbVllpSUxOTJk4mLiyMgIIDevXuzf/9+6tWr\nl+325cr5Yzabcn28rISEXCfl3MBuh8GDYdQoCAyEzz6Dli2hVi0DJUsayHpwf4mbPl6lSjf2/hdf\nhC5d4LHHYMECH7Zt8+H11yExEU6dgpMn4fx5uHIFUlKc/wNUrQrVqzv/nTkDX3wRQFoa3HEHREfD\ne+8Z6d7dn2HDYNAgZz18/z188gmsXOk85owZUKpUzmVMS3OeqBSHsPf0z2NxoXp0D9Wje+RXPeYY\n3sOGDaN///6u5W7dujF8+HC++eab624XGhrKuXPnXMtnz54lJCQEgMOHD1OtWjWC/hp11bRpU3bv\n3n3d8L5wITmnol5XSEgg8fGX8rSP67l0CZ5/vgRxcT7cequdb75JcY3yTkpy/isMAgJg7lz44ANf\nPvjAl+efz5ySvr4OSpQAPz8HdjscOJDxpKNGDTtvvpnK/fdbMRqhdWsjzzxTksGDjXz/vZXjx42c\nOOHcpmJFO4sWGWnVysa336ZQocLVk7gjRwx8/LEv+/cbOX/eyLlzBhITDdSta+P991Np0SJzS76o\n8PTPY3GhenQP1aN7eKIeszsZyDG8bTZbhkvmTZs2zdCKzk6rVq346KOP6NmzJ3v27CE0NJSAgAAA\nqlSpwuHDh7ly5QolSpRg9+7dhIeH5/azFDppafDQQ/5s327i3nutTJmSkqHfubAxm+G11yxER1vZ\nudNEhQp2KlRwUKGCg+BgB6a/XeBISYFTpwwcP26kRAl/7rjjMj4+V1+/8047P/xwmX79SrJqlZlS\npRz06WPhySfTqF3bzmuv+TFtmi/R0f5Mn55C6dIOPvjAl+nTfbBaDRiNDoKCHFSqZKdOHdiyxcj9\n95fkqafSeOutVP76sclSaqrzX+nSnqkrEZHCKMfwDgwMZPr06bRo0QK73c66desolYvrn02aNCEs\nLIyePXtiMBgYMmQI8+fPJzAwkPbt2/P000/zxBNPYDKZuOOOO7z6/vEJE3zZvt1ETEwan3xyJVd9\n2IXBbbfZue22nO8BL1kSatZ0ULOmjZAQiI/P/J6gIJg+PYX//c/EbbfZMvTDf/BBKrfc4mDUKD+i\no/2xWiE11UCtWnZee+0K999vzXDCsHmzkZdeKsEXX/iybJmZF16wEBDgwMfHeeKRkGBg504jO3aY\n2LfPiN0OvXqlMXCghYoVr39imZYGGzeaMBr5a+yB88TheicIIiKFjcGRQzM6ISGBcePGsXPnTgDu\nuOMOBgwY4LrknV/yeinCU5eF9u410r69P+XLO1i37nKRbwHmpR7nzzfz4oslqFjRwauvpvLQQ9Zs\nT3SuXIEPP/Rl4kRfbLasO8D9/ByEhdm5eNHAb78ZKVnSwTPPWPi//7NQtmzG99psMG+emffe8+Po\n0cxjDyIirLz1VmquTmbcQZcp3UP16B6qR/fIz8vmOYZ3VqZOncoTTzyR50LdiMIY3lYrREf7s2OH\nienTk4mMLLp9tOnyWo8XLjj73a+97H49Bw8a2bbNiNVqwGp1hrC/v4OGDe3Uq2fHx8f5fZg504f3\n3vPl1Ckj/v4OGjWyERZmJyzMjq+vg4kTfTl40ISPj4OePdOoUMFBYqLBFfxbtzqb/l27pvH666mE\nhDj45RfnZDnbtxtJSDBgtRqw2Zyt95AQB02a2Gja1MYddzgnzbHbITkZkpIMmM0QHOzIduCd/li6\nh+rRPVSP7lGownvfvn18+umnXLhwAQCLxcLp06dZs2aNWwuYk8IY3hMn+jJihB8PP5zGxx9fceu+\nC6vC/EuekgJffOHDjBk+HD5sxG6/mpwmkzO0X37ZkmmSGocD1qwxMWKEH7t2mTCZHDgcZNgewMfH\ngdkMRiNcvpzxNX9/B8nJGdcFBdmpW9f5r2FDO+3bW133w19bjw6HM/Sz6426fBnmz/fht9+MnDlj\n4MwZA/HxBipUcNC4sY3Gje00amSjevXsTxaKqsL88+hNVI/uUajCu2fPnjz++ON89tlnvPTSS8TF\nxdG1a1eaN2/u1gLmpLCF98GU79saAAAgAElEQVSDRiIi/Clb1nm5vDAPUHMnb/klT052jpLfs8fE\n6dMGYmLSqFXr+heZ7Hb47jszkyb5UqKEgyZN7Nx5p40777RlmoTm3DkD27c7W+xbt5pISDAQEODs\nOy9VysGVK3DggIkjRww4HFcTtUkTGx07WmnXzo91666waZOJzZtNxMcbiYy08n//Z+Guu2wYDM6r\nDDNn+jB2rG+mCX5Kl3bw558Zk7paNTvR0VY6dbLSvLkt08DDoshbfh4LO9WjexSq8O7Tpw///e9/\neeyxx/j222+x2Wz069ePyZMnu7WAOSlM4e1wQOfO/vz8s4n//jeFjh2tbtmvN9Av+Y1JSXFOprN5\ns4klS8xs3GjK1IdfsaKdkBAHu3Y50/bOO21065bG1Kk+7N9vomRJB889ZyEy0krFig5CQx34+cHF\ni7Bzp4kdO0z88ouRNWvMXLrk3Hf58nYaNbL/dULhoFQpaNbMxgMPZJ5Zz26HESN8+fZbX2zX9Pz4\n+TmoWfPq1YOwMDstW9ow5uJBwna7866B4GAHNWt67oqAfh7dQ/XoHoXqVrHU1FQOHjyIn58fmzdv\npnbt2pw4ccKthfM2GzaY+PlnE9HRacUquOXGlSx5dVT/00+nkZAAy5eb+f33ktSunUKLFjaqVXOG\n288/G/noI1/i4nzYutWEweCgVy/n1LhZTT9btizce6/NNbOdxQLr1ztPEuLizKxalfHXe8oU2LDB\nwqhRqa4xB1YrDBhQgtmzfQgJsVO16tXjJCcb2LrVxObNV/dTt66NV1+10KWLNcsQt9lg0SIzH37o\ny759zpOR6tXttGtnJSLCyj332HI1WY+IXF+OLe+tW7dy8eJFypcvz2uvvcb58+f55z//ybPPPptf\nZQQKV8u7d+8SLF3qw+LFl2nWrHg9blNn6O5xvXo8cMDI8uVmIiKshIXd3M9Xej/65csGkpLg/HkD\nr71Wgj17TLRubeXzz1MoWRKefdb5s9ykiY3p05P5+00kqalw+LCRAweMrFplZu5cMzabcyKd/v0t\nVKni+Gs2PgOnThn4/HNfDh82YjQ6eOABK2lp8OOPV68IlCjhoE0bKx07WomKsua5u0k/j+6henSP\nQnXZfO/evTRo0MCthbkZ+RnesbFmxo/35eBBI3Xq2BkwwEJMjLOFffSogebNS3H77Xbi4pI1QEhu\nSkHUY1KScxbApUt9qFHDTqVKdjZuNNO6tZWvv07J1b3uv/1mYPx4P+bMMWd5C1/6aP4XXrBQo4bz\nT0taGmzdauKHH0wsXWrm4EFni9xkclCxogObjb/uJDBQsqSDW26xc+utzv9r1HBesq9Vy46fn/MY\niYmwaZOJDRvMnDvnS3i4s+vqZu7Vv3gRLl40/FUG5/81atizfVBQUaXfa/coVOH9xBNPMHXqVLcW\n5mbkV3jHxpp59tnMv7mTJ6cQE2NlyBA/Jk3y5ZNPUujevfhdMtcvuXsUVD3a7TBmjC/jxzuTMDra\n+VS7Ejc4xf5vvxmYO9cHu93ZNeDn58DfH9q1s1KlyvUHBh46ZGDJEh+WLjUTH2/AZAKz2TmS/88/\nDZw8mXGQHziDvkYNOyVKOOdW+PudAP7+Djp1svLgg87bAP38nK38EiWgTBlHhs/3++8Gliwxs2SJ\nD1u2GDMdq3x5O/37W+jdO+2G6yUtzTlVcj5Pg5Fn+r12j0IV3m+88QYnTpygcePG+Fxzc+61853n\nh/wK7/Bwf1df3bUaNLDx/ffJ3H57ACVKONi27TK+vnkqklfSL7l7FHQ9Ll7sfFzt889bCt2MgKmp\ncPy4gaNHjRw65Lxk7/xnIiXFOaDv7rtttGplo149f778MpXZs304diz7kXQlSjhn0/P1hT/+cL7P\naHTQvLmNW291uE4e0tJg4UIfkpIMVKpk5+WXLTRsaGPbtvR7/p0DCB95JI2HH05zXfa/eBGmTvXl\n8899OH3aSJMmNrp0SaNLFyvVqjn49VcjP/xgYtUqM4cPG+nRI41+/SzXfSJgfiron8eiolCF98cf\nf5zl+hdeeCHvpboB+RXelSoFZHk50Gx2MHJkKq+/XoKBA1MZONCSp/J4K/2Su4fq8cY5HM4Bcdee\nbKTXo8PhvJS+Zo2Jy5cNpKQ4p+BNSYHERINrQp6kJLjjDrurz718+cx//s6fN/DJJz588YWv67G+\n6cqWdZCcDBaLAT8/B507WylXzsH06T4kJztvF2zY0MbPP1+9q6B8eTvnzl09sUifEyA42Nkl16dP\nmqtLwOFwnkC4s2GQ/hf+el18+nl0j0IV3nZ71gNmjLm5X8SNCrrlXb++DasVjhwxsn37ZUJDb3hi\nuiJBv+TuoXp0D0/W45kzBqZM8eHSJYPrfv+aNR0kJBiYNcvMN984B+cBVKpkp29fC48/nkbp0s4T\ngCVLzHz3nZn9+420aGGjXTsrbdvaCAhw8NlnvnzyiS+XLhmoUMFOmTIOLl50nmBYLM51zhkCnTMF\n1q5t55Zb7JQpk3O5HQ749Vcj69eb2LjR+c/PD4YPT6Vz56y7+vTz6B6FKrzr1auH4ZpTNoPBQGBg\nIJs2bXJrAXNS0H3e/funMmGCH927Ox8+Ulzpl9w9VI/uUZD16HDATz+Z+PNPaNfOluspf9OdP29g\n4kRfpk3zwcfHQZkyzpa9v7+D33+/+ljda5Ut6xzIFxrq7AZIf7hOUpKBo0edXQ1HjxozzPZXsaKd\nCxcMpKYa6NIljVGjUjM8mhec9Xj27CV27zYyb54P27cbadrUxn33OacAvpEJf44fNzBvng916tjp\n0CHzvAJFWaEK72tZLBZ++uknDhw4QN++fd1WuNzI79HmEyZcHW3ev7+FOXN8WLnSzIoVl2ncuHjd\nHnYthY57qB7doyjX44ULsHevib17jfz+u/GvYHYGdGpq1okYEOAM93r17LRqZePuu63UqOHg0CEj\nAwaU4OefTZQt66B//1SCg6/+6U9IKMm0aTZ+/TVzSgcH24mIsNGsmY3GjW00aHB15P+1duwwMmmS\nLwsXXr0TITo6jbFjUzM97e/iRefTAcuUcQ4ovLYrJC3NeWdESorzeQbOf84uicqVs48ru51cTSDk\nSYU2vNP17t2br7/+Os+FuhEFeZ/3sWMGmjYNoGlTG0uWJOepHN6uKP+xzE+qR/cojvXocDjDLf0y\ne2Ki8xa7W291Pt42u5au3Q5ffeXD8OF+mebhB+egvvvus9K1q5W777ayaZOJ5cvNLF9u5syZq6lo\nNjuoU8dOYODV6EhKMrBnjzP469e30adPGgsXmtm40Uzp0g6GDbtCs2Z2li1z7nPzZlOGOwYCAhz4\n+jq4fNmQ7YkJQFSUlQEDUrnzzqsNqK1bnScNS5aY/7oykX67oZ2777Zx1122mxpDkJTknKgoJCT3\nMwQWqvCeO3duhuVTp06xePFi4uLi3Fe6XCjI8P7gA1/GjPFjwoQUHnmk+N0edq3i+MfSE1SP7qF6\nvHEnThhYv97EtX/5K1cuSZMml7K8V95uh337jOzYYWTHDhM7dzqvBqSmXn2P0Qj33GOjXz8L4eHO\nufntdvjmGx+GDvUjKenarlcHd95p5x//sJOYiGswYVoarmcDBAQ4KFnS+fTB9DsB9u51zmwJcO+9\nVrp0sTJnjtk1A2Dt2jZsNgN//OF8AmC6wEAHbdtaad/eyj/+Yf+rqyFzix+ct/nFxZlZtMg5Q6HF\nYsDf33kycMstdoKCHNhsV68IhIQ46NfP4pqZsFCF95tvvplhOSAggG7dulGvXj33lS4XCiq8HQ5o\n2bIUp04Z2L07qdDc2lFQ9MfSPVSP7qF6dA9P1uPJkwZGj/YjORnat7fSrp2NkJAbH/CbPsbggw98\nWbv2aupGRlr5178s3HPP1Qf6nDxp4OBB56yAy5aZs72N0N/f8dfJgvPrQ4eudknUr2+jRg07x44Z\nOXLEmOEE5Fp+fg7++U8L/ftbqF27EIU3wJEjR7j11luBgptxraDCe8sWIx07lqJr1zQ+/bT4DlRL\npz+W7qF6dA/Vo3t4Wz1u3WpkwwYzUVFW6ta9/hgkh8M55fDq1SbOnDG6ZtVz3jqY/s85lXD16nbu\nv9/K/fc7W+nX7iMhwUBiovNWRbMZTCbno4RHj/bj5EkjQUF2Roww8tBDheTBJB9++CFnz55l9OjR\nAHz22WdUq1aNV155xa0FLKxmz3YOIX344bQCLomIiADceaedO+/M3VwbBgPUq+ccxHezDAYIDnYQ\nHJxxfY8ezqCfMsWXCRN86dcPWrc2ZBrN7wk5js3btGmTK7gBxo8fz5YtWzxaqMIiNdU521JoqN31\n5CYREZF0JUvCiy9a2Lz5Mps3ky/BDbkI77S0NCyWq2c4ly9fxmotHoO2Vq40c+GCgW7drIVuCkkR\nESk8goMdNGuWf8fLMZJ69uxJx44dadiwIXa7nV27duX71KgFZfZsZ/V0765L5iIiUnjkGN7du3en\nVatW7Nq1C4PBwJtvvkmlSpXyo2wFKiHB2fJu0MBGw4bFd1IWEREpfHK8bH7o0CFmzJhBVFQU9913\nHxMnTuTgwYP5UbYCtWCBD2lpBg1UExGRQifH8B46dCjh4eGu5W7dujF8+HCPFqowmDPHB6PRQbdu\nxaN/X0REvEeO4W2z2WjatKlruWnTptzEjKpe5ehRA1u3mggPt+XbyEEREZHcyrHPOzAwkOnTp9Oi\nRQvsdjvr1q2jVKlS+VG2ArN9u3MKvjZt1OoWEZHCJ8fwHj16NOPGjWPGjBkANGnSJMN930XR7t3O\nCxK33aaBaiIiUvjkGN5BQUGMHDnStZySksKyZct48MEHPVqwgrR7t7PlHRamiVlERKTwyfXTT7dt\n28bbb79NmzZtWLFihSfLVOD27DFStaqdcuUKuiQiIiKZXbflfebMGRYsWEBsbCwWiwWLxcLChQup\nWLFifpUv3509a+DMGSNRUervFhGRwinblvc///lPOnXqxKFDh3jnnXdYuXIl5cuXL9LBDc5WN+iS\nuYiIFF7ZhvfJkycpV64ct9xyC7feeitGoxGDIevnmRYl6f3dmlVNREQKq2wvmy9evJgdO3Ywd+5c\nHnjgAcLCwkhMTCQtLQ0fH5/8LGO+Sm95N2yolreIiBRO1x2w1rhxY4YPH87atWu5//77qVixIvfe\ney/vvfdefpUv3+3ebSQw0EH16pqcRURECqdcjTYvWbIkXbt2Zfr06UybNs3TZSowKSlw6JCRsDAb\nxlyPwxcREclfNxxRNWvWZODAgZ4oS4Hbv9+I3W5Qf7eIiBRqal9e4+rkLApvEREpvBTe10ifFlWD\n1UREpDDLMbwTExMZO3Ysr776KgCrVq0iISHB4wUrCLt3mzCZHNStq5a3iIgUXjmG99tvv02lSpU4\nfvw4ABaLhddff93jBctvdrvzNrE6deyUKFHQpREREclejuGdkJDAE0884bq3u0OHDly5csXjBctv\nR44YSE42qL9bREQKvVz1eaelpblmVzt37hzJyckeLVRB2LMnfWY19XeLiEjhluMjQR999FEeeugh\n4uPjee6559i1axdvvfVWfpQtX6UPVlPLW0RECrscw7tjx440adKE7du34+vry7BhwwgNDc2PsuUr\n3SYmIiLeIsfwnjt3ruvry5cvs3btWsxmMzVq1KBx48YeLVx+2r3bSKVKdsqX17SoIiJSuOUY3hs2\nbGDDhg00adIEk8nE1q1badasGX/88Qfh4eG89NJL+VFOjzp/3sCpU0bat9czvEVEpPDLMbxtNhtL\nliyhfPnyAJw/f57Ro0cTGxtLz549PV7A/LBvn57hLSIi3iPH0eZnzpxxBTdAcHAwx48fx2AwYLcX\njf7hkyedI+n1JDEREfEGOba8K1euzIsvvkjz5s0xGAxs376dUqVKERcXR6VKlfKjjB539qwzvENC\nisbJiIiIFG05hvfYsWNZuHAh+/fvx26307hxY7p27UpSUhLh4eH5UUaPO3vWeQEiNFQtbxERKfxy\nDG9fX1+6d+/uWrZYLLz66qtMnDjRowXLT9u2OcM7OtqfunXtDBhgISZGg9dERKRwyjG8FyxYwJgx\nY0hMTATAaDRy1113ebxg+SU21szmzc5qsNsN7Ntn4tlnSwIpCnARESmUchyw9s0337Bo0SKaNm3K\n1q1beeedd+jWrVt+lC1fjB/vm+X6CROyXi8iIlLQcgzvwMBAQkJCsNls+Pv706NHD+bNm5ernY8a\nNYoePXrQs2dPdu7c6Vp/5swZHn/8cde/Nm3asGjRopv/FHlw8GDWVZDdehERkYKW42Vzk8nE6tWr\nqVSpEh999BG1a9fmxIkTOe548+bNHD16lFmzZnH48GEGDRrErFmzAKhQoQLffPMNAFarlccff5yI\niIg8fpSbU7u2nQMHTJnW16mjkeciIlI45di8/Pe//03FihUZNGgQZ8+e5bvvvmPw4ME57vinn34i\nMjISgFq1apGYmEhSUlKm98XGxhIVFUWpUqVuovh516dPWpbr+/e35HNJREREcifHlveaNWtcfdzD\nhw/P9Y7PnTtHWFiYazkoKIj4+HgCAgIyvG/OnDl8+eWXOe6vXDl/zObMLeQbERISmGldu3bO/8uX\nh4sXoUEDePNN6NmzZJ6OVZRlVY9y41SP7qF6dA/Vo3vkVz3mGN4rVqzgvvvuIzAwbwVyODLfQ719\n+3Zq1qyZKdCzcuFC3p4hHhISSHz8pUzrnZfM/enX7wovvHC1FR4fn6fDFVnZ1aPcGNWje6ge3UP1\n6B6eqMfsTgZyDO8rV64QERFBjRo18PHxca2fNm3adbcLDQ3l3LlzruWzZ88SEhKS4T1r1qyhZcuW\nORXBozRBi4iIeJscw7tfv343teNWrVrx0Ucf0bNnT/bs2UNoaGimFvauXbvo2LHjTe3fXeLj06dG\nVXiLiIh3yDG8mzdvzpo1azh+/DiPPfYYx44do1q1ajnuuEmTJoSFhdGzZ08MBgNDhgxh/vz5BAYG\n0r59ewDi4+MJDg7O+6fIg/R5zdXyFhERb5FjeL/33nscPXqUkydP8thjj7Fo0SISEhJyNeL81Vdf\nzbBcr169DMsFdW/3tRTeIiLibXK8Veznn3/m448/dt3K9fzzz7Nnzx6PFyy/nD1rwGRyEBSk8BYR\nEe+QY3j7+fkBYDA4W6g2mw2bzebZUuWjs2eNlC/vwJS3u9BERETyTY6XzZs0acIbb7zB2bNn+eqr\nr1i+fDnNmzfPj7Lli7NnDdSqpdnURETEe+QY3i+99BJxcXGULFmS06dP8+STT3LfffflR9k8LikJ\nkpMN6u8WERGvkmN4v/zyyzzwwAMMHjwYo7FoPaxDg9VERMQb5ZjGbdq0YcaMGURERDBixAh27dqV\nH+XKF1cnaNFlcxER8R45trzvv/9+7r//fi5dusSKFSuYNGkSx44d4/vvv8+P8nlU+gQtanmLiIg3\nydV1cIfDwd69e9m1axe///57pvu1vZUum4uIiDfKseX9zjvv8OOPP1K/fn06derEa6+9RsmSReOJ\nW2p5i4iIN8oxvOvWrcuAAQMICgpyrTt58iSVK1f2aMHyQ3rLOyREfd4iIuI9cgzvRx99FIDU1FSW\nLVvGvHnzOHz4MOvXr/d44TxNTxQTERFvlGN4//LLL8ybN4+lS5dit9sZNmwYUVFR+VE2jzt71kCJ\nEg7y+KhyERGRfJXtgLUpU6bQsWNHXnrpJYKDg5k3bx7Vq1enc+fOGZ7r7c3OnnVO0PLXzK8iIiJe\nIduW9/jx46lduzbvvPMOd911F3B1fvOiwG53Dlhr1Ej93SIi4l2yDe81a9YQGxvLkCFDsNvtxMTE\nkJaWlp9l86iLFyEtzaAJWkRExOtke9k8JCSEvn37smzZMkaNGsWxY8c4ceIEzz33HD/++GN+ltEj\nNFhNRES8Va4maWnWrBljxoxh3bp1tGnThk8++cTT5fI4TdAiIiLe6oaeNBIQEEDPnj2ZPXu2p8qT\nbxTeIiLirYrWY8JugMJbRES8VTEObz1RTEREvFOxDW/Nay4iIt6q2Ib31XnNFd4iIuJdinV4ly7t\noESJgi6JiIjIjSm24R0frwlaRETEOxXL8E5Lg/PnDervFhERr1Qsw/v8eQMOh8JbRES8U7EMb93j\nLSIi3kzhLSIi4mWKZXgnJzvDu0oVDVgTERHvk+0jQYuyyEgrn36aQseO1oIuioiIyA0rluHt7w9d\nuyq4RUTEOxXLy+YiIiLeTOEtIiLiZRTeIiIiXkbhLSIi4mUU3iIiIl5G4S0iIuJlFN4iIiJeRuEt\nIiLiZRTeIiIiXkbhLSIi4mUU3iIiIl5G4S0iIuJlFN4iIiJeRuEtIiLiZRTeIiIiXkbhLSIi4mUU\n3iIiIl5G4S0iIuJlFN4iIiJeRuEtIiLiZRTeIiIiXsbsyZ2PGjWKHTt2YDAYGDRoEI0aNXK9durU\nKV5++WXS0tJo0KABw4YN82RRREREigyPtbw3b97M0aNHmTVrFiNHjmTkyJEZXh8zZgxPPfUUc+fO\nxWQycfLkSU8VRUREpEjxWHj/9NNPREZGAlCrVi0SExNJSkoCwG63s3XrViIiIgAYMmQIlStX9lRR\nREREihSPXTY/d+4cYWFhruWgoCDi4+MJCAggISGBUqVKMXr0aPbs2UPTpk155ZVXrru/cuX8MZtN\neSpTSEhgnrYXJ9Wje6ge3UP16B6qR/fIr3r0aJ/3tRwOR4avz5w5wxNPPEGVKlXo27cva9asoU2b\nNtluf+FCcp6OHxISSHz8pTztQ1SP7qJ6dA/Vo3uoHt3DE/WY3cmAxy6bh4aGcu7cOdfy2bNnCQkJ\nAaBcuXJUrlyZ6tWrYzKZaNmyJb/++quniiIiIlKkeCy8W7VqxbJlywDYs2cPoaGhBAQEAGA2m6lW\nrRpHjhxxvV6jRg1PFUVERKRI8dhl8yZNmhAWFkbPnj0xGAwMGTKE+fPnExgYSPv27Rk0aBBvvPEG\nDoeDOnXquAaviYiIyPUZHNd2Rhdiee1HUJ+Oe6ge3UP16B6qR/dQPbpHkejzFhEREc9QeIuIiHgZ\nhbeIiIiXUXiLiIh4GYW3iIiIl1F4i4iIeBmFt4iIiJdReIuIiHgZhbeIiIiXUXiLiIh4GYW3iIiI\nl1F4i4iIeBmFt4iIiJdReIuIiHgZhbeIiIiXUXiLiIh4GYW3iIiIl1F4i4iIeBmFt4iIiJdReIuI\niHgZhbeIiIiXUXiLiIh4GYW3iIiIl1F4i4iIeBmFt4iIiJdReIuICAArVsQRHt6CixcvFnRRMhg6\n9G1eeKEvDz3UhZ49Y3jhhb68//6YXG37zTf/Zffundm+PmTIm6SmXnFXUfONweFwOAq6ELkRH38p\nT9uHhATmeR+ienQX1aN7FNd6jI01M368LwcPGqlTx86AARZiYqw3vb/0enzttZc4fvwYDz/8CA8+\n+JAbS+weX3wxmbJly9KtW4+CLkqWPPHzGBISmOV6s1uPIiIiHhUba+bZZ0u6lvftM/21nJKnAP/z\nz0T27dvDm2++w/TpU3nwwYf49deDfPTRB0yc+CkAX375GYGBpWnatDkffvhvDAYD/v7+DBr0LklJ\nlxg2bDAlS/rTrdvDXL6cxNy5szCZjNx6ay1ef/0tkpKSePvt10hNTaVly1YsWrSAOXO+Y8eO7Uye\n/Alms5nQ0Aq8/vrb+Pj45FjmJUsW8b//beTcuXiGDh3FzJnfsnfvHiwWCw8+2I0uXR5k5Mh3adOm\nHYmJF9m58xcuXrzAsWNH6dXrcTp3fpCHHurC1Kmz+PDDf1O+fAgHDuzjzJnTvPPOCOrWrcf48e+x\na9dOatSoybFjRxk6dBSVKlV2lWH58qWuz1m/fj1efPE1rFYrI0YM4cyZU/j6+vH220MpVy4o07qQ\nkNCb/n7psrmIiBcZP943y/UTJmS9PrdWrVrJ3XffQ4sWLfnjj2PEx5/lH/+ow7lz8Vy65GxNrl+/\nljZtIhg//j0GDhzEhAmTaNbsLubPnw3Ar78eYMiQ4bRq1ZqUlBTGjfuISZO+5NixIxw+fIi4uO+5\n9daaTJr0BQEBgaRf+B0//j3GjBnHxImfEhQUxOrVK3Nd7jNnTvPJJ1MoXboMFStWZtKkL/jPf6bw\n+eefZnrv4cOHGDnyPUaPHsfcubMzvW6xWPjgg4/p3r0ncXGLOXz4EDt3/sKUKV/zyCOPc+DAvkzb\nXPs5f/vtNw4fPsTSpd8THBzMpElf0qXLg6xfvzbLdXmhlreIiBc5eDDrNld263Nr5cpl9O79NCaT\nibZt2/HDD8vp2fMxWrW6l02bNtKwYWP8/HwJCQll7949jB07AoC0tDTq128AQJUqVSlTpiwApUuX\n5s03XwHg6NHfSUy8yJEjR7jjjjsBuOeee5k+fSoJCec5fvwPBg0aCMCVK1dc+8iN+vUbYDAY8PPz\n488/E3nuuacwm81cvHgh03sbNmyEyWQiJCSUy5eTMr3euPEdAISEVGDv3j0cOfI7DRrchtFopFat\n2lSsWCnTNll9zgMH9tO0aTMAIiOjAHj//TGZ1uWFwltExIvUqWNn3z5Tlutv1unTp9m7dzcffzwe\ng8HAlStXCAwMoGfPxwgPb8u8ebNJTLxIeHgEACVKlOCjjyZjMBhc+zh16iRms/NSd1paGh988G/+\n+9/pBAeX57XXBvz1LgdGo3Ob9G3NZh/Klw/h448/u6mypx9z+/atbNu2hY8//gyz2Uz79q0zvddk\nulpvWQ33yvz61fJeW+Z0f/+cb7/96l/7MWK3Z9x/VuvyQpfNRUS8yIABlizX9++f9frc+P7774mJ\n6c7XX8/gv/+dzowZ8/jzzz85ceI4YWG3ceTIb2zcuIE2bSIBqF37H/zvfxsBZ4t9y5bNGfaXnHwZ\nk8lEcHB5zpw5zf79+7BarVSuXJX9+52XntO3L126NAC///4bAHPnzuTQoV9v+DMkJl4kNLQCZrOZ\n9et/xGazk5aWdnMV8sW31gwAAA8QSURBVJcqVapy4MB+HA4HR478zunTpzK8/vfPuXv3bqxWK/Xq\nNWDbtp8B2LBhHVOnfpnlurxQeIuIeJGYGCuTJ6fQoIENs9lBgwY2Jk/O22C1xYsX06lTF9eywWAg\nOrozK1cuw2Aw0LBhYy5fTqJixYoA9O//Kt988xUvvNCXJUu+p06duhn2V6ZMWZo1a8EzzzzBV19N\noVevx5k48QOiozuzc+d2XnihLwkJ5zEanRH0xhvvMGrUUPr1e4adO3dQvfotN/wZmjZtwfHjx3jh\nhb6cOHGcu+++h/ffH33TdQJQr14DqlWrTt++vZk9ezq33lrTVeasPuczzzzDxIkf0K7dfaSkpPDC\nC32ZPXsG0dGdiYyMyrQuL3SrmNwQ1aN7qB7dQ/XoHvlVj6dPn+Lo0SO0aNGS3bt38sUXk/nww088\nftybZbFY+OGH5URHdyYlJYVHH32I2bMXYjZn3eOsW8VERKTIKVUqgFmzpvHf/07B4YABA14t6CJd\nl6+vL/v372Xu3FkYjQaeeea5bIM7vxWOUoiISJEXGBjIBx98XNDFuCEvvfRaQRchS+rzFhER8TIK\nbxERES+j8BYREfEyCm8REREvo/AWESnmevTo4Zo8Jd2nn37MjBnfZvn+bdu28PbbzoFcb7zxcqbX\n582bxRdfTM72eIcO/cqxY0cB730kZ0FTeIuIFHOdO3dm1aoVGdatWbOKyMj7ctx2zJgPbvh4P/64\nij/+OAbA0KGj8fMrccP7KO6K3a1i7n4OroiIt+vYsSMPP9yDfv1eBGD//n2EhIQQEhLKzz9v4vPP\nP8XHx4fAwECGDRuTYdtOndqxePEPbNmymYkTxxEUFExwcHkqV66C1Wpl5Mh3iY8/S0pKCk891ZeK\nFSuxcOF8fvxxFeXKleOdd95k6tRZJCVdYvToYaSlpWE0GnnjjcEYDAZGjnyXypWrcOjQr9SpU5c3\n3hic4fjXPpIz/dGj+fFIzoJWrMLbU8/BFRFxl3ff9WPRIvf+ae7Sxcq776Zm+3pwcDCVK1dh797d\nNGjQkFWrVtC+fQcALl26xJAhI6hcuQrDh7/Dpk0/4e/vn2kfkyd/zODBw/nHP+rw6qsvUrlyFS5d\n+pPmze8iOrozJ04cZ/DgN/jyy29p0aIlbdq0o0GDhq7tP//8Uzp3foB27e5j9eqVfPnlZzz99LMc\nOLCPoUNHUa5cEDExHbl06RKBgVdnHUt/JGdgYCDPP/9PDh8+xN69uwkODubdd0eycuUy1q9fi9ls\nzrQuJuYhN9Zy/ipW4X295+AqvEWkOGvfvgM//LCCBg0asmHDWiZNcj44o2zZsowdOwKbzfb/7d17\nTFR3Gsbx78AUKYhVcAZXa7vWUqwJVbE1sUVRW6iJTdPVYGJLmzYQK7CVrE0BCYimWQWlBrWb1Aht\nLesFg+slG6q9bNhoHKmCS1u1azRprXhpAWFwCqwDZ/8gO5UtUMHB6QzP5y85Z87MO0+OeTm/Yc7L\n5ct1TJ/+RI/N+8qVK0REPALA1KnRtLe3ExIygrNnT3Pw4N8wmfyw25t7ff1///ssy5b9EYDo6Mf5\n8MNiAMaNG09Y2GgARo+24HDc6Na8PTWS09OGVPMerDm4IiLusnp1e59XyYMlNnYuH330PnFxzzJ+\n/AOuaV/r1r3Nhg1F/P73E9i4saDX428d2PG/kRmffnoIu93OX/5SjN1uJzn55T4qMLmOu3nTicnU\n9Xy3jum89bm7Htfz6NG7MZLT04ZU1+pt3u2dzMEVEfEFQUHBTJwYwUcffeBaMgdwOG4QHj6GlpYW\namqqex2zOXq0hYsXv8UwDE6dqgagqamJ3/1uLH5+fvzzn/9wHWsymejo6Oh2/KOPTqam5iQA//pX\nNZMmPfqrNfc2evRujOT0tCHVvAdjDq6IiK+Ii5vPiRNVxMTMdm1buDCBlJQk1q//My+99Ap//euH\nNDTU/+LYpUtTycnJJDPzT1it4QDMmTOPY8eOkJ6ewr333ovVauWDD7YxZco0ioo2dJsDnpy8jEOH\nKli+fBkVFX8nKen1X623t9Gjd2Mkp6cNuZGg+/aZ2bTp5782T0/XX5v3h0YwuodydA/l6B7K0T00\nEnQQ/eEPTjVrERHxakNq2VxERMQXqHmLiIh4mUFdNl+7di21tbWYTCays7N57LHHXPvmzZvHmDFj\nXF8DKCwsJDw8fDDLERER8QmD1ry/+OILvvvuO8rKyrhw4QLZ2dmUlZV1e8y2bdsIDg4erBJERER8\n0qAtm9tsNp555hkAJk6cSHNzMzdu3BislxMRERkyBq1519fXM2rUKNfPoaGh/Pjjj90ek5eXx5Il\nSygsLMRLvrEmIiLicXftq2L/35yXL1/OrFmzuO+++0hLS+Pw4cPMnz+/l6Nh1KggzGb/Xvffjt6+\nLyf9oxzdQzm6h3J0D+XoHncrx0Fr3larlfr6n+/C88MPP2CxWFw/v/DCC65/z549m3PnzvXZvK9f\n/+mO6tFNCNxDObqHcnQP5egeytE97uZNWgZt2fypp57i8OHDAJw+fRqr1crw4cOBrhFzSUlJ/Oc/\nXbclPXHiBBEREYNVioiIiE8Z1NujFhYWcvLkSUwmE3l5eZw5c4aQkBDi4uLYvn07+/fvZ9iwYUye\nPJnc3K7B6yIiItI3r7m3uYiIiHTRHdZERES8jJq3iIiIl1HzFhER8TJq3iIiIl5GzVtERMTL3LU7\nrHlSX9PNpG/r16+nuroap9PJ66+/TlRUFBkZGXR0dGCxWNiwYQMBAQGeLtMrtLW18dxzz5GamsrM\nmTOV4wAcPHiQ4uJizGYzy5cvJzIyUjn2k8PhIDMzk+bmZm7evElaWhoWi4XVq1cDEBkZyZo1azxb\n5G/cuXPnSE1N5dVXXyUxMZErV670eB4ePHiQ7du34+fnx+LFi0lISHBfEYaPq6qqMpYuXWoYhmGc\nP3/eWLx4sYcr8h42m81ITk42DMMwGhsbjdjYWCMrK8uoqKgwDMMw3nnnHWPHjh2eLNGrbNy40Vi4\ncKGxd+9e5TgAjY2NRnx8vNHS0mJcu3bNyMnJUY4DUFpaahQWFhqGYRhXr141nn32WSMxMdGora01\nDMMwVqxYYVRWVnqyxN80h8NhJCYmGjk5OUZpaalhGEaP56HD4TDi4+MNu91utLa2GgsWLDCuX7/u\ntjp8ftlc080G7oknnmDTpk0AjBgxgtbWVqqqqnj66acBmDt3LjabzZMleo0LFy5w/vx55syZA6Ac\nB8BmszFz5kyGDx+O1Wrl7bffVo4DMGrUKJqamgCw2+2MHDmSuro614qkcuxbQEAA27Ztw2q1urb1\ndB7W1tYSFRVFSEgIgYGBREdHU1NT47Y6fL553850M+mZv78/QUFBAJSXlzN79mxaW1tdy5JhYWHK\n8jYVFBSQlZXl+lk59t+lS5doa2tj2bJlvPjii9hsNuU4AAsWLODy5cvExcWRmJhIRkYGI0aMcO1X\njn0zm80EBgZ229bTeVhfX09oaKjrMe7uPUPiM+9bGbqhXL999tlnlJeX8/777xMfH+/arixvz/79\n+5k6dSrjx4/vcb9yvH1NTU28++67XL58mVdeeaVbdsrx9hw4cICxY8dSUlLCN998Q1paGiEhPw+/\nUI53prf83J2rzzfvX5tuJn07cuQI7733HsXFxYSEhBAUFERbWxuBgYFcu3at29KR9KyyspLvv/+e\nyspKrl69SkBAgHIcgLCwMKZNm4bZbOaBBx4gODgYf39/5dhPNTU1xMTEADBp0iTa29txOp2u/cqx\n/3r6/9xT75k6darbXtPnl837mm4mfWtpaWH9+vVs3bqVkSNHAvDkk0+68vzkk0+YNWuWJ0v0CkVF\nRezdu5c9e/aQkJBAamqqchyAmJgYjh8/TmdnJ9evX+enn35SjgPw4IMPUltbC0BdXR3BwcFMnDiR\nkydPAspxIHo6D6dMmcJXX32F3W7H4XBQU1PD448/7rbXHBKDSf5/utmkSZM8XZJXKCsrY8uWLUyY\nMMG1LT8/n5ycHNrb2xk7dizr1q3jnnvu8WCV3mXLli2MGzeOmJgYMjMzlWM/7d69m/LycgBSUlKI\niopSjv3kcDjIzs6moaEBp9NJeno6FouFVatW0dnZyZQpU1i5cqWny/zN+vrrrykoKKCurg6z2Ux4\neDiFhYVkZWX94jw8dOgQJSUlmEwmEhMTef75591Wx5Bo3iIiIr7E55fNRUREfI2at4iIiJdR8xYR\nEfEyat4iIiJeRs1bRETEy/j8TVpEhrJLly4xf/58pk2b1m17bGwsycnJd/z8VVVVFBUVsWvXrjt+\nLhG5fWreIj4uNDSU0tJST5chIm6k5i0yRE2ePJnU1FSqqqpwOBzk5+fzyCOPUFtbS35+PmazGZPJ\nxKpVq3j44Yf59ttvyc3NpbOzk2HDhrFu3ToAOjs7ycvL4+zZswQEBLB161YA3nzzTex2O06nk7lz\n55KSkuLJtyviU/SZt8gQ1dHRQUREBKWlpSxZsoTNmzcDkJGRwcqVKyktLeW1115jzZo1AOTl5ZGU\nlMSOHTtYtGgRH3/8MdA17vSNN95gz549mM1mjh49yrFjx3A6nezcuZPdu3cTFBREZ2enx96riK/R\nlbeIj2tsbOTll1/utu2tt94CcA2oiI6OpqSkBLvdTkNDg2u284wZM1ixYgUAX375JTNmzAC6xkpC\n12feDz30EKNHjwZgzJgx2O125s2bx+bNm0lPTyc2NpaEhAT8/HStIOIuat4iPq6vz7xvvTuyyWTC\nZDL1uh/o8erZ39//F9vCwsI4cOAAp06d4vPPP2fRokXs27fvF3OQRWRg9KuwyBB2/PhxAKqrq4mM\njCQkJASLxeKaOmWz2VxjDKOjozly5AgAFRUVbNy4sdfnPXr0KJWVlUyfPp2MjAyCgoJoaGgY5Hcj\nMnToylvEx/W0bH7//fcDcObMGXbt2kVzczMFBQUAFBQUkJ+fj7+/P35+fqxevRqA3NxccnNz2blz\nJ2azmbVr13Lx4sUeX3PChAlkZWVRXFyMv78/MTExjBs3bvDepMgQo6liIkNUZGQkp0+fxmzW7/Ai\n3kbL5iIiIl5GV94iIiJeRlfeIiIiXkbNW0RExMuoeYuIiHgZNW8REREvo+YtIiLiZdS8RUREvMx/\nASNQYz7/uxBmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "pS0D5_0iXd_y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> ### Inference"
      ]
    },
    {
      "metadata": {
        "id": "r9HNF524XkCy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Find the best performance model by seeing the performance plot.\n",
        "* Calculate the accuracy on test set using the best performance model.\n",
        "  * Here, you should use a majority voting method to get the prediction for a test data point.\n",
        "  * Specifically, given a test data point, get the predicted class from the trained model on each fold, and then decide the final predicted class by majority voting.\n",
        "* **Do not retrain the model.**"
      ]
    },
    {
      "metadata": {
        "id": "-AnZ8nIiq9kC",
        "colab_type": "code",
        "outputId": "7c8d11bb-3750-42b3-e598-b65dedf5fa1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "cell_type": "code",
      "source": [
        "#  그래프를 좀더 보기 쉽게 만들기 위해서 수업시간에 배운 smooth_curve를 사용한다.\n",
        "def smooth_curve(points, factor=0.9):\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous = smoothed_points[-1]\n",
        "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points\n",
        "\n",
        "smooth_average_train_acc = smooth_curve(average_train_acc[5:])\n",
        "smooth_average_val_acc = smooth_curve(average_val_acc[5:])\n",
        "plt.plot(range(1, len(smooth_average_train_acc) + 1), smooth_average_train_acc, 'bo', label = 'Average Training acc')\n",
        "plt.plot(range(1, len(smooth_average_val_acc) + 1), smooth_average_val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation Average accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.legend()\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFnCAYAAAChL+DqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUsM9+QZHAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "6HF3iD-sw3lv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* 그래프의 결과를 보면 10번째 epoch 이후로 과대 적합이 시작 된다. 10번째 epoch 모델을 불러 와서 test 데이터의 정확도를 계산한다."
      ]
    },
    {
      "metadata": {
        "id": "bhDWsnWuY5w-",
        "colab_type": "code",
        "outputId": "a0cf5da7-4ca9-4f65-d2a4-b0455b6509d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2195
        }
      },
      "cell_type": "code",
      "source": [
        "# write a code here\n",
        "best_model_path = ['/content/gdrive/My Drive/hw2_callbacks/hw2_model.0.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.1.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.2.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.3.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.4.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.5.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.6.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.7.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.8.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.9.10.hdf5']\n",
        "best_models = []\n",
        "best_model_predictions = []\n",
        "for i in range(len(best_model_path)):\n",
        "  best_model_i = models.load_model(filepath=best_model_path[i])\n",
        "  best_model.append(best_model_i)\n",
        "  \n",
        "  best_model_prediction = best_model_i.predict(x_test)\n",
        "  best_model_predictions.append(best_model_prediction)\n",
        "\n",
        "print(best_model_predictions)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[1.02339145e-05, 1.29869004e-04, 6.52811559e-06, ...,\n",
            "        6.96512288e-05, 1.10757828e-05, 6.18530112e-06],\n",
            "       [1.95409084e-05, 2.53263656e-02, 5.05303293e-02, ...,\n",
            "        2.15553024e-04, 1.73723136e-04, 4.69277802e-05],\n",
            "       [1.34294643e-03, 9.41598117e-01, 1.20212135e-04, ...,\n",
            "        5.50377008e-04, 7.46728037e-05, 9.35031949e-06],\n",
            "       ...,\n",
            "       [3.86227948e-05, 8.74293910e-05, 8.07045399e-06, ...,\n",
            "        5.08026023e-05, 7.95393044e-06, 5.14176054e-06],\n",
            "       [1.14715088e-03, 1.67380333e-01, 1.24583940e-03, ...,\n",
            "        2.25933851e-03, 9.50093730e-04, 2.73904356e-04],\n",
            "       [1.39104650e-05, 8.85345161e-01, 6.92616508e-04, ...,\n",
            "        1.39874275e-04, 4.37071896e-04, 2.33446713e-06]], dtype=float32), array([[2.12525865e-05, 7.02164602e-04, 6.44052780e-05, ...,\n",
            "        5.09821111e-05, 5.17018316e-06, 1.41875571e-04],\n",
            "       [5.27475365e-02, 7.75408223e-02, 1.42759923e-02, ...,\n",
            "        1.79180724e-03, 1.34774426e-04, 4.03351760e-06],\n",
            "       [1.33028487e-03, 9.50932145e-01, 4.79803333e-04, ...,\n",
            "        2.17898143e-03, 1.35008268e-05, 1.37084135e-05],\n",
            "       ...,\n",
            "       [9.95483697e-06, 1.89588842e-04, 3.65622473e-05, ...,\n",
            "        7.91941784e-06, 3.71148417e-06, 5.22596893e-05],\n",
            "       [1.22155144e-03, 3.02502159e-02, 2.52628070e-03, ...,\n",
            "        9.06857545e-04, 3.77753371e-04, 1.91129732e-03],\n",
            "       [5.10455202e-03, 1.40566140e-01, 1.69267859e-02, ...,\n",
            "        1.45013444e-02, 1.19112351e-03, 6.40940736e-04]], dtype=float32), array([[8.4823597e-04, 3.7481668e-04, 1.2173790e-05, ..., 7.4646727e-05,\n",
            "        3.9297512e-07, 1.3682629e-06],\n",
            "       [4.5088120e-05, 2.9211720e-02, 1.1298943e-02, ..., 4.2551070e-05,\n",
            "        2.2394297e-06, 2.6371656e-02],\n",
            "       [8.1120210e-04, 9.0992236e-01, 3.1652518e-03, ..., 2.7526161e-04,\n",
            "        3.0516693e-04, 1.5976420e-03],\n",
            "       ...,\n",
            "       [9.0560490e-05, 3.3565180e-04, 2.2927049e-06, ..., 4.6004254e-05,\n",
            "        1.4166820e-06, 1.2171999e-07],\n",
            "       [1.3112131e-02, 6.2338684e-02, 5.3608464e-03, ..., 1.3339412e-03,\n",
            "        2.6975188e-04, 8.1572676e-04],\n",
            "       [9.1067288e-04, 7.0471823e-01, 2.5646028e-03, ..., 5.9463549e-04,\n",
            "        8.2769810e-05, 4.6800391e-04]], dtype=float32), array([[1.3242198e-05, 1.2554709e-04, 9.1132761e-06, ..., 3.0566200e-06,\n",
            "        1.0259629e-05, 8.5849177e-07],\n",
            "       [2.8280439e-03, 1.3823657e-01, 1.2279004e-04, ..., 7.8220549e-04,\n",
            "        7.1390117e-05, 4.7970582e-03],\n",
            "       [6.6303234e-03, 8.1897980e-01, 6.1656004e-03, ..., 3.9743868e-04,\n",
            "        2.4361494e-04, 6.4121000e-03],\n",
            "       ...,\n",
            "       [2.9094859e-05, 2.9403350e-04, 2.6684611e-05, ..., 6.8252789e-06,\n",
            "        2.3030374e-05, 7.0010892e-06],\n",
            "       [3.9516127e-04, 3.1396609e-02, 6.0849957e-04, ..., 1.4729932e-04,\n",
            "        5.0542259e-04, 3.6373857e-04],\n",
            "       [9.3543797e-04, 4.3444175e-01, 2.5171641e-02, ..., 2.9848336e-04,\n",
            "        5.2591541e-04, 1.0694747e-03]], dtype=float32), array([[1.7095446e-04, 6.0090405e-04, 4.4053359e-06, ..., 5.4453958e-06,\n",
            "        2.9774781e-06, 2.0399750e-05],\n",
            "       [9.5827330e-04, 7.6530164e-04, 1.0103981e-02, ..., 2.3113137e-04,\n",
            "        2.3010194e-05, 1.0831819e-05],\n",
            "       [1.2588734e-03, 8.5703546e-01, 1.6320197e-04, ..., 3.8514932e-04,\n",
            "        2.2158983e-04, 2.5841457e-04],\n",
            "       ...,\n",
            "       [1.2206762e-05, 2.8097007e-04, 9.1866792e-07, ..., 1.4919987e-06,\n",
            "        4.0680976e-07, 3.6571876e-06],\n",
            "       [3.9484669e-03, 3.0536314e-02, 1.6296305e-03, ..., 3.2205431e-04,\n",
            "        2.7152765e-04, 9.3779573e-04],\n",
            "       [3.2939378e-04, 4.2385593e-01, 1.1701727e-02, ..., 5.0179678e-04,\n",
            "        1.5652254e-04, 4.8048704e-04]], dtype=float32), array([[2.3646210e-04, 6.2120240e-04, 4.4068482e-05, ..., 1.5853067e-05,\n",
            "        3.1686275e-06, 7.0313989e-05],\n",
            "       [2.6923746e-03, 1.0138870e-03, 1.8373879e-02, ..., 3.9109057e-03,\n",
            "        2.6245285e-05, 1.2986183e-04],\n",
            "       [1.0090107e-03, 8.7309992e-01, 4.9145631e-03, ..., 3.9983112e-03,\n",
            "        8.2992185e-05, 1.4543235e-04],\n",
            "       ...,\n",
            "       [9.0887761e-06, 3.3353575e-04, 6.2937324e-06, ..., 1.5273496e-06,\n",
            "        9.8061662e-07, 4.7865992e-06],\n",
            "       [1.7030468e-03, 2.4841003e-02, 1.7614386e-03, ..., 2.1606266e-04,\n",
            "        3.8683189e-05, 5.7053956e-04],\n",
            "       [1.0369254e-04, 5.9122139e-01, 5.4532336e-03, ..., 5.1370583e-04,\n",
            "        4.8333546e-04, 1.2927187e-04]], dtype=float32), array([[5.2277435e-05, 3.7115940e-04, 8.3768704e-05, ..., 1.3229660e-05,\n",
            "        1.9020983e-06, 1.3130029e-06],\n",
            "       [9.3943626e-03, 2.9705337e-01, 5.0076257e-02, ..., 1.2647012e-03,\n",
            "        2.0998329e-04, 6.9475506e-04],\n",
            "       [5.4558681e-04, 9.3070602e-01, 1.5646745e-03, ..., 1.1566485e-04,\n",
            "        3.5470535e-04, 2.6754185e-04],\n",
            "       ...,\n",
            "       [1.0394949e-05, 3.0977864e-04, 2.0822510e-05, ..., 5.5948449e-06,\n",
            "        7.2138696e-06, 2.8525681e-06],\n",
            "       [1.9594638e-03, 1.0381765e-01, 9.8851311e-04, ..., 1.8089115e-04,\n",
            "        1.2338576e-04, 2.0724275e-04],\n",
            "       [7.3656323e-04, 5.2295822e-01, 1.2642919e-02, ..., 6.0502812e-04,\n",
            "        8.5875532e-04, 2.3933890e-04]], dtype=float32), array([[1.0526312e-04, 1.2450728e-04, 7.0820024e-05, ..., 3.6496820e-04,\n",
            "        9.2506536e-07, 3.6913898e-06],\n",
            "       [2.2453938e-04, 5.3549256e-02, 1.3082210e-04, ..., 4.9177861e-06,\n",
            "        3.2606833e-05, 3.4402587e-05],\n",
            "       [2.1444883e-03, 7.8334004e-01, 3.8087987e-03, ..., 1.5852903e-04,\n",
            "        1.6544376e-03, 2.3448472e-04],\n",
            "       ...,\n",
            "       [2.8327480e-05, 1.1340235e-04, 6.6222252e-05, ..., 5.3726591e-05,\n",
            "        1.9228123e-06, 3.6867350e-06],\n",
            "       [1.8937605e-03, 9.3113042e-02, 6.8750731e-03, ..., 2.4036306e-03,\n",
            "        3.1064276e-04, 2.1510453e-04],\n",
            "       [3.6262881e-04, 1.2953053e-01, 2.3142308e-02, ..., 3.0745909e-05,\n",
            "        7.5251667e-04, 2.7950920e-04]], dtype=float32), array([[1.08623906e-04, 1.64640776e-04, 9.19289159e-05, ...,\n",
            "        2.79286951e-06, 4.54365618e-05, 1.33986168e-05],\n",
            "       [1.70678704e-03, 6.49014302e-03, 7.54814318e-05, ...,\n",
            "        8.16228348e-05, 5.91003982e-07, 2.20261878e-04],\n",
            "       [2.77786457e-04, 9.24314380e-01, 2.22800212e-04, ...,\n",
            "        7.47887389e-05, 1.23930440e-05, 1.31933019e-04],\n",
            "       ...,\n",
            "       [1.84191704e-05, 1.12543930e-04, 7.67782403e-05, ...,\n",
            "        6.42903478e-06, 6.19058701e-05, 2.51652273e-06],\n",
            "       [2.91226944e-03, 6.53514862e-02, 1.15147121e-02, ...,\n",
            "        3.33199074e-04, 3.77663382e-04, 7.28819694e-04],\n",
            "       [2.05450837e-04, 3.48783284e-01, 5.90689667e-02, ...,\n",
            "        7.71772058e-04, 6.72618626e-05, 4.72774205e-04]], dtype=float32), array([[1.1813521e-04, 1.6779435e-04, 9.0771837e-06, ..., 6.5610703e-07,\n",
            "        1.0354875e-06, 1.5358450e-05],\n",
            "       [9.8917400e-03, 4.8388219e-01, 1.2801448e-02, ..., 8.2977269e-05,\n",
            "        1.5616130e-05, 1.2454796e-03],\n",
            "       [1.2915848e-03, 9.3857950e-01, 9.2645420e-04, ..., 3.1519467e-05,\n",
            "        2.0147915e-04, 1.8608861e-04],\n",
            "       ...,\n",
            "       [3.2372933e-05, 1.3986393e-04, 3.7307131e-05, ..., 3.7643272e-06,\n",
            "        3.6740771e-06, 3.2606586e-05],\n",
            "       [1.5670675e-03, 5.6676269e-02, 4.3548609e-04, ..., 7.5369528e-05,\n",
            "        4.8347400e-05, 4.5988642e-04],\n",
            "       [3.2503487e-04, 7.5960279e-01, 1.1593792e-02, ..., 3.0309698e-04,\n",
            "        6.6289118e-05, 5.0879602e-04]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p5TICLipj5Z8",
        "colab_type": "code",
        "outputId": "c24e347c-bc0f-4f52-e8f9-dd1d06bb2d67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "np.argmax(best_model_predictions[0][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "metadata": {
        "id": "Pn3JN27ekrM-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "best_model_predictions_argmax = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WcEnfITPZNBS",
        "colab_type": "code",
        "outputId": "b4b78e7a-fcfa-4012-db18-b1d62061c09f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/JoSungHun/Deeplearning.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Deeplearning'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/20)   \u001b[K\rremote: Counting objects:  10% (2/20)   \u001b[K\rremote: Counting objects:  15% (3/20)   \u001b[K\rremote: Counting objects:  20% (4/20)   \u001b[K\rremote: Counting objects:  25% (5/20)   \u001b[K\rremote: Counting objects:  30% (6/20)   \u001b[K\rremote: Counting objects:  35% (7/20)   \u001b[K\rremote: Counting objects:  40% (8/20)   \u001b[K\rremote: Counting objects:  45% (9/20)   \u001b[K\rremote: Counting objects:  50% (10/20)   \u001b[K\rremote: Counting objects:  55% (11/20)   \u001b[K\rremote: Counting objects:  60% (12/20)   \u001b[K\rremote: Counting objects:  65% (13/20)   \u001b[K\rremote: Counting objects:  70% (14/20)   \u001b[K\rremote: Counting objects:  75% (15/20)   \u001b[K\rremote: Counting objects:  80% (16/20)   \u001b[K\rremote: Counting objects:  85% (17/20)   \u001b[K\rremote: Counting objects:  90% (18/20)   \u001b[K\rremote: Counting objects:  95% (19/20)   \u001b[K\rremote: Counting objects: 100% (20/20)   \u001b[K\rremote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects:   5% (1/18)   \u001b[K\rremote: Compressing objects:  11% (2/18)   \u001b[K\rremote: Compressing objects:  16% (3/18)   \u001b[K\rremote: Compressing objects:  22% (4/18)   \u001b[K\rremote: Compressing objects:  27% (5/18)   \u001b[K\rremote: Compressing objects:  33% (6/18)   \u001b[K\rremote: Compressing objects:  38% (7/18)   \u001b[K\rremote: Compressing objects:  44% (8/18)   \u001b[K\rremote: Compressing objects:  50% (9/18)   \u001b[K\rremote: Compressing objects:  55% (10/18)   \u001b[K\rremote: Compressing objects:  61% (11/18)   \u001b[K\rremote: Compressing objects:  66% (12/18)   \u001b[K\rremote: Compressing objects:  72% (13/18)   \u001b[K\rremote: Compressing objects:  77% (14/18)   \u001b[K\rremote: Compressing objects:  83% (15/18)   \u001b[K\rremote: Compressing objects:  88% (16/18)   \u001b[K\rremote: Compressing objects:  94% (17/18)   \u001b[K\rremote: Compressing objects: 100% (18/18)   \u001b[K\rremote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "Unpacking objects:   5% (1/20)   \rUnpacking objects:  10% (2/20)   \rUnpacking objects:  15% (3/20)   \rUnpacking objects:  20% (4/20)   \rUnpacking objects:  25% (5/20)   \rUnpacking objects:  30% (6/20)   \rUnpacking objects:  35% (7/20)   \rUnpacking objects:  40% (8/20)   \rremote: Total 20 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  45% (9/20)   \rUnpacking objects:  50% (10/20)   \rUnpacking objects:  55% (11/20)   \rUnpacking objects:  60% (12/20)   \rUnpacking objects:  65% (13/20)   \rUnpacking objects:  70% (14/20)   \rUnpacking objects:  75% (15/20)   \rUnpacking objects:  80% (16/20)   \rUnpacking objects:  85% (17/20)   \rUnpacking objects:  90% (18/20)   \rUnpacking objects:  95% (19/20)   \rUnpacking objects: 100% (20/20)   \rUnpacking objects: 100% (20/20), done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}