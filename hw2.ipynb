{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2_14113359_조성훈.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoSungHun/Deeplearning/blob/master/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6vFXgflvZ6Ga",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 14113359 조성훈 HW2\n",
        "## Assignment"
      ]
    },
    {
      "metadata": {
        "id": "UevAU0CYVhiu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Due date: **2018/04/09 00:00** (will not accept late submission)\n",
        "* Submittion format: notebook file which can be executed in Colab environment\n",
        "\n",
        "* We want to build a multi-class classification model using Reuters dataset."
      ]
    },
    {
      "metadata": {
        "id": "KxrhNNhWbfHD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> ### Loading and preprocessing data"
      ]
    },
    {
      "metadata": {
        "id": "8E-2WLHgcAit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import reuters\n",
        "\n",
        "# Like IMDB, the argument num_words restricts the data to \n",
        "# the 10,000 most frequently occurring words \n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yrjD9AerdW4X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZncD2OhTpMIK",
        "colab_type": "code",
        "outputId": "f203cdbf-8668-46ec-b213-8686bc17f42e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3, 10,  1, ...,  3,  3, 24])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "metadata": {
        "id": "NmLRa5GNeSOi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UzoKfo9PUArN",
        "colab_type": "code",
        "outputId": "8768d0c7-7b2a-4570-ae1a-9586fb2fbeeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2246"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "metadata": {
        "id": "GSjE_6ZavO2-",
        "colab_type": "code",
        "outputId": "d94da818-c318-4bbc-d5e3-def6f959c438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_labels.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2246,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "metadata": {
        "id": "Cigo4yG7pG-x",
        "colab_type": "code",
        "outputId": "5fc2d323-7cbd-45a5-ffa8-d9ce263c6577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "cell_type": "code",
      "source": [
        "one_hot_test_labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "metadata": {
        "id": "yWfgUJiPeicE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> ### Building the network"
      ]
    },
    {
      "metadata": {
        "id": "Edwc4GbneqKA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(46, activation='softmax'))\n",
        "  model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j0TUwiHLgI_-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> ### Validation"
      ]
    },
    {
      "metadata": {
        "id": "KFe4FOkfgUII",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* We employ *k-fold cross validation* for monitoring the performance of trained model.\n",
        "* Write a code in the below to perform *10-fold cross validation*.\n",
        "* **For each fold, save a model at every epoch in your Google Drive.**"
      ]
    },
    {
      "metadata": {
        "id": "ZKOrz-PXN1nf",
        "colab_type": "code",
        "outputId": "6374271e-7455-48d3-e122-f52c37420bd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive#구글드라이브 마운트 및 파일경로 설정\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "drive.mount('/content/gdrive')\n",
        "# filepath = '/content/gdrive/My Drive/hw2_callbacks/hw2_model.(폴드).{에폭}.hdf5'\n",
        "# for 문을 통해 경로 설정을 하고 싶었지만 이상하게 오류가 뜨며 안되서 스태틱하게 배열로 저장\n",
        "filepath = ['/content/gdrive/My Drive/hw2_callbacks/hw2_model.0.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.1.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.2.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.3.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.4.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.5.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.6.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.7.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.8.{epoch:02d}.hdf5',\n",
        "              '/content/gdrive/My Drive/hw2_callbacks/hw2_model.9.{epoch:02d}.hdf5']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bri7RrUs3Nf5",
        "colab_type": "code",
        "outputId": "f63e9cce-6000-408b-e061-56a66dfaf91b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "%cd hw2_callbacks"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/hw2_callbacks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jPWjg2s4WYfm",
        "colab_type": "code",
        "outputId": "ea6e1455-12f4-4b47-a5c8-5c17bcd27ebd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35569
        }
      },
      "cell_type": "code",
      "source": [
        "# write a code for 10-fold cross validation here\n",
        "\n",
        "k = 10\n",
        "num_val_samples = len(x_train) // k\n",
        "num_epochs = 100\n",
        "all_train_acc = []\n",
        "all_val_acc = []\n",
        "for i in range(k):\n",
        "  print('처리중인 폴드 #', i)\n",
        "  \n",
        "  modelckpt = ModelCheckpoint(filepath=filepath[i]) \n",
        "  # \n",
        "  val_data = x_train[i * num_val_samples: (i+1) * num_val_samples]\n",
        "  val_labels = one_hot_train_labels[i * num_val_samples: (i+1) * num_val_samples]\n",
        "  \n",
        "  partial_train_data = np.concatenate([x_train[:i * num_val_samples], x_train[(i + 1) * num_val_samples:]], axis=0)\n",
        "  partial_train_labels = np.concatenate([one_hot_train_labels[:i * num_val_samples], one_hot_train_labels[(i + 1) * num_val_samples:]], axis=0)\n",
        "    \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data,\n",
        "                      partial_train_labels,\n",
        "                      epochs=num_epochs,\n",
        "                      batch_size=512,\n",
        "                      validation_data=(val_data, val_labels),\n",
        "                      callbacks=[modelckpt])\n",
        "  \n",
        "  train_acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  \n",
        "  all_train_acc.append(train_acc)\n",
        "  all_val_acc.append(val_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "처리중인 폴드 # 0\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 8s 943us/step - loss: 2.5157 - acc: 0.5148 - val_loss: 1.7344 - val_acc: 0.5969\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 1.4390 - acc: 0.6879 - val_loss: 1.3630 - val_acc: 0.7116\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 1.0890 - acc: 0.7672 - val_loss: 1.1935 - val_acc: 0.7361\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.8648 - acc: 0.8152 - val_loss: 1.0857 - val_acc: 0.7717\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.6950 - acc: 0.8537 - val_loss: 1.0224 - val_acc: 0.7862\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.5573 - acc: 0.8816 - val_loss: 0.9984 - val_acc: 0.7906\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.4488 - acc: 0.9064 - val_loss: 0.9252 - val_acc: 0.8062\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 93us/step - loss: 0.3621 - acc: 0.9231 - val_loss: 0.9292 - val_acc: 0.8062\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.2992 - acc: 0.9315 - val_loss: 0.9184 - val_acc: 0.8073\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.2492 - acc: 0.9410 - val_loss: 0.9219 - val_acc: 0.8163\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.2107 - acc: 0.9499 - val_loss: 0.9571 - val_acc: 0.8040\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.1873 - acc: 0.9503 - val_loss: 1.0270 - val_acc: 0.7951\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1639 - acc: 0.9545 - val_loss: 0.9889 - val_acc: 0.8051\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1497 - acc: 0.9547 - val_loss: 0.9974 - val_acc: 0.8129\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1412 - acc: 0.9545 - val_loss: 1.0245 - val_acc: 0.8029\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1316 - acc: 0.9578 - val_loss: 1.0505 - val_acc: 0.7996\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.1244 - acc: 0.9570 - val_loss: 1.0730 - val_acc: 0.8040\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1174 - acc: 0.9568 - val_loss: 1.0656 - val_acc: 0.8096\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1143 - acc: 0.9578 - val_loss: 1.1405 - val_acc: 0.7851\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1114 - acc: 0.9562 - val_loss: 1.1540 - val_acc: 0.7973\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 92us/step - loss: 0.1049 - acc: 0.9598 - val_loss: 1.1543 - val_acc: 0.7918\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1087 - acc: 0.9566 - val_loss: 1.1582 - val_acc: 0.7940\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0989 - acc: 0.9587 - val_loss: 1.2066 - val_acc: 0.7929\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0993 - acc: 0.9591 - val_loss: 1.1566 - val_acc: 0.7862\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0988 - acc: 0.9571 - val_loss: 1.2378 - val_acc: 0.7862\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0980 - acc: 0.9586 - val_loss: 1.2540 - val_acc: 0.7840\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0980 - acc: 0.9587 - val_loss: 1.2298 - val_acc: 0.7840\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0938 - acc: 0.9570 - val_loss: 1.2130 - val_acc: 0.7951\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0944 - acc: 0.9578 - val_loss: 1.2526 - val_acc: 0.7951\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0933 - acc: 0.9582 - val_loss: 1.2344 - val_acc: 0.7873\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0889 - acc: 0.9613 - val_loss: 1.2728 - val_acc: 0.7862\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0959 - acc: 0.9574 - val_loss: 1.2881 - val_acc: 0.7895\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0897 - acc: 0.9583 - val_loss: 1.3003 - val_acc: 0.7884\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0916 - acc: 0.9578 - val_loss: 1.2674 - val_acc: 0.7851\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0870 - acc: 0.9583 - val_loss: 1.2904 - val_acc: 0.7851\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0874 - acc: 0.9605 - val_loss: 1.3279 - val_acc: 0.7862\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0872 - acc: 0.9578 - val_loss: 1.3180 - val_acc: 0.7829\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0851 - acc: 0.9593 - val_loss: 1.3116 - val_acc: 0.7918\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0860 - acc: 0.9581 - val_loss: 1.3256 - val_acc: 0.7906\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0839 - acc: 0.9589 - val_loss: 1.3544 - val_acc: 0.7851\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0860 - acc: 0.9577 - val_loss: 1.3784 - val_acc: 0.7751\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0851 - acc: 0.9573 - val_loss: 1.3350 - val_acc: 0.7873\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0837 - acc: 0.9589 - val_loss: 1.3771 - val_acc: 0.7739\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0821 - acc: 0.9600 - val_loss: 1.3620 - val_acc: 0.7862\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0809 - acc: 0.9576 - val_loss: 1.3471 - val_acc: 0.7795\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0853 - acc: 0.9589 - val_loss: 1.3427 - val_acc: 0.7795\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0798 - acc: 0.9595 - val_loss: 1.3969 - val_acc: 0.7706\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0831 - acc: 0.9589 - val_loss: 1.3685 - val_acc: 0.7851\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0821 - acc: 0.9592 - val_loss: 1.3907 - val_acc: 0.7773\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0806 - acc: 0.9574 - val_loss: 1.3842 - val_acc: 0.7795\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0807 - acc: 0.9588 - val_loss: 1.4284 - val_acc: 0.7806\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0800 - acc: 0.9568 - val_loss: 1.3886 - val_acc: 0.7806\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0798 - acc: 0.9587 - val_loss: 1.4432 - val_acc: 0.7617\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0790 - acc: 0.9588 - val_loss: 1.4440 - val_acc: 0.7717\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0804 - acc: 0.9567 - val_loss: 1.4475 - val_acc: 0.7728\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0764 - acc: 0.9586 - val_loss: 1.4476 - val_acc: 0.7751\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0777 - acc: 0.9603 - val_loss: 1.4290 - val_acc: 0.7806\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0767 - acc: 0.9592 - val_loss: 1.4305 - val_acc: 0.7739\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0757 - acc: 0.9598 - val_loss: 1.4479 - val_acc: 0.7762\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0754 - acc: 0.9592 - val_loss: 1.4759 - val_acc: 0.7751\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0779 - acc: 0.9570 - val_loss: 1.4745 - val_acc: 0.7695\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0732 - acc: 0.9593 - val_loss: 1.4736 - val_acc: 0.7706\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0762 - acc: 0.9577 - val_loss: 1.4728 - val_acc: 0.7795\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0743 - acc: 0.9587 - val_loss: 1.5439 - val_acc: 0.7617\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0734 - acc: 0.9591 - val_loss: 1.4896 - val_acc: 0.7795\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0754 - acc: 0.9572 - val_loss: 1.5522 - val_acc: 0.7661\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0740 - acc: 0.9587 - val_loss: 1.5226 - val_acc: 0.7728\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0736 - acc: 0.9579 - val_loss: 1.5013 - val_acc: 0.7717\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0730 - acc: 0.9598 - val_loss: 1.5354 - val_acc: 0.7717\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0747 - acc: 0.9567 - val_loss: 1.5428 - val_acc: 0.7684\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0725 - acc: 0.9583 - val_loss: 1.5541 - val_acc: 0.7695\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0721 - acc: 0.9584 - val_loss: 1.5272 - val_acc: 0.7684\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0711 - acc: 0.9592 - val_loss: 1.5601 - val_acc: 0.7751\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0705 - acc: 0.9588 - val_loss: 1.5826 - val_acc: 0.7684\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0703 - acc: 0.9600 - val_loss: 1.5553 - val_acc: 0.7728\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0725 - acc: 0.9582 - val_loss: 1.5576 - val_acc: 0.7706\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0712 - acc: 0.9567 - val_loss: 1.6330 - val_acc: 0.7539\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0697 - acc: 0.9593 - val_loss: 1.5573 - val_acc: 0.7661\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0686 - acc: 0.9604 - val_loss: 1.7115 - val_acc: 0.7483\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0708 - acc: 0.9573 - val_loss: 1.6543 - val_acc: 0.7572\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0688 - acc: 0.9591 - val_loss: 1.6329 - val_acc: 0.7639\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 93us/step - loss: 0.0689 - acc: 0.9603 - val_loss: 1.6404 - val_acc: 0.7561\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0703 - acc: 0.9578 - val_loss: 1.6285 - val_acc: 0.7706\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0685 - acc: 0.9591 - val_loss: 1.6138 - val_acc: 0.7695\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0689 - acc: 0.9578 - val_loss: 1.6693 - val_acc: 0.7617\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0668 - acc: 0.9593 - val_loss: 1.6627 - val_acc: 0.7739\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0687 - acc: 0.9591 - val_loss: 1.6597 - val_acc: 0.7673\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0681 - acc: 0.9584 - val_loss: 1.6328 - val_acc: 0.7639\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0674 - acc: 0.9597 - val_loss: 1.6670 - val_acc: 0.7539\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0672 - acc: 0.9587 - val_loss: 1.6557 - val_acc: 0.7717\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0674 - acc: 0.9589 - val_loss: 1.6720 - val_acc: 0.7584\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0667 - acc: 0.9594 - val_loss: 1.6810 - val_acc: 0.7684\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0678 - acc: 0.9572 - val_loss: 1.7321 - val_acc: 0.7650\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0660 - acc: 0.9594 - val_loss: 1.6858 - val_acc: 0.7650\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0675 - acc: 0.9583 - val_loss: 1.7409 - val_acc: 0.7673\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0659 - acc: 0.9572 - val_loss: 1.7205 - val_acc: 0.7706\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0668 - acc: 0.9573 - val_loss: 1.7394 - val_acc: 0.7650\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0646 - acc: 0.9598 - val_loss: 1.7271 - val_acc: 0.7639\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0667 - acc: 0.9581 - val_loss: 1.7547 - val_acc: 0.7561\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0652 - acc: 0.9597 - val_loss: 1.7252 - val_acc: 0.7673\n",
            "처리중인 폴드 # 1\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 8s 1ms/step - loss: 2.6411 - acc: 0.5426 - val_loss: 1.7605 - val_acc: 0.6670\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 1.4016 - acc: 0.7109 - val_loss: 1.3919 - val_acc: 0.7027\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 1.0391 - acc: 0.7791 - val_loss: 1.2534 - val_acc: 0.7294\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.8197 - acc: 0.8252 - val_loss: 1.1685 - val_acc: 0.7461\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.6542 - acc: 0.8634 - val_loss: 1.0646 - val_acc: 0.7728\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.5260 - acc: 0.8923 - val_loss: 1.0279 - val_acc: 0.7739\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.4222 - acc: 0.9146 - val_loss: 1.0804 - val_acc: 0.7695\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.3426 - acc: 0.9300 - val_loss: 1.0133 - val_acc: 0.7851\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.2823 - acc: 0.9399 - val_loss: 0.9850 - val_acc: 0.7884\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.2425 - acc: 0.9461 - val_loss: 0.9971 - val_acc: 0.7862\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 93us/step - loss: 0.2038 - acc: 0.9510 - val_loss: 1.0056 - val_acc: 0.8040\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1810 - acc: 0.9519 - val_loss: 0.9860 - val_acc: 0.7918\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1664 - acc: 0.9527 - val_loss: 1.0305 - val_acc: 0.7918\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1532 - acc: 0.9521 - val_loss: 1.0978 - val_acc: 0.7918\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 0.1430 - acc: 0.9555 - val_loss: 1.0431 - val_acc: 0.7984\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1299 - acc: 0.9542 - val_loss: 1.0591 - val_acc: 0.7940\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1264 - acc: 0.9568 - val_loss: 1.0826 - val_acc: 0.7962\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1243 - acc: 0.9550 - val_loss: 1.0997 - val_acc: 0.7962\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.1123 - acc: 0.9571 - val_loss: 1.2271 - val_acc: 0.7862\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.1151 - acc: 0.9577 - val_loss: 1.0951 - val_acc: 0.7973\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.1121 - acc: 0.9557 - val_loss: 1.1931 - val_acc: 0.7906\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.1051 - acc: 0.9577 - val_loss: 1.1806 - val_acc: 0.7918\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1033 - acc: 0.9584 - val_loss: 1.2139 - val_acc: 0.7996\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1031 - acc: 0.9586 - val_loss: 1.2491 - val_acc: 0.7906\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1031 - acc: 0.9567 - val_loss: 1.2357 - val_acc: 0.7884\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1000 - acc: 0.9568 - val_loss: 1.2179 - val_acc: 0.7929\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0994 - acc: 0.9563 - val_loss: 1.2451 - val_acc: 0.7951\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0959 - acc: 0.9568 - val_loss: 1.2301 - val_acc: 0.7951\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0951 - acc: 0.9577 - val_loss: 1.2873 - val_acc: 0.7873\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0970 - acc: 0.9583 - val_loss: 1.3037 - val_acc: 0.7829\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0953 - acc: 0.9579 - val_loss: 1.2928 - val_acc: 0.7851\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0918 - acc: 0.9584 - val_loss: 1.2557 - val_acc: 0.7951\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0932 - acc: 0.9584 - val_loss: 1.3606 - val_acc: 0.7829\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0927 - acc: 0.9572 - val_loss: 1.3510 - val_acc: 0.7906\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0948 - acc: 0.9586 - val_loss: 1.3431 - val_acc: 0.7929\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0868 - acc: 0.9608 - val_loss: 1.3906 - val_acc: 0.7829\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0952 - acc: 0.9573 - val_loss: 1.3829 - val_acc: 0.7918\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0901 - acc: 0.9574 - val_loss: 1.3270 - val_acc: 0.7940\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0887 - acc: 0.9591 - val_loss: 1.3587 - val_acc: 0.7851\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0896 - acc: 0.9571 - val_loss: 1.3517 - val_acc: 0.7873\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0876 - acc: 0.9571 - val_loss: 1.3889 - val_acc: 0.7895\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0861 - acc: 0.9593 - val_loss: 1.3895 - val_acc: 0.7829\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0863 - acc: 0.9578 - val_loss: 1.3869 - val_acc: 0.7895\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0841 - acc: 0.9589 - val_loss: 1.3800 - val_acc: 0.7884\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0883 - acc: 0.9570 - val_loss: 1.4015 - val_acc: 0.7806\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0851 - acc: 0.9593 - val_loss: 1.4479 - val_acc: 0.7773\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0866 - acc: 0.9576 - val_loss: 1.4356 - val_acc: 0.7918\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0832 - acc: 0.9600 - val_loss: 1.4632 - val_acc: 0.7784\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0847 - acc: 0.9574 - val_loss: 1.4852 - val_acc: 0.7739\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0850 - acc: 0.9586 - val_loss: 1.4355 - val_acc: 0.7840\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0823 - acc: 0.9566 - val_loss: 1.4278 - val_acc: 0.7829\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0815 - acc: 0.9602 - val_loss: 1.4029 - val_acc: 0.7906\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0838 - acc: 0.9595 - val_loss: 1.4355 - val_acc: 0.7906\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0810 - acc: 0.9574 - val_loss: 1.5026 - val_acc: 0.7773\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0809 - acc: 0.9576 - val_loss: 1.4465 - val_acc: 0.7884\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0828 - acc: 0.9582 - val_loss: 1.4971 - val_acc: 0.7751\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0792 - acc: 0.9588 - val_loss: 1.4822 - val_acc: 0.7706\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0793 - acc: 0.9592 - val_loss: 1.4291 - val_acc: 0.7806\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0799 - acc: 0.9589 - val_loss: 1.4047 - val_acc: 0.7918\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0791 - acc: 0.9591 - val_loss: 1.5261 - val_acc: 0.7728\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0802 - acc: 0.9578 - val_loss: 1.4799 - val_acc: 0.7817\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0751 - acc: 0.9610 - val_loss: 1.4400 - val_acc: 0.7895\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 93us/step - loss: 0.0781 - acc: 0.9599 - val_loss: 1.5641 - val_acc: 0.7717\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0776 - acc: 0.9591 - val_loss: 1.5774 - val_acc: 0.7650\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0783 - acc: 0.9583 - val_loss: 1.4756 - val_acc: 0.7873\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0731 - acc: 0.9609 - val_loss: 1.4708 - val_acc: 0.7806\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0784 - acc: 0.9588 - val_loss: 1.4829 - val_acc: 0.7717\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0766 - acc: 0.9578 - val_loss: 1.5726 - val_acc: 0.7661\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0762 - acc: 0.9593 - val_loss: 1.5723 - val_acc: 0.7673\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0770 - acc: 0.9570 - val_loss: 1.4980 - val_acc: 0.7829\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0751 - acc: 0.9594 - val_loss: 1.5236 - val_acc: 0.7762\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0750 - acc: 0.9583 - val_loss: 1.5927 - val_acc: 0.7606\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0736 - acc: 0.9588 - val_loss: 1.5606 - val_acc: 0.7717\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0749 - acc: 0.9582 - val_loss: 1.5334 - val_acc: 0.7751\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0747 - acc: 0.9572 - val_loss: 1.6113 - val_acc: 0.7695\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0741 - acc: 0.9593 - val_loss: 1.5692 - val_acc: 0.7661\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0727 - acc: 0.9594 - val_loss: 1.5897 - val_acc: 0.7684\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0726 - acc: 0.9602 - val_loss: 1.5484 - val_acc: 0.7706\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0726 - acc: 0.9589 - val_loss: 1.5611 - val_acc: 0.7706\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0699 - acc: 0.9607 - val_loss: 1.6057 - val_acc: 0.7684\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0709 - acc: 0.9604 - val_loss: 1.5606 - val_acc: 0.7728\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0737 - acc: 0.9587 - val_loss: 1.6014 - val_acc: 0.7717\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0712 - acc: 0.9595 - val_loss: 1.5308 - val_acc: 0.7739\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0719 - acc: 0.9589 - val_loss: 1.5964 - val_acc: 0.7639\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0710 - acc: 0.9586 - val_loss: 1.6262 - val_acc: 0.7695\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0706 - acc: 0.9603 - val_loss: 1.6017 - val_acc: 0.7706\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0712 - acc: 0.9577 - val_loss: 1.6028 - val_acc: 0.7639\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0709 - acc: 0.9579 - val_loss: 1.6257 - val_acc: 0.7673\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0691 - acc: 0.9610 - val_loss: 1.6294 - val_acc: 0.7728\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0692 - acc: 0.9584 - val_loss: 1.6549 - val_acc: 0.7595\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0676 - acc: 0.9602 - val_loss: 1.6977 - val_acc: 0.7717\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0686 - acc: 0.9618 - val_loss: 1.7276 - val_acc: 0.7528\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 0.0686 - acc: 0.9602 - val_loss: 1.6575 - val_acc: 0.7595\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0693 - acc: 0.9591 - val_loss: 1.6503 - val_acc: 0.7751\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0682 - acc: 0.9591 - val_loss: 1.6684 - val_acc: 0.7628\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0669 - acc: 0.9583 - val_loss: 1.6943 - val_acc: 0.7650\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0676 - acc: 0.9589 - val_loss: 1.7553 - val_acc: 0.7494\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0676 - acc: 0.9588 - val_loss: 1.6661 - val_acc: 0.7661\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0676 - acc: 0.9592 - val_loss: 1.7643 - val_acc: 0.7506\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0669 - acc: 0.9591 - val_loss: 1.7321 - val_acc: 0.7628\n",
            "처리중인 폴드 # 2\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 8s 1ms/step - loss: 2.5920 - acc: 0.5345 - val_loss: 1.8003 - val_acc: 0.6425\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 1.4351 - acc: 0.7034 - val_loss: 1.3756 - val_acc: 0.6915\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 1.0623 - acc: 0.7686 - val_loss: 1.1906 - val_acc: 0.7506\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.8340 - acc: 0.8231 - val_loss: 1.1044 - val_acc: 0.7595\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.6641 - acc: 0.8616 - val_loss: 1.0567 - val_acc: 0.7829\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.5306 - acc: 0.8937 - val_loss: 0.9902 - val_acc: 0.7884\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.4306 - acc: 0.9114 - val_loss: 0.9580 - val_acc: 0.7984\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.3489 - acc: 0.9286 - val_loss: 0.9756 - val_acc: 0.7918\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.2893 - acc: 0.9383 - val_loss: 0.9953 - val_acc: 0.7951\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.2404 - acc: 0.9459 - val_loss: 1.0094 - val_acc: 0.7918\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 0.2087 - acc: 0.9494 - val_loss: 1.0011 - val_acc: 0.7951\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.1834 - acc: 0.9527 - val_loss: 0.9830 - val_acc: 0.8062\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 92us/step - loss: 0.1621 - acc: 0.9540 - val_loss: 1.0039 - val_acc: 0.8051\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 0.1524 - acc: 0.9547 - val_loss: 1.0057 - val_acc: 0.8040\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.1353 - acc: 0.9558 - val_loss: 1.0703 - val_acc: 0.7929\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.1328 - acc: 0.9582 - val_loss: 1.0367 - val_acc: 0.7984\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.1270 - acc: 0.9573 - val_loss: 1.0444 - val_acc: 0.8018\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1184 - acc: 0.9581 - val_loss: 1.0756 - val_acc: 0.7973\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1127 - acc: 0.9573 - val_loss: 1.1188 - val_acc: 0.7940\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1078 - acc: 0.9589 - val_loss: 1.1241 - val_acc: 0.7962\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1097 - acc: 0.9576 - val_loss: 1.1258 - val_acc: 0.7973\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.1029 - acc: 0.9600 - val_loss: 1.1349 - val_acc: 0.7951\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1039 - acc: 0.9586 - val_loss: 1.1249 - val_acc: 0.7929\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1028 - acc: 0.9594 - val_loss: 1.1873 - val_acc: 0.7973\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0998 - acc: 0.9595 - val_loss: 1.1764 - val_acc: 0.7918\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0969 - acc: 0.9584 - val_loss: 1.2107 - val_acc: 0.7906\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0991 - acc: 0.9587 - val_loss: 1.2200 - val_acc: 0.7906\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0955 - acc: 0.9578 - val_loss: 1.2833 - val_acc: 0.7817\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0960 - acc: 0.9586 - val_loss: 1.2229 - val_acc: 0.7940\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0921 - acc: 0.9594 - val_loss: 1.2494 - val_acc: 0.7918\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0921 - acc: 0.9597 - val_loss: 1.3047 - val_acc: 0.7851\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0936 - acc: 0.9608 - val_loss: 1.2549 - val_acc: 0.7884\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0942 - acc: 0.9581 - val_loss: 1.2602 - val_acc: 0.7884\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0881 - acc: 0.9612 - val_loss: 1.2498 - val_acc: 0.7906\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0880 - acc: 0.9592 - val_loss: 1.2693 - val_acc: 0.7884\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0863 - acc: 0.9612 - val_loss: 1.3398 - val_acc: 0.7817\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0890 - acc: 0.9609 - val_loss: 1.2622 - val_acc: 0.7851\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0871 - acc: 0.9593 - val_loss: 1.3199 - val_acc: 0.7817\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0867 - acc: 0.9608 - val_loss: 1.3420 - val_acc: 0.7906\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0841 - acc: 0.9604 - val_loss: 1.3950 - val_acc: 0.7751\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0852 - acc: 0.9599 - val_loss: 1.4084 - val_acc: 0.7784\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0872 - acc: 0.9607 - val_loss: 1.3154 - val_acc: 0.7873\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0847 - acc: 0.9614 - val_loss: 1.3835 - val_acc: 0.7784\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0833 - acc: 0.9583 - val_loss: 1.3431 - val_acc: 0.7829\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0836 - acc: 0.9594 - val_loss: 1.3939 - val_acc: 0.7817\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0823 - acc: 0.9605 - val_loss: 1.3390 - val_acc: 0.7829\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0852 - acc: 0.9597 - val_loss: 1.3592 - val_acc: 0.7884\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0830 - acc: 0.9604 - val_loss: 1.3855 - val_acc: 0.7795\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0811 - acc: 0.9604 - val_loss: 1.3826 - val_acc: 0.7829\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0839 - acc: 0.9602 - val_loss: 1.3979 - val_acc: 0.7840\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0795 - acc: 0.9607 - val_loss: 1.4367 - val_acc: 0.7806\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0805 - acc: 0.9600 - val_loss: 1.3857 - val_acc: 0.7795\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0816 - acc: 0.9604 - val_loss: 1.4516 - val_acc: 0.7784\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0777 - acc: 0.9623 - val_loss: 1.4712 - val_acc: 0.7762\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0803 - acc: 0.9599 - val_loss: 1.4080 - val_acc: 0.7762\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0795 - acc: 0.9619 - val_loss: 1.4621 - val_acc: 0.7751\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0787 - acc: 0.9623 - val_loss: 1.4634 - val_acc: 0.7817\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0777 - acc: 0.9591 - val_loss: 1.5546 - val_acc: 0.7739\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0775 - acc: 0.9614 - val_loss: 1.4775 - val_acc: 0.7739\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0784 - acc: 0.9595 - val_loss: 1.4897 - val_acc: 0.7806\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0773 - acc: 0.9604 - val_loss: 1.4347 - val_acc: 0.7739\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0773 - acc: 0.9598 - val_loss: 1.4722 - val_acc: 0.7795\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0750 - acc: 0.9604 - val_loss: 1.5390 - val_acc: 0.7806\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0768 - acc: 0.9603 - val_loss: 1.5223 - val_acc: 0.7728\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 92us/step - loss: 0.0761 - acc: 0.9597 - val_loss: 1.4706 - val_acc: 0.7773\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0731 - acc: 0.9618 - val_loss: 1.5628 - val_acc: 0.7706\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0759 - acc: 0.9612 - val_loss: 1.5405 - val_acc: 0.7840\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0749 - acc: 0.9604 - val_loss: 1.5486 - val_acc: 0.7728\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 92us/step - loss: 0.0725 - acc: 0.9592 - val_loss: 1.5914 - val_acc: 0.7795\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0721 - acc: 0.9603 - val_loss: 1.5161 - val_acc: 0.7784\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0751 - acc: 0.9600 - val_loss: 1.5923 - val_acc: 0.7739\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0731 - acc: 0.9597 - val_loss: 1.5945 - val_acc: 0.7739\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0726 - acc: 0.9604 - val_loss: 1.5758 - val_acc: 0.7806\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0726 - acc: 0.9618 - val_loss: 1.6308 - val_acc: 0.7762\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0729 - acc: 0.9618 - val_loss: 1.5880 - val_acc: 0.7773\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0702 - acc: 0.9612 - val_loss: 1.6612 - val_acc: 0.7650\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0714 - acc: 0.9603 - val_loss: 1.6111 - val_acc: 0.7829\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 93us/step - loss: 0.0711 - acc: 0.9618 - val_loss: 1.5872 - val_acc: 0.7773\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0698 - acc: 0.9621 - val_loss: 1.6242 - val_acc: 0.7739\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0702 - acc: 0.9614 - val_loss: 1.6688 - val_acc: 0.7806\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0702 - acc: 0.9602 - val_loss: 1.6496 - val_acc: 0.7717\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0704 - acc: 0.9589 - val_loss: 1.6579 - val_acc: 0.7706\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0701 - acc: 0.9609 - val_loss: 1.6678 - val_acc: 0.7773\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0687 - acc: 0.9610 - val_loss: 1.6273 - val_acc: 0.7795\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0697 - acc: 0.9592 - val_loss: 1.6593 - val_acc: 0.7773\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0690 - acc: 0.9605 - val_loss: 1.7100 - val_acc: 0.7739\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0674 - acc: 0.9605 - val_loss: 1.6108 - val_acc: 0.7728\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0693 - acc: 0.9625 - val_loss: 1.7020 - val_acc: 0.7739\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0675 - acc: 0.9610 - val_loss: 1.6723 - val_acc: 0.7751\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0687 - acc: 0.9609 - val_loss: 1.7112 - val_acc: 0.7695\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0671 - acc: 0.9594 - val_loss: 1.6971 - val_acc: 0.7784\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0679 - acc: 0.9602 - val_loss: 1.7308 - val_acc: 0.7728\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0681 - acc: 0.9593 - val_loss: 1.7720 - val_acc: 0.7762\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0672 - acc: 0.9610 - val_loss: 1.7745 - val_acc: 0.7650\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0671 - acc: 0.9608 - val_loss: 1.7974 - val_acc: 0.7673\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0656 - acc: 0.9614 - val_loss: 1.7602 - val_acc: 0.7684\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0667 - acc: 0.9617 - val_loss: 1.8189 - val_acc: 0.7684\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0668 - acc: 0.9629 - val_loss: 1.7989 - val_acc: 0.7684\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0653 - acc: 0.9597 - val_loss: 1.7897 - val_acc: 0.7706\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0664 - acc: 0.9591 - val_loss: 1.8012 - val_acc: 0.7695\n",
            "처리중인 폴드 # 3\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 8s 1ms/step - loss: 2.5649 - acc: 0.4762 - val_loss: 1.7547 - val_acc: 0.6481\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 1.4203 - acc: 0.7057 - val_loss: 1.3147 - val_acc: 0.7171\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 1.0452 - acc: 0.7786 - val_loss: 1.1412 - val_acc: 0.7450\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.8232 - acc: 0.8237 - val_loss: 1.0396 - val_acc: 0.7695\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.6541 - acc: 0.8641 - val_loss: 0.9802 - val_acc: 0.7773\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 94us/step - loss: 0.5171 - acc: 0.8936 - val_loss: 0.9416 - val_acc: 0.7851\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.4204 - acc: 0.9151 - val_loss: 0.9217 - val_acc: 0.7951\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.3418 - acc: 0.9278 - val_loss: 0.9082 - val_acc: 0.7951\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.2802 - acc: 0.9411 - val_loss: 0.9158 - val_acc: 0.7940\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.2446 - acc: 0.9451 - val_loss: 0.9369 - val_acc: 0.7962\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.2074 - acc: 0.9498 - val_loss: 0.9470 - val_acc: 0.7929\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1793 - acc: 0.9555 - val_loss: 0.9954 - val_acc: 0.7940\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.1686 - acc: 0.9534 - val_loss: 1.0068 - val_acc: 0.7973\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1469 - acc: 0.9568 - val_loss: 1.0460 - val_acc: 0.7962\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.1348 - acc: 0.9579 - val_loss: 1.0421 - val_acc: 0.7906\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1292 - acc: 0.9574 - val_loss: 1.0803 - val_acc: 0.7806\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.1250 - acc: 0.9589 - val_loss: 1.0733 - val_acc: 0.7829\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.1211 - acc: 0.9576 - val_loss: 1.0578 - val_acc: 0.7962\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.1077 - acc: 0.9620 - val_loss: 1.1134 - val_acc: 0.7817\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1163 - acc: 0.9582 - val_loss: 1.1014 - val_acc: 0.7884\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1063 - acc: 0.9602 - val_loss: 1.1949 - val_acc: 0.7762\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1028 - acc: 0.9612 - val_loss: 1.1667 - val_acc: 0.7851\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1005 - acc: 0.9593 - val_loss: 1.1800 - val_acc: 0.7840\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1013 - acc: 0.9588 - val_loss: 1.2154 - val_acc: 0.7840\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1017 - acc: 0.9605 - val_loss: 1.2712 - val_acc: 0.7717\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0950 - acc: 0.9617 - val_loss: 1.2436 - val_acc: 0.7829\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1004 - acc: 0.9604 - val_loss: 1.2584 - val_acc: 0.7806\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0965 - acc: 0.9599 - val_loss: 1.2452 - val_acc: 0.7951\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0938 - acc: 0.9599 - val_loss: 1.3043 - val_acc: 0.7862\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0948 - acc: 0.9598 - val_loss: 1.3466 - val_acc: 0.7806\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0951 - acc: 0.9598 - val_loss: 1.3590 - val_acc: 0.7695\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0901 - acc: 0.9605 - val_loss: 1.3604 - val_acc: 0.7728\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0931 - acc: 0.9600 - val_loss: 1.3417 - val_acc: 0.7795\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0899 - acc: 0.9621 - val_loss: 1.3111 - val_acc: 0.7884\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0880 - acc: 0.9628 - val_loss: 1.3628 - val_acc: 0.7773\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0941 - acc: 0.9605 - val_loss: 1.3562 - val_acc: 0.7829\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0846 - acc: 0.9618 - val_loss: 1.4209 - val_acc: 0.7728\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 92us/step - loss: 0.0900 - acc: 0.9597 - val_loss: 1.3688 - val_acc: 0.7817\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0870 - acc: 0.9602 - val_loss: 1.3623 - val_acc: 0.7851\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0882 - acc: 0.9602 - val_loss: 1.4127 - val_acc: 0.7751\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0852 - acc: 0.9630 - val_loss: 1.3998 - val_acc: 0.7806\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0849 - acc: 0.9610 - val_loss: 1.4808 - val_acc: 0.7673\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0851 - acc: 0.9623 - val_loss: 1.3577 - val_acc: 0.7862\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0849 - acc: 0.9600 - val_loss: 1.4100 - val_acc: 0.7784\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0846 - acc: 0.9621 - val_loss: 1.3951 - val_acc: 0.7817\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0817 - acc: 0.9615 - val_loss: 1.4489 - val_acc: 0.7840\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0836 - acc: 0.9603 - val_loss: 1.4249 - val_acc: 0.7762\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0819 - acc: 0.9624 - val_loss: 1.4389 - val_acc: 0.7817\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 0.0841 - acc: 0.9593 - val_loss: 1.4822 - val_acc: 0.7717\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0816 - acc: 0.9619 - val_loss: 1.4769 - val_acc: 0.7739\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0788 - acc: 0.9619 - val_loss: 1.4600 - val_acc: 0.7817\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0819 - acc: 0.9587 - val_loss: 1.4913 - val_acc: 0.7739\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 93us/step - loss: 0.0810 - acc: 0.9603 - val_loss: 1.5237 - val_acc: 0.7639\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0788 - acc: 0.9605 - val_loss: 1.5615 - val_acc: 0.7617\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0777 - acc: 0.9619 - val_loss: 1.5023 - val_acc: 0.7762\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0782 - acc: 0.9608 - val_loss: 1.5300 - val_acc: 0.7773\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0780 - acc: 0.9609 - val_loss: 1.5405 - val_acc: 0.7728\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 0.0792 - acc: 0.9599 - val_loss: 1.5293 - val_acc: 0.7762\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0769 - acc: 0.9621 - val_loss: 1.4694 - val_acc: 0.7840\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0767 - acc: 0.9603 - val_loss: 1.5168 - val_acc: 0.7795\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0788 - acc: 0.9603 - val_loss: 1.5225 - val_acc: 0.7795\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0779 - acc: 0.9602 - val_loss: 1.5689 - val_acc: 0.7717\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0757 - acc: 0.9610 - val_loss: 1.5266 - val_acc: 0.7762\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0753 - acc: 0.9618 - val_loss: 1.5768 - val_acc: 0.7673\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0761 - acc: 0.9610 - val_loss: 1.5459 - val_acc: 0.7806\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0755 - acc: 0.9597 - val_loss: 1.5479 - val_acc: 0.7717\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0715 - acc: 0.9625 - val_loss: 1.6583 - val_acc: 0.7595\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0746 - acc: 0.9614 - val_loss: 1.6246 - val_acc: 0.7572\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0756 - acc: 0.9604 - val_loss: 1.5476 - val_acc: 0.7751\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0729 - acc: 0.9610 - val_loss: 1.5714 - val_acc: 0.7762\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0746 - acc: 0.9610 - val_loss: 1.5745 - val_acc: 0.7751\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0738 - acc: 0.9612 - val_loss: 1.6162 - val_acc: 0.7684\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0722 - acc: 0.9600 - val_loss: 1.6264 - val_acc: 0.7695\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0715 - acc: 0.9610 - val_loss: 1.6125 - val_acc: 0.7650\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0713 - acc: 0.9634 - val_loss: 1.5987 - val_acc: 0.7739\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0723 - acc: 0.9603 - val_loss: 1.6056 - val_acc: 0.7728\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0708 - acc: 0.9608 - val_loss: 1.6110 - val_acc: 0.7773\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0714 - acc: 0.9623 - val_loss: 1.6397 - val_acc: 0.7661\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0703 - acc: 0.9617 - val_loss: 1.6765 - val_acc: 0.7684\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0690 - acc: 0.9612 - val_loss: 1.6791 - val_acc: 0.7639\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0699 - acc: 0.9612 - val_loss: 1.6677 - val_acc: 0.7617\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0693 - acc: 0.9621 - val_loss: 1.6367 - val_acc: 0.7673\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0704 - acc: 0.9618 - val_loss: 1.6254 - val_acc: 0.7717\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0676 - acc: 0.9612 - val_loss: 1.6867 - val_acc: 0.7639\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0696 - acc: 0.9589 - val_loss: 1.6817 - val_acc: 0.7717\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0690 - acc: 0.9604 - val_loss: 1.6762 - val_acc: 0.7684\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0667 - acc: 0.9604 - val_loss: 1.7031 - val_acc: 0.7539\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 0.0688 - acc: 0.9609 - val_loss: 1.7030 - val_acc: 0.7628\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0679 - acc: 0.9615 - val_loss: 1.7470 - val_acc: 0.7550\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0653 - acc: 0.9636 - val_loss: 1.7138 - val_acc: 0.7684\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0669 - acc: 0.9593 - val_loss: 1.7321 - val_acc: 0.7728\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0665 - acc: 0.9612 - val_loss: 1.7285 - val_acc: 0.7684\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0682 - acc: 0.9609 - val_loss: 1.7042 - val_acc: 0.7673\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0653 - acc: 0.9628 - val_loss: 1.7145 - val_acc: 0.7773\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0646 - acc: 0.9626 - val_loss: 1.7362 - val_acc: 0.7728\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0660 - acc: 0.9614 - val_loss: 1.7537 - val_acc: 0.7728\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0661 - acc: 0.9630 - val_loss: 1.7507 - val_acc: 0.7628\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0653 - acc: 0.9612 - val_loss: 1.7765 - val_acc: 0.7628\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0649 - acc: 0.9617 - val_loss: 1.7525 - val_acc: 0.7650\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0655 - acc: 0.9623 - val_loss: 1.7650 - val_acc: 0.7639\n",
            "처리중인 폴드 # 4\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 8s 1ms/step - loss: 2.5101 - acc: 0.5374 - val_loss: 1.7051 - val_acc: 0.6537\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 1.4097 - acc: 0.6999 - val_loss: 1.3510 - val_acc: 0.6949\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 1.0588 - acc: 0.7716 - val_loss: 1.1774 - val_acc: 0.7494\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.8340 - acc: 0.8243 - val_loss: 1.0955 - val_acc: 0.7595\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.6579 - acc: 0.8616 - val_loss: 1.0059 - val_acc: 0.7884\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.5232 - acc: 0.8902 - val_loss: 0.9736 - val_acc: 0.7940\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.4205 - acc: 0.9145 - val_loss: 0.9699 - val_acc: 0.7918\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.3416 - acc: 0.9283 - val_loss: 0.9696 - val_acc: 0.7996\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.2792 - acc: 0.9400 - val_loss: 0.9548 - val_acc: 0.7973\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.2390 - acc: 0.9454 - val_loss: 0.9356 - val_acc: 0.8051\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.2013 - acc: 0.9518 - val_loss: 0.9891 - val_acc: 0.7906\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1800 - acc: 0.9530 - val_loss: 0.9843 - val_acc: 0.7929\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.1589 - acc: 0.9557 - val_loss: 0.9989 - val_acc: 0.8007\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1473 - acc: 0.9568 - val_loss: 1.0252 - val_acc: 0.7973\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1357 - acc: 0.9583 - val_loss: 1.0353 - val_acc: 0.7873\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.1283 - acc: 0.9571 - val_loss: 1.0661 - val_acc: 0.8029\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1203 - acc: 0.9612 - val_loss: 1.0793 - val_acc: 0.7951\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1170 - acc: 0.9599 - val_loss: 1.1242 - val_acc: 0.7951\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1129 - acc: 0.9579 - val_loss: 1.1565 - val_acc: 0.7873\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.1087 - acc: 0.9588 - val_loss: 1.1443 - val_acc: 0.7840\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1043 - acc: 0.9593 - val_loss: 1.1366 - val_acc: 0.7962\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1061 - acc: 0.9589 - val_loss: 1.1804 - val_acc: 0.7929\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0998 - acc: 0.9593 - val_loss: 1.2072 - val_acc: 0.7929\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.1009 - acc: 0.9588 - val_loss: 1.2606 - val_acc: 0.7795\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0977 - acc: 0.9591 - val_loss: 1.2013 - val_acc: 0.7951\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0955 - acc: 0.9605 - val_loss: 1.2550 - val_acc: 0.7840\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0927 - acc: 0.9620 - val_loss: 1.2952 - val_acc: 0.7817\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0954 - acc: 0.9587 - val_loss: 1.2891 - val_acc: 0.7784\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0921 - acc: 0.9589 - val_loss: 1.3175 - val_acc: 0.7762\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0944 - acc: 0.9584 - val_loss: 1.3071 - val_acc: 0.7851\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0892 - acc: 0.9586 - val_loss: 1.3323 - val_acc: 0.7862\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0900 - acc: 0.9593 - val_loss: 1.2772 - val_acc: 0.7918\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 0.0878 - acc: 0.9600 - val_loss: 1.3224 - val_acc: 0.7851\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0884 - acc: 0.9602 - val_loss: 1.3367 - val_acc: 0.7906\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0900 - acc: 0.9588 - val_loss: 1.3501 - val_acc: 0.7773\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0893 - acc: 0.9597 - val_loss: 1.3330 - val_acc: 0.7918\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0870 - acc: 0.9600 - val_loss: 1.4319 - val_acc: 0.7639\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0898 - acc: 0.9579 - val_loss: 1.3788 - val_acc: 0.7840\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0835 - acc: 0.9603 - val_loss: 1.3716 - val_acc: 0.7951\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0832 - acc: 0.9608 - val_loss: 1.4169 - val_acc: 0.7895\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0850 - acc: 0.9615 - val_loss: 1.4108 - val_acc: 0.7929\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0815 - acc: 0.9614 - val_loss: 1.4306 - val_acc: 0.7784\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0835 - acc: 0.9604 - val_loss: 1.3907 - val_acc: 0.7873\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0830 - acc: 0.9597 - val_loss: 1.4470 - val_acc: 0.7862\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0836 - acc: 0.9589 - val_loss: 1.4000 - val_acc: 0.7873\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0823 - acc: 0.9615 - val_loss: 1.4601 - val_acc: 0.7751\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0833 - acc: 0.9597 - val_loss: 1.4427 - val_acc: 0.7851\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0790 - acc: 0.9610 - val_loss: 1.4530 - val_acc: 0.7918\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0826 - acc: 0.9599 - val_loss: 1.4625 - val_acc: 0.7906\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0814 - acc: 0.9608 - val_loss: 1.4865 - val_acc: 0.7773\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0790 - acc: 0.9602 - val_loss: 1.4482 - val_acc: 0.7895\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0795 - acc: 0.9626 - val_loss: 1.5066 - val_acc: 0.7929\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0759 - acc: 0.9605 - val_loss: 1.4876 - val_acc: 0.7840\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0787 - acc: 0.9603 - val_loss: 1.5398 - val_acc: 0.7829\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0772 - acc: 0.9615 - val_loss: 1.5583 - val_acc: 0.7751\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0756 - acc: 0.9631 - val_loss: 1.5226 - val_acc: 0.7806\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0785 - acc: 0.9608 - val_loss: 1.4935 - val_acc: 0.7884\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0764 - acc: 0.9604 - val_loss: 1.5561 - val_acc: 0.7806\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0763 - acc: 0.9597 - val_loss: 1.5304 - val_acc: 0.7873\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0750 - acc: 0.9618 - val_loss: 1.5923 - val_acc: 0.7851\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0753 - acc: 0.9600 - val_loss: 1.5077 - val_acc: 0.7762\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0761 - acc: 0.9591 - val_loss: 1.5860 - val_acc: 0.7762\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0744 - acc: 0.9600 - val_loss: 1.5604 - val_acc: 0.7862\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0746 - acc: 0.9599 - val_loss: 1.5365 - val_acc: 0.7795\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0744 - acc: 0.9604 - val_loss: 1.6280 - val_acc: 0.7795\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0735 - acc: 0.9607 - val_loss: 1.6065 - val_acc: 0.7739\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0720 - acc: 0.9614 - val_loss: 1.6286 - val_acc: 0.7840\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0739 - acc: 0.9605 - val_loss: 1.6124 - val_acc: 0.7795\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0729 - acc: 0.9610 - val_loss: 1.6505 - val_acc: 0.7817\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0720 - acc: 0.9603 - val_loss: 1.6683 - val_acc: 0.7795\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0729 - acc: 0.9619 - val_loss: 1.6296 - val_acc: 0.7773\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0714 - acc: 0.9615 - val_loss: 1.6741 - val_acc: 0.7795\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0714 - acc: 0.9608 - val_loss: 1.6030 - val_acc: 0.7773\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0708 - acc: 0.9605 - val_loss: 1.6715 - val_acc: 0.7717\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0714 - acc: 0.9586 - val_loss: 1.6887 - val_acc: 0.7840\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0705 - acc: 0.9604 - val_loss: 1.6728 - val_acc: 0.7717\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0703 - acc: 0.9615 - val_loss: 1.7168 - val_acc: 0.7795\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0696 - acc: 0.9604 - val_loss: 1.7173 - val_acc: 0.7706\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0680 - acc: 0.9620 - val_loss: 1.7381 - val_acc: 0.7706\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0701 - acc: 0.9597 - val_loss: 1.6737 - val_acc: 0.7706\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0698 - acc: 0.9609 - val_loss: 1.6999 - val_acc: 0.7739\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0697 - acc: 0.9602 - val_loss: 1.7179 - val_acc: 0.7784\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0696 - acc: 0.9597 - val_loss: 1.7232 - val_acc: 0.7739\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0688 - acc: 0.9613 - val_loss: 1.7087 - val_acc: 0.7684\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0671 - acc: 0.9617 - val_loss: 1.7171 - val_acc: 0.7728\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0691 - acc: 0.9608 - val_loss: 1.7188 - val_acc: 0.7661\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0656 - acc: 0.9629 - val_loss: 1.7165 - val_acc: 0.7673\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0676 - acc: 0.9597 - val_loss: 1.7536 - val_acc: 0.7717\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0665 - acc: 0.9612 - val_loss: 1.7529 - val_acc: 0.7728\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0673 - acc: 0.9619 - val_loss: 1.7823 - val_acc: 0.7728\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0670 - acc: 0.9623 - val_loss: 1.7554 - val_acc: 0.7706\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0656 - acc: 0.9614 - val_loss: 1.7731 - val_acc: 0.7684\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0661 - acc: 0.9594 - val_loss: 1.7890 - val_acc: 0.7739\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0657 - acc: 0.9603 - val_loss: 1.8199 - val_acc: 0.7650\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0661 - acc: 0.9598 - val_loss: 1.8330 - val_acc: 0.7617\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0655 - acc: 0.9605 - val_loss: 1.8023 - val_acc: 0.7650\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0656 - acc: 0.9603 - val_loss: 1.8319 - val_acc: 0.7661\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0647 - acc: 0.9603 - val_loss: 1.7746 - val_acc: 0.7784\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0644 - acc: 0.9623 - val_loss: 1.8429 - val_acc: 0.7673\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0639 - acc: 0.9592 - val_loss: 1.8294 - val_acc: 0.7728\n",
            "처리중인 폴드 # 5\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 8s 1ms/step - loss: 2.5931 - acc: 0.5407 - val_loss: 1.7147 - val_acc: 0.6481\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 1.3727 - acc: 0.7084 - val_loss: 1.3477 - val_acc: 0.7194\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 1.0296 - acc: 0.7851 - val_loss: 1.2478 - val_acc: 0.7316\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.8214 - acc: 0.8266 - val_loss: 1.0830 - val_acc: 0.7784\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.6613 - acc: 0.8637 - val_loss: 1.0431 - val_acc: 0.7706\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.5302 - acc: 0.8900 - val_loss: 0.9763 - val_acc: 0.7829\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.4287 - acc: 0.9101 - val_loss: 0.9354 - val_acc: 0.8040\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.3488 - acc: 0.9269 - val_loss: 0.9446 - val_acc: 0.7973\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 92us/step - loss: 0.2903 - acc: 0.9372 - val_loss: 0.9553 - val_acc: 0.7940\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.2426 - acc: 0.9437 - val_loss: 0.9121 - val_acc: 0.8040\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.2139 - acc: 0.9482 - val_loss: 0.9353 - val_acc: 0.8073\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1861 - acc: 0.9495 - val_loss: 0.9386 - val_acc: 0.8040\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.1705 - acc: 0.9503 - val_loss: 0.9399 - val_acc: 0.8151\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.1558 - acc: 0.9540 - val_loss: 1.0094 - val_acc: 0.8096\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1427 - acc: 0.9545 - val_loss: 1.0070 - val_acc: 0.8118\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1389 - acc: 0.9570 - val_loss: 1.0430 - val_acc: 0.8018\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1289 - acc: 0.9556 - val_loss: 1.1018 - val_acc: 0.7951\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1254 - acc: 0.9574 - val_loss: 1.0854 - val_acc: 0.8062\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1172 - acc: 0.9563 - val_loss: 1.0699 - val_acc: 0.8051\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1215 - acc: 0.9551 - val_loss: 1.1175 - val_acc: 0.7973\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1116 - acc: 0.9563 - val_loss: 1.1216 - val_acc: 0.8029\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1117 - acc: 0.9563 - val_loss: 1.1491 - val_acc: 0.8029\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1108 - acc: 0.9563 - val_loss: 1.1609 - val_acc: 0.7906\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1076 - acc: 0.9566 - val_loss: 1.2050 - val_acc: 0.7962\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1074 - acc: 0.9568 - val_loss: 1.2472 - val_acc: 0.7784\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.1052 - acc: 0.9563 - val_loss: 1.2388 - val_acc: 0.7829\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1030 - acc: 0.9574 - val_loss: 1.1814 - val_acc: 0.7962\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1028 - acc: 0.9568 - val_loss: 1.2458 - val_acc: 0.7862\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0985 - acc: 0.9568 - val_loss: 1.3719 - val_acc: 0.7639\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1012 - acc: 0.9570 - val_loss: 1.2535 - val_acc: 0.7929\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0977 - acc: 0.9567 - val_loss: 1.2794 - val_acc: 0.7940\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0990 - acc: 0.9558 - val_loss: 1.2612 - val_acc: 0.7906\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0993 - acc: 0.9560 - val_loss: 1.2602 - val_acc: 0.8018\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.1001 - acc: 0.9555 - val_loss: 1.2863 - val_acc: 0.7884\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 94us/step - loss: 0.0909 - acc: 0.9582 - val_loss: 1.3129 - val_acc: 0.7973\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0938 - acc: 0.9556 - val_loss: 1.3320 - val_acc: 0.7851\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0942 - acc: 0.9576 - val_loss: 1.2772 - val_acc: 0.7918\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0947 - acc: 0.9565 - val_loss: 1.3197 - val_acc: 0.7940\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 0.0899 - acc: 0.9578 - val_loss: 1.3015 - val_acc: 0.8040\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0920 - acc: 0.9568 - val_loss: 1.3724 - val_acc: 0.7829\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0925 - acc: 0.9560 - val_loss: 1.3751 - val_acc: 0.7862\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0922 - acc: 0.9567 - val_loss: 1.4696 - val_acc: 0.7695\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0895 - acc: 0.9561 - val_loss: 1.3489 - val_acc: 0.7918\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0921 - acc: 0.9561 - val_loss: 1.3748 - val_acc: 0.7817\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0889 - acc: 0.9571 - val_loss: 1.3141 - val_acc: 0.7940\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0888 - acc: 0.9562 - val_loss: 1.3413 - val_acc: 0.7951\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0844 - acc: 0.9574 - val_loss: 1.4436 - val_acc: 0.7806\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0889 - acc: 0.9568 - val_loss: 1.3425 - val_acc: 0.7906\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0870 - acc: 0.9573 - val_loss: 1.3612 - val_acc: 0.7873\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0858 - acc: 0.9578 - val_loss: 1.3923 - val_acc: 0.7951\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0871 - acc: 0.9566 - val_loss: 1.3925 - val_acc: 0.7851\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0858 - acc: 0.9577 - val_loss: 1.4146 - val_acc: 0.7817\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0855 - acc: 0.9586 - val_loss: 1.3933 - val_acc: 0.7873\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0841 - acc: 0.9579 - val_loss: 1.4365 - val_acc: 0.7829\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0848 - acc: 0.9558 - val_loss: 1.3978 - val_acc: 0.7829\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 92us/step - loss: 0.0835 - acc: 0.9563 - val_loss: 1.4582 - val_acc: 0.7762\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0822 - acc: 0.9567 - val_loss: 1.3877 - val_acc: 0.7873\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0833 - acc: 0.9573 - val_loss: 1.3970 - val_acc: 0.7851\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0796 - acc: 0.9607 - val_loss: 1.4368 - val_acc: 0.7873\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0822 - acc: 0.9550 - val_loss: 1.4745 - val_acc: 0.7806\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0830 - acc: 0.9584 - val_loss: 1.4345 - val_acc: 0.7829\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0818 - acc: 0.9573 - val_loss: 1.4164 - val_acc: 0.7851\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0829 - acc: 0.9541 - val_loss: 1.4587 - val_acc: 0.7762\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0782 - acc: 0.9578 - val_loss: 1.4185 - val_acc: 0.7873\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0793 - acc: 0.9579 - val_loss: 1.4811 - val_acc: 0.7706\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0799 - acc: 0.9576 - val_loss: 1.4761 - val_acc: 0.7806\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0788 - acc: 0.9570 - val_loss: 1.5768 - val_acc: 0.7661\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0801 - acc: 0.9570 - val_loss: 1.5093 - val_acc: 0.7739\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0776 - acc: 0.9592 - val_loss: 1.5027 - val_acc: 0.7739\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0798 - acc: 0.9565 - val_loss: 1.4884 - val_acc: 0.7762\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0772 - acc: 0.9578 - val_loss: 1.4560 - val_acc: 0.7806\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0786 - acc: 0.9553 - val_loss: 1.4763 - val_acc: 0.7895\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0778 - acc: 0.9567 - val_loss: 1.5290 - val_acc: 0.7706\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0772 - acc: 0.9582 - val_loss: 1.5647 - val_acc: 0.7728\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0758 - acc: 0.9578 - val_loss: 1.5100 - val_acc: 0.7739\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0761 - acc: 0.9583 - val_loss: 1.5659 - val_acc: 0.7706\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0774 - acc: 0.9581 - val_loss: 1.5558 - val_acc: 0.7661\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0764 - acc: 0.9573 - val_loss: 1.5230 - val_acc: 0.7784\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0763 - acc: 0.9592 - val_loss: 1.5300 - val_acc: 0.7739\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0759 - acc: 0.9561 - val_loss: 1.5393 - val_acc: 0.7795\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0761 - acc: 0.9587 - val_loss: 1.5737 - val_acc: 0.7706\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0748 - acc: 0.9574 - val_loss: 1.6124 - val_acc: 0.7639\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0743 - acc: 0.9589 - val_loss: 1.5587 - val_acc: 0.7751\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0727 - acc: 0.9583 - val_loss: 1.5504 - val_acc: 0.7817\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0732 - acc: 0.9565 - val_loss: 1.5732 - val_acc: 0.7639\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0730 - acc: 0.9581 - val_loss: 1.6249 - val_acc: 0.7650\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0750 - acc: 0.9558 - val_loss: 1.5678 - val_acc: 0.7695\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0713 - acc: 0.9584 - val_loss: 1.6267 - val_acc: 0.7617\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0736 - acc: 0.9582 - val_loss: 1.6710 - val_acc: 0.7628\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0726 - acc: 0.9586 - val_loss: 1.5819 - val_acc: 0.7717\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0726 - acc: 0.9574 - val_loss: 1.6679 - val_acc: 0.7606\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0730 - acc: 0.9561 - val_loss: 1.5782 - val_acc: 0.7673\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0704 - acc: 0.9598 - val_loss: 1.6018 - val_acc: 0.7784\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0718 - acc: 0.9568 - val_loss: 1.6775 - val_acc: 0.7628\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0723 - acc: 0.9587 - val_loss: 1.6369 - val_acc: 0.7706\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0703 - acc: 0.9577 - val_loss: 1.6992 - val_acc: 0.7584\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0713 - acc: 0.9593 - val_loss: 1.6757 - val_acc: 0.7606\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0708 - acc: 0.9586 - val_loss: 1.6405 - val_acc: 0.7728\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0713 - acc: 0.9570 - val_loss: 1.6318 - val_acc: 0.7762\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0718 - acc: 0.9565 - val_loss: 1.6970 - val_acc: 0.7572\n",
            "처리중인 폴드 # 6\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 8s 1ms/step - loss: 2.6836 - acc: 0.5252 - val_loss: 1.7440 - val_acc: 0.6559\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 1.4416 - acc: 0.7058 - val_loss: 1.2549 - val_acc: 0.7316\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 1.0444 - acc: 0.7799 - val_loss: 1.0634 - val_acc: 0.7751\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.8067 - acc: 0.8342 - val_loss: 0.9448 - val_acc: 0.7940\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.6374 - acc: 0.8707 - val_loss: 0.8789 - val_acc: 0.8007\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 0.5073 - acc: 0.8962 - val_loss: 0.8365 - val_acc: 0.8129\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.4097 - acc: 0.9140 - val_loss: 0.7910 - val_acc: 0.8241\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.3297 - acc: 0.9297 - val_loss: 0.7747 - val_acc: 0.8241\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.2791 - acc: 0.9378 - val_loss: 0.8163 - val_acc: 0.8118\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.2388 - acc: 0.9453 - val_loss: 0.7735 - val_acc: 0.8218\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.2057 - acc: 0.9472 - val_loss: 0.8113 - val_acc: 0.8207\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1819 - acc: 0.9525 - val_loss: 0.8115 - val_acc: 0.8140\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1661 - acc: 0.9535 - val_loss: 0.8572 - val_acc: 0.8107\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.1551 - acc: 0.9541 - val_loss: 0.8200 - val_acc: 0.8241\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1415 - acc: 0.9555 - val_loss: 0.8622 - val_acc: 0.8185\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1356 - acc: 0.9536 - val_loss: 0.8531 - val_acc: 0.8252\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.1320 - acc: 0.9567 - val_loss: 0.8702 - val_acc: 0.8163\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1205 - acc: 0.9565 - val_loss: 0.8928 - val_acc: 0.8185\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1210 - acc: 0.9563 - val_loss: 0.9145 - val_acc: 0.8174\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.1142 - acc: 0.9584 - val_loss: 0.9520 - val_acc: 0.8163\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1113 - acc: 0.9577 - val_loss: 0.9341 - val_acc: 0.8174\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.1125 - acc: 0.9566 - val_loss: 0.9449 - val_acc: 0.8107\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.1078 - acc: 0.9571 - val_loss: 0.9260 - val_acc: 0.8174\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.1065 - acc: 0.9562 - val_loss: 0.9485 - val_acc: 0.8151\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.1052 - acc: 0.9584 - val_loss: 1.0005 - val_acc: 0.8151\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1017 - acc: 0.9582 - val_loss: 0.9800 - val_acc: 0.8140\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0995 - acc: 0.9586 - val_loss: 0.9654 - val_acc: 0.8185\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1006 - acc: 0.9567 - val_loss: 1.0331 - val_acc: 0.8040\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0990 - acc: 0.9583 - val_loss: 1.0253 - val_acc: 0.8062\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0980 - acc: 0.9586 - val_loss: 1.0011 - val_acc: 0.8140\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0951 - acc: 0.9584 - val_loss: 1.0281 - val_acc: 0.8040\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0983 - acc: 0.9587 - val_loss: 1.0566 - val_acc: 0.8107\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0968 - acc: 0.9588 - val_loss: 1.0080 - val_acc: 0.8140\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0938 - acc: 0.9595 - val_loss: 1.0705 - val_acc: 0.8085\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0964 - acc: 0.9573 - val_loss: 1.0655 - val_acc: 0.8029\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0922 - acc: 0.9600 - val_loss: 1.0163 - val_acc: 0.8163\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0910 - acc: 0.9589 - val_loss: 1.0377 - val_acc: 0.8118\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0938 - acc: 0.9581 - val_loss: 1.0284 - val_acc: 0.8140\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0927 - acc: 0.9582 - val_loss: 1.0840 - val_acc: 0.7984\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0903 - acc: 0.9582 - val_loss: 1.1016 - val_acc: 0.7996\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0890 - acc: 0.9583 - val_loss: 1.0844 - val_acc: 0.8107\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0886 - acc: 0.9572 - val_loss: 1.1406 - val_acc: 0.7940\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0935 - acc: 0.9555 - val_loss: 1.0830 - val_acc: 0.8085\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0900 - acc: 0.9570 - val_loss: 1.0883 - val_acc: 0.8062\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 92us/step - loss: 0.0904 - acc: 0.9572 - val_loss: 1.1157 - val_acc: 0.7973\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0866 - acc: 0.9592 - val_loss: 1.0914 - val_acc: 0.8062\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0885 - acc: 0.9560 - val_loss: 1.0980 - val_acc: 0.8140\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0889 - acc: 0.9572 - val_loss: 1.0886 - val_acc: 0.8029\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 92us/step - loss: 0.0820 - acc: 0.9603 - val_loss: 1.1182 - val_acc: 0.8107\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0841 - acc: 0.9586 - val_loss: 1.1415 - val_acc: 0.7996\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0859 - acc: 0.9584 - val_loss: 1.0811 - val_acc: 0.8107\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0830 - acc: 0.9577 - val_loss: 1.0865 - val_acc: 0.8118\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0830 - acc: 0.9588 - val_loss: 1.1165 - val_acc: 0.8096\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0809 - acc: 0.9595 - val_loss: 1.1713 - val_acc: 0.7973\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0826 - acc: 0.9583 - val_loss: 1.1597 - val_acc: 0.7996\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0824 - acc: 0.9598 - val_loss: 1.1957 - val_acc: 0.8040\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0815 - acc: 0.9571 - val_loss: 1.1469 - val_acc: 0.8029\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0816 - acc: 0.9578 - val_loss: 1.1485 - val_acc: 0.8018\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0790 - acc: 0.9586 - val_loss: 1.1609 - val_acc: 0.8073\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0798 - acc: 0.9587 - val_loss: 1.1557 - val_acc: 0.8040\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0808 - acc: 0.9573 - val_loss: 1.1842 - val_acc: 0.7973\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0778 - acc: 0.9588 - val_loss: 1.1517 - val_acc: 0.8018\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0791 - acc: 0.9582 - val_loss: 1.2061 - val_acc: 0.8040\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0788 - acc: 0.9583 - val_loss: 1.2289 - val_acc: 0.7862\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0772 - acc: 0.9586 - val_loss: 1.1545 - val_acc: 0.8062\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0772 - acc: 0.9568 - val_loss: 1.2239 - val_acc: 0.7973\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0775 - acc: 0.9563 - val_loss: 1.1843 - val_acc: 0.8040\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0760 - acc: 0.9572 - val_loss: 1.2316 - val_acc: 0.7918\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0763 - acc: 0.9583 - val_loss: 1.2145 - val_acc: 0.8096\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0735 - acc: 0.9604 - val_loss: 1.1964 - val_acc: 0.8040\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0761 - acc: 0.9581 - val_loss: 1.1935 - val_acc: 0.7996\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0751 - acc: 0.9583 - val_loss: 1.2356 - val_acc: 0.7940\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0739 - acc: 0.9563 - val_loss: 1.2419 - val_acc: 0.7996\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0736 - acc: 0.9584 - val_loss: 1.2640 - val_acc: 0.7940\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0734 - acc: 0.9583 - val_loss: 1.2512 - val_acc: 0.7973\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0736 - acc: 0.9578 - val_loss: 1.2460 - val_acc: 0.8018\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0718 - acc: 0.9592 - val_loss: 1.2946 - val_acc: 0.7884\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0728 - acc: 0.9576 - val_loss: 1.2689 - val_acc: 0.7929\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0705 - acc: 0.9599 - val_loss: 1.2565 - val_acc: 0.7929\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0716 - acc: 0.9579 - val_loss: 1.3200 - val_acc: 0.7873\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0708 - acc: 0.9581 - val_loss: 1.2639 - val_acc: 0.7973\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0709 - acc: 0.9572 - val_loss: 1.3279 - val_acc: 0.7906\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0720 - acc: 0.9584 - val_loss: 1.3301 - val_acc: 0.7873\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 93us/step - loss: 0.0694 - acc: 0.9572 - val_loss: 1.3462 - val_acc: 0.7895\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0707 - acc: 0.9584 - val_loss: 1.3401 - val_acc: 0.7918\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0699 - acc: 0.9581 - val_loss: 1.3118 - val_acc: 0.7940\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0687 - acc: 0.9597 - val_loss: 1.3431 - val_acc: 0.7840\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 0.0697 - acc: 0.9592 - val_loss: 1.3590 - val_acc: 0.7918\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0700 - acc: 0.9589 - val_loss: 1.3722 - val_acc: 0.7851\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0689 - acc: 0.9561 - val_loss: 1.3381 - val_acc: 0.7851\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0671 - acc: 0.9588 - val_loss: 1.3841 - val_acc: 0.7918\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0694 - acc: 0.9567 - val_loss: 1.3928 - val_acc: 0.7840\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0685 - acc: 0.9584 - val_loss: 1.4344 - val_acc: 0.7906\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0674 - acc: 0.9579 - val_loss: 1.4011 - val_acc: 0.7884\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0676 - acc: 0.9586 - val_loss: 1.4156 - val_acc: 0.7851\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0674 - acc: 0.9586 - val_loss: 1.3871 - val_acc: 0.7873\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 0.0668 - acc: 0.9587 - val_loss: 1.4221 - val_acc: 0.7862\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0664 - acc: 0.9588 - val_loss: 1.4052 - val_acc: 0.7951\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0661 - acc: 0.9592 - val_loss: 1.4163 - val_acc: 0.7918\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0660 - acc: 0.9578 - val_loss: 1.4705 - val_acc: 0.7840\n",
            "처리중인 폴드 # 7\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 9s 1ms/step - loss: 2.7401 - acc: 0.5015 - val_loss: 1.8463 - val_acc: 0.6514\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 1.5448 - acc: 0.6816 - val_loss: 1.3311 - val_acc: 0.7350\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 1.1028 - acc: 0.7658 - val_loss: 1.1025 - val_acc: 0.7762\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.8480 - acc: 0.8231 - val_loss: 1.0100 - val_acc: 0.7884\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.6656 - acc: 0.8607 - val_loss: 0.9367 - val_acc: 0.7996\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.5302 - acc: 0.8928 - val_loss: 0.8966 - val_acc: 0.8062\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.4232 - acc: 0.9153 - val_loss: 0.8717 - val_acc: 0.8096\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.3379 - acc: 0.9299 - val_loss: 0.8747 - val_acc: 0.8196\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.2808 - acc: 0.9404 - val_loss: 0.8957 - val_acc: 0.8185\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.2323 - acc: 0.9479 - val_loss: 0.9276 - val_acc: 0.8118\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.2002 - acc: 0.9524 - val_loss: 0.9186 - val_acc: 0.8140\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1779 - acc: 0.9553 - val_loss: 0.9337 - val_acc: 0.8140\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1598 - acc: 0.9565 - val_loss: 1.0220 - val_acc: 0.8062\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1449 - acc: 0.9566 - val_loss: 0.9951 - val_acc: 0.8129\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.1317 - acc: 0.9591 - val_loss: 1.0554 - val_acc: 0.8040\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1286 - acc: 0.9582 - val_loss: 1.0352 - val_acc: 0.8085\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.1168 - acc: 0.9604 - val_loss: 1.0942 - val_acc: 0.7962\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.1138 - acc: 0.9594 - val_loss: 1.0815 - val_acc: 0.8062\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.1076 - acc: 0.9623 - val_loss: 1.0812 - val_acc: 0.8151\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1070 - acc: 0.9609 - val_loss: 1.1026 - val_acc: 0.8107\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.1009 - acc: 0.9608 - val_loss: 1.1466 - val_acc: 0.8040\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0998 - acc: 0.9625 - val_loss: 1.2221 - val_acc: 0.8040\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.1002 - acc: 0.9602 - val_loss: 1.1334 - val_acc: 0.8073\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0979 - acc: 0.9602 - val_loss: 1.1654 - val_acc: 0.8062\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 0.0920 - acc: 0.9621 - val_loss: 1.2225 - val_acc: 0.7973\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0933 - acc: 0.9625 - val_loss: 1.2150 - val_acc: 0.7984\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0943 - acc: 0.9608 - val_loss: 1.2133 - val_acc: 0.8073\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0915 - acc: 0.9624 - val_loss: 1.1837 - val_acc: 0.8107\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0924 - acc: 0.9597 - val_loss: 1.2435 - val_acc: 0.8018\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0899 - acc: 0.9609 - val_loss: 1.2672 - val_acc: 0.7996\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0876 - acc: 0.9610 - val_loss: 1.2291 - val_acc: 0.8040\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0883 - acc: 0.9628 - val_loss: 1.2725 - val_acc: 0.8007\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0876 - acc: 0.9621 - val_loss: 1.3081 - val_acc: 0.7929\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0898 - acc: 0.9600 - val_loss: 1.2522 - val_acc: 0.8040\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0834 - acc: 0.9620 - val_loss: 1.2675 - val_acc: 0.8029\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0854 - acc: 0.9618 - val_loss: 1.2800 - val_acc: 0.7984\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0840 - acc: 0.9621 - val_loss: 1.2941 - val_acc: 0.7984\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0827 - acc: 0.9639 - val_loss: 1.3317 - val_acc: 0.8007\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0869 - acc: 0.9600 - val_loss: 1.2975 - val_acc: 0.8007\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0843 - acc: 0.9621 - val_loss: 1.3300 - val_acc: 0.8040\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0831 - acc: 0.9612 - val_loss: 1.3173 - val_acc: 0.8007\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0803 - acc: 0.9624 - val_loss: 1.4113 - val_acc: 0.7940\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0857 - acc: 0.9599 - val_loss: 1.3393 - val_acc: 0.7929\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0804 - acc: 0.9621 - val_loss: 1.3883 - val_acc: 0.7895\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0805 - acc: 0.9614 - val_loss: 1.3173 - val_acc: 0.7973\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0839 - acc: 0.9609 - val_loss: 1.3349 - val_acc: 0.8051\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0785 - acc: 0.9614 - val_loss: 1.3660 - val_acc: 0.7962\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0794 - acc: 0.9626 - val_loss: 1.3635 - val_acc: 0.7973\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0800 - acc: 0.9615 - val_loss: 1.3522 - val_acc: 0.7929\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0796 - acc: 0.9599 - val_loss: 1.4239 - val_acc: 0.7884\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0775 - acc: 0.9608 - val_loss: 1.4146 - val_acc: 0.7884\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0813 - acc: 0.9621 - val_loss: 1.3687 - val_acc: 0.7940\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0774 - acc: 0.9610 - val_loss: 1.3864 - val_acc: 0.7951\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0761 - acc: 0.9619 - val_loss: 1.4110 - val_acc: 0.7984\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0773 - acc: 0.9635 - val_loss: 1.4154 - val_acc: 0.7929\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0739 - acc: 0.9643 - val_loss: 1.3888 - val_acc: 0.7918\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0749 - acc: 0.9615 - val_loss: 1.4546 - val_acc: 0.7906\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0774 - acc: 0.9599 - val_loss: 1.4009 - val_acc: 0.7973\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0732 - acc: 0.9628 - val_loss: 1.4358 - val_acc: 0.7973\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0761 - acc: 0.9609 - val_loss: 1.4332 - val_acc: 0.7951\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0724 - acc: 0.9641 - val_loss: 1.4505 - val_acc: 0.7895\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0744 - acc: 0.9615 - val_loss: 1.4887 - val_acc: 0.7873\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0743 - acc: 0.9597 - val_loss: 1.4490 - val_acc: 0.7929\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0744 - acc: 0.9615 - val_loss: 1.4109 - val_acc: 0.7929\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0716 - acc: 0.9608 - val_loss: 1.4288 - val_acc: 0.7873\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0712 - acc: 0.9617 - val_loss: 1.5088 - val_acc: 0.7873\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0724 - acc: 0.9619 - val_loss: 1.4408 - val_acc: 0.7929\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0696 - acc: 0.9628 - val_loss: 1.4616 - val_acc: 0.7884\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0705 - acc: 0.9633 - val_loss: 1.4822 - val_acc: 0.7817\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0707 - acc: 0.9623 - val_loss: 1.4823 - val_acc: 0.7862\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0721 - acc: 0.9625 - val_loss: 1.4679 - val_acc: 0.7829\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0708 - acc: 0.9628 - val_loss: 1.5011 - val_acc: 0.7873\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 92us/step - loss: 0.0692 - acc: 0.9614 - val_loss: 1.4811 - val_acc: 0.7884\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0698 - acc: 0.9605 - val_loss: 1.5352 - val_acc: 0.7817\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0688 - acc: 0.9612 - val_loss: 1.5160 - val_acc: 0.7817\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0686 - acc: 0.9619 - val_loss: 1.4843 - val_acc: 0.7862\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0697 - acc: 0.9599 - val_loss: 1.5476 - val_acc: 0.7817\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0675 - acc: 0.9604 - val_loss: 1.5620 - val_acc: 0.7806\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0678 - acc: 0.9621 - val_loss: 1.5274 - val_acc: 0.7851\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0674 - acc: 0.9628 - val_loss: 1.5559 - val_acc: 0.7873\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0664 - acc: 0.9624 - val_loss: 1.5632 - val_acc: 0.7817\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0668 - acc: 0.9607 - val_loss: 1.5598 - val_acc: 0.7773\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0669 - acc: 0.9617 - val_loss: 1.5359 - val_acc: 0.7862\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0651 - acc: 0.9638 - val_loss: 1.5954 - val_acc: 0.7695\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0657 - acc: 0.9626 - val_loss: 1.5658 - val_acc: 0.7751\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0674 - acc: 0.9613 - val_loss: 1.6022 - val_acc: 0.7862\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0643 - acc: 0.9633 - val_loss: 1.5762 - val_acc: 0.7840\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0636 - acc: 0.9618 - val_loss: 1.6305 - val_acc: 0.7739\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0659 - acc: 0.9603 - val_loss: 1.5909 - val_acc: 0.7773\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0654 - acc: 0.9621 - val_loss: 1.6025 - val_acc: 0.7840\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0638 - acc: 0.9623 - val_loss: 1.6434 - val_acc: 0.7784\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0639 - acc: 0.9613 - val_loss: 1.5951 - val_acc: 0.7806\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0650 - acc: 0.9603 - val_loss: 1.5876 - val_acc: 0.7851\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0625 - acc: 0.9630 - val_loss: 1.6555 - val_acc: 0.7728\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0626 - acc: 0.9619 - val_loss: 1.6663 - val_acc: 0.7773\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0655 - acc: 0.9625 - val_loss: 1.6534 - val_acc: 0.7806\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0630 - acc: 0.9609 - val_loss: 1.6864 - val_acc: 0.7739\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0627 - acc: 0.9626 - val_loss: 1.6687 - val_acc: 0.7695\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0634 - acc: 0.9617 - val_loss: 1.6588 - val_acc: 0.7773\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0628 - acc: 0.9588 - val_loss: 1.6806 - val_acc: 0.7762\n",
            "처리중인 폴드 # 8\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 9s 1ms/step - loss: 2.8887 - acc: 0.4874 - val_loss: 1.8404 - val_acc: 0.6459\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 79us/step - loss: 1.5182 - acc: 0.6979 - val_loss: 1.2680 - val_acc: 0.7327\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 1.0737 - acc: 0.7788 - val_loss: 1.0933 - val_acc: 0.7751\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.8391 - acc: 0.8272 - val_loss: 1.0118 - val_acc: 0.7895\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.6681 - acc: 0.8634 - val_loss: 0.9342 - val_acc: 0.8051\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.5332 - acc: 0.8961 - val_loss: 0.9035 - val_acc: 0.8051\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.4318 - acc: 0.9111 - val_loss: 0.8744 - val_acc: 0.8040\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.3542 - acc: 0.9253 - val_loss: 0.8634 - val_acc: 0.8085\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 0.2930 - acc: 0.9381 - val_loss: 0.8588 - val_acc: 0.8096\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.2495 - acc: 0.9432 - val_loss: 0.8645 - val_acc: 0.8163\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.2142 - acc: 0.9493 - val_loss: 0.9163 - val_acc: 0.8018\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1898 - acc: 0.9510 - val_loss: 0.8857 - val_acc: 0.8129\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1712 - acc: 0.9526 - val_loss: 0.9468 - val_acc: 0.7973\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.1536 - acc: 0.9562 - val_loss: 0.9380 - val_acc: 0.7996\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1473 - acc: 0.9552 - val_loss: 0.9394 - val_acc: 0.8040\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1342 - acc: 0.9552 - val_loss: 0.9473 - val_acc: 0.8051\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1252 - acc: 0.9586 - val_loss: 0.9948 - val_acc: 0.8096\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.1245 - acc: 0.9567 - val_loss: 1.0257 - val_acc: 0.7984\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1161 - acc: 0.9574 - val_loss: 1.0704 - val_acc: 0.7984\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1151 - acc: 0.9563 - val_loss: 1.0661 - val_acc: 0.7962\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1150 - acc: 0.9581 - val_loss: 1.1107 - val_acc: 0.7962\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1085 - acc: 0.9591 - val_loss: 1.0427 - val_acc: 0.8085\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1062 - acc: 0.9583 - val_loss: 1.0775 - val_acc: 0.8118\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.1008 - acc: 0.9604 - val_loss: 1.1172 - val_acc: 0.7996\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1042 - acc: 0.9597 - val_loss: 1.1059 - val_acc: 0.8018\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1010 - acc: 0.9574 - val_loss: 1.1392 - val_acc: 0.7996\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0994 - acc: 0.9589 - val_loss: 1.2470 - val_acc: 0.7851\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0964 - acc: 0.9594 - val_loss: 1.1382 - val_acc: 0.8029\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0976 - acc: 0.9598 - val_loss: 1.1802 - val_acc: 0.8018\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0974 - acc: 0.9573 - val_loss: 1.1611 - val_acc: 0.8107\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0951 - acc: 0.9594 - val_loss: 1.2478 - val_acc: 0.7918\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0923 - acc: 0.9579 - val_loss: 1.2450 - val_acc: 0.7984\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0908 - acc: 0.9624 - val_loss: 1.2037 - val_acc: 0.7984\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0962 - acc: 0.9587 - val_loss: 1.2356 - val_acc: 0.7895\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0908 - acc: 0.9591 - val_loss: 1.2273 - val_acc: 0.7962\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0890 - acc: 0.9597 - val_loss: 1.3227 - val_acc: 0.7895\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0916 - acc: 0.9584 - val_loss: 1.2386 - val_acc: 0.8018\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0920 - acc: 0.9598 - val_loss: 1.3163 - val_acc: 0.7840\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0908 - acc: 0.9586 - val_loss: 1.2493 - val_acc: 0.7962\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0860 - acc: 0.9600 - val_loss: 1.2875 - val_acc: 0.7951\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0924 - acc: 0.9572 - val_loss: 1.3240 - val_acc: 0.7929\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0902 - acc: 0.9586 - val_loss: 1.3961 - val_acc: 0.7873\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0883 - acc: 0.9582 - val_loss: 1.3598 - val_acc: 0.7884\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 91us/step - loss: 0.0848 - acc: 0.9612 - val_loss: 1.3592 - val_acc: 0.7895\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0877 - acc: 0.9593 - val_loss: 1.3251 - val_acc: 0.7929\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0866 - acc: 0.9591 - val_loss: 1.3342 - val_acc: 0.7984\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0852 - acc: 0.9617 - val_loss: 1.4071 - val_acc: 0.7873\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0835 - acc: 0.9600 - val_loss: 1.3445 - val_acc: 0.7862\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0832 - acc: 0.9598 - val_loss: 1.3879 - val_acc: 0.7817\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0826 - acc: 0.9582 - val_loss: 1.4459 - val_acc: 0.7840\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0848 - acc: 0.9584 - val_loss: 1.3706 - val_acc: 0.7940\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0839 - acc: 0.9598 - val_loss: 1.4389 - val_acc: 0.7751\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0813 - acc: 0.9597 - val_loss: 1.4228 - val_acc: 0.7895\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0831 - acc: 0.9589 - val_loss: 1.3928 - val_acc: 0.7795\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0809 - acc: 0.9582 - val_loss: 1.3832 - val_acc: 0.7873\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0804 - acc: 0.9594 - val_loss: 1.4848 - val_acc: 0.7684\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0817 - acc: 0.9584 - val_loss: 1.4374 - val_acc: 0.7873\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0807 - acc: 0.9591 - val_loss: 1.4358 - val_acc: 0.7829\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0803 - acc: 0.9595 - val_loss: 1.4447 - val_acc: 0.7806\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0787 - acc: 0.9598 - val_loss: 1.4966 - val_acc: 0.7762\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0782 - acc: 0.9595 - val_loss: 1.4905 - val_acc: 0.7739\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0779 - acc: 0.9599 - val_loss: 1.4805 - val_acc: 0.7795\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0790 - acc: 0.9577 - val_loss: 1.4651 - val_acc: 0.7873\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0781 - acc: 0.9574 - val_loss: 1.4355 - val_acc: 0.7784\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0753 - acc: 0.9613 - val_loss: 1.4745 - val_acc: 0.7829\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 94us/step - loss: 0.0771 - acc: 0.9594 - val_loss: 1.5384 - val_acc: 0.7684\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0774 - acc: 0.9578 - val_loss: 1.4819 - val_acc: 0.7739\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0778 - acc: 0.9603 - val_loss: 1.4896 - val_acc: 0.7762\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0769 - acc: 0.9591 - val_loss: 1.5534 - val_acc: 0.7784\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0768 - acc: 0.9579 - val_loss: 1.5422 - val_acc: 0.7751\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0738 - acc: 0.9594 - val_loss: 1.5155 - val_acc: 0.7795\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0736 - acc: 0.9598 - val_loss: 1.5566 - val_acc: 0.7829\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0736 - acc: 0.9588 - val_loss: 1.5943 - val_acc: 0.7739\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0740 - acc: 0.9612 - val_loss: 1.5416 - val_acc: 0.7817\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0739 - acc: 0.9593 - val_loss: 1.5346 - val_acc: 0.7817\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0732 - acc: 0.9599 - val_loss: 1.5357 - val_acc: 0.7773\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0736 - acc: 0.9602 - val_loss: 1.5552 - val_acc: 0.7840\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0732 - acc: 0.9588 - val_loss: 1.5654 - val_acc: 0.7873\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 93us/step - loss: 0.0723 - acc: 0.9587 - val_loss: 1.6065 - val_acc: 0.7706\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0721 - acc: 0.9599 - val_loss: 1.6110 - val_acc: 0.7773\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0711 - acc: 0.9593 - val_loss: 1.6254 - val_acc: 0.7706\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0713 - acc: 0.9591 - val_loss: 1.6650 - val_acc: 0.7717\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0699 - acc: 0.9605 - val_loss: 1.6316 - val_acc: 0.7717\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0712 - acc: 0.9604 - val_loss: 1.6324 - val_acc: 0.7773\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0702 - acc: 0.9612 - val_loss: 1.6503 - val_acc: 0.7684\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0705 - acc: 0.9599 - val_loss: 1.6179 - val_acc: 0.7773\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0708 - acc: 0.9595 - val_loss: 1.6531 - val_acc: 0.7695\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0704 - acc: 0.9588 - val_loss: 1.6113 - val_acc: 0.7840\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0686 - acc: 0.9592 - val_loss: 1.6715 - val_acc: 0.7795\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0682 - acc: 0.9635 - val_loss: 1.6167 - val_acc: 0.7817\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0697 - acc: 0.9584 - val_loss: 1.6749 - val_acc: 0.7684\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0686 - acc: 0.9592 - val_loss: 1.7025 - val_acc: 0.7817\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0682 - acc: 0.9593 - val_loss: 1.7581 - val_acc: 0.7739\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.0677 - acc: 0.9603 - val_loss: 1.6948 - val_acc: 0.7829\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0699 - acc: 0.9588 - val_loss: 1.7223 - val_acc: 0.7751\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0663 - acc: 0.9608 - val_loss: 1.7677 - val_acc: 0.7595\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0697 - acc: 0.9582 - val_loss: 1.7183 - val_acc: 0.7773\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0680 - acc: 0.9593 - val_loss: 1.7169 - val_acc: 0.7684\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0680 - acc: 0.9587 - val_loss: 1.7745 - val_acc: 0.7673\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0670 - acc: 0.9598 - val_loss: 1.7557 - val_acc: 0.7706\n",
            "처리중인 폴드 # 9\n",
            "Train on 8084 samples, validate on 898 samples\n",
            "Epoch 1/100\n",
            "8084/8084 [==============================] - 9s 1ms/step - loss: 2.5940 - acc: 0.5445 - val_loss: 1.7926 - val_acc: 0.6481\n",
            "Epoch 2/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 1.3963 - acc: 0.7098 - val_loss: 1.3934 - val_acc: 0.7071\n",
            "Epoch 3/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 1.0479 - acc: 0.7736 - val_loss: 1.2284 - val_acc: 0.7350\n",
            "Epoch 4/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.8204 - acc: 0.8230 - val_loss: 1.1426 - val_acc: 0.7506\n",
            "Epoch 5/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.6512 - acc: 0.8659 - val_loss: 1.0902 - val_acc: 0.7717\n",
            "Epoch 6/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.5199 - acc: 0.8939 - val_loss: 1.0335 - val_acc: 0.7840\n",
            "Epoch 7/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.4191 - acc: 0.9151 - val_loss: 1.0235 - val_acc: 0.7817\n",
            "Epoch 8/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.3428 - acc: 0.9283 - val_loss: 0.9891 - val_acc: 0.8029\n",
            "Epoch 9/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.2823 - acc: 0.9381 - val_loss: 1.0196 - val_acc: 0.7951\n",
            "Epoch 10/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.2393 - acc: 0.9448 - val_loss: 1.0047 - val_acc: 0.7906\n",
            "Epoch 11/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.2033 - acc: 0.9515 - val_loss: 1.0130 - val_acc: 0.7984\n",
            "Epoch 12/100\n",
            "8084/8084 [==============================] - 1s 80us/step - loss: 0.1838 - acc: 0.9552 - val_loss: 1.0141 - val_acc: 0.7906\n",
            "Epoch 13/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1642 - acc: 0.9544 - val_loss: 1.0141 - val_acc: 0.7895\n",
            "Epoch 14/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.1474 - acc: 0.9558 - val_loss: 1.0546 - val_acc: 0.7929\n",
            "Epoch 15/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1384 - acc: 0.9553 - val_loss: 1.1699 - val_acc: 0.7684\n",
            "Epoch 16/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.1306 - acc: 0.9588 - val_loss: 1.1718 - val_acc: 0.7717\n",
            "Epoch 17/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1209 - acc: 0.9574 - val_loss: 1.0989 - val_acc: 0.7929\n",
            "Epoch 18/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.1159 - acc: 0.9599 - val_loss: 1.2027 - val_acc: 0.7773\n",
            "Epoch 19/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1113 - acc: 0.9577 - val_loss: 1.1544 - val_acc: 0.7751\n",
            "Epoch 20/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.1083 - acc: 0.9603 - val_loss: 1.2991 - val_acc: 0.7617\n",
            "Epoch 21/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.1080 - acc: 0.9579 - val_loss: 1.1705 - val_acc: 0.7895\n",
            "Epoch 22/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1067 - acc: 0.9587 - val_loss: 1.1990 - val_acc: 0.7829\n",
            "Epoch 23/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.1011 - acc: 0.9592 - val_loss: 1.2316 - val_acc: 0.7873\n",
            "Epoch 24/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.1010 - acc: 0.9587 - val_loss: 1.2565 - val_acc: 0.7884\n",
            "Epoch 25/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0976 - acc: 0.9589 - val_loss: 1.2698 - val_acc: 0.7773\n",
            "Epoch 26/100\n",
            "8084/8084 [==============================] - 1s 92us/step - loss: 0.0974 - acc: 0.9579 - val_loss: 1.2701 - val_acc: 0.7784\n",
            "Epoch 27/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0975 - acc: 0.9597 - val_loss: 1.3938 - val_acc: 0.7606\n",
            "Epoch 28/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0941 - acc: 0.9593 - val_loss: 1.2995 - val_acc: 0.7762\n",
            "Epoch 29/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0952 - acc: 0.9603 - val_loss: 1.3641 - val_acc: 0.7661\n",
            "Epoch 30/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0917 - acc: 0.9610 - val_loss: 1.3712 - val_acc: 0.7739\n",
            "Epoch 31/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0953 - acc: 0.9594 - val_loss: 1.3240 - val_acc: 0.7784\n",
            "Epoch 32/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0906 - acc: 0.9608 - val_loss: 1.4085 - val_acc: 0.7695\n",
            "Epoch 33/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0908 - acc: 0.9593 - val_loss: 1.3465 - val_acc: 0.7751\n",
            "Epoch 34/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0888 - acc: 0.9613 - val_loss: 1.4169 - val_acc: 0.7717\n",
            "Epoch 35/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0896 - acc: 0.9589 - val_loss: 1.3963 - val_acc: 0.7717\n",
            "Epoch 36/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0886 - acc: 0.9586 - val_loss: 1.4328 - val_acc: 0.7728\n",
            "Epoch 37/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0874 - acc: 0.9609 - val_loss: 1.5163 - val_acc: 0.7584\n",
            "Epoch 38/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0908 - acc: 0.9582 - val_loss: 1.4472 - val_acc: 0.7728\n",
            "Epoch 39/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0857 - acc: 0.9594 - val_loss: 1.4659 - val_acc: 0.7684\n",
            "Epoch 40/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0842 - acc: 0.9618 - val_loss: 1.4124 - val_acc: 0.7773\n",
            "Epoch 41/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0844 - acc: 0.9618 - val_loss: 1.4933 - val_acc: 0.7661\n",
            "Epoch 42/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0823 - acc: 0.9608 - val_loss: 1.4114 - val_acc: 0.7728\n",
            "Epoch 43/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0849 - acc: 0.9608 - val_loss: 1.5348 - val_acc: 0.7528\n",
            "Epoch 44/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0820 - acc: 0.9614 - val_loss: 1.4815 - val_acc: 0.7728\n",
            "Epoch 45/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0821 - acc: 0.9604 - val_loss: 1.5114 - val_acc: 0.7684\n",
            "Epoch 46/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0838 - acc: 0.9607 - val_loss: 1.4766 - val_acc: 0.7673\n",
            "Epoch 47/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0823 - acc: 0.9583 - val_loss: 1.5102 - val_acc: 0.7572\n",
            "Epoch 48/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0787 - acc: 0.9594 - val_loss: 1.5040 - val_acc: 0.7628\n",
            "Epoch 49/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0811 - acc: 0.9589 - val_loss: 1.5334 - val_acc: 0.7561\n",
            "Epoch 50/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0794 - acc: 0.9608 - val_loss: 1.4903 - val_acc: 0.7706\n",
            "Epoch 51/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0809 - acc: 0.9594 - val_loss: 1.5065 - val_acc: 0.7739\n",
            "Epoch 52/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0805 - acc: 0.9593 - val_loss: 1.5248 - val_acc: 0.7650\n",
            "Epoch 53/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0790 - acc: 0.9617 - val_loss: 1.5444 - val_acc: 0.7639\n",
            "Epoch 54/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0797 - acc: 0.9595 - val_loss: 1.5203 - val_acc: 0.7706\n",
            "Epoch 55/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0784 - acc: 0.9583 - val_loss: 1.6547 - val_acc: 0.7506\n",
            "Epoch 56/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0765 - acc: 0.9597 - val_loss: 1.6120 - val_acc: 0.7572\n",
            "Epoch 57/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0768 - acc: 0.9620 - val_loss: 1.5174 - val_acc: 0.7661\n",
            "Epoch 58/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0772 - acc: 0.9605 - val_loss: 1.6622 - val_acc: 0.7472\n",
            "Epoch 59/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0787 - acc: 0.9574 - val_loss: 1.5989 - val_acc: 0.7606\n",
            "Epoch 60/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0778 - acc: 0.9591 - val_loss: 1.5955 - val_acc: 0.7673\n",
            "Epoch 61/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0744 - acc: 0.9603 - val_loss: 1.6217 - val_acc: 0.7584\n",
            "Epoch 62/100\n",
            "8084/8084 [==============================] - 1s 81us/step - loss: 0.0756 - acc: 0.9607 - val_loss: 1.6608 - val_acc: 0.7472\n",
            "Epoch 63/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0748 - acc: 0.9618 - val_loss: 1.6365 - val_acc: 0.7517\n",
            "Epoch 64/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0752 - acc: 0.9598 - val_loss: 1.6430 - val_acc: 0.7539\n",
            "Epoch 65/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0720 - acc: 0.9630 - val_loss: 1.6250 - val_acc: 0.7650\n",
            "Epoch 66/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0744 - acc: 0.9614 - val_loss: 1.7353 - val_acc: 0.7506\n",
            "Epoch 67/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0720 - acc: 0.9610 - val_loss: 1.7581 - val_acc: 0.7472\n",
            "Epoch 68/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0722 - acc: 0.9619 - val_loss: 1.7044 - val_acc: 0.7461\n",
            "Epoch 69/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0718 - acc: 0.9614 - val_loss: 1.6631 - val_acc: 0.7539\n",
            "Epoch 70/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0741 - acc: 0.9591 - val_loss: 1.6173 - val_acc: 0.7572\n",
            "Epoch 71/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0714 - acc: 0.9615 - val_loss: 1.7190 - val_acc: 0.7416\n",
            "Epoch 72/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0709 - acc: 0.9602 - val_loss: 1.6652 - val_acc: 0.7494\n",
            "Epoch 73/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0718 - acc: 0.9614 - val_loss: 1.6684 - val_acc: 0.7617\n",
            "Epoch 74/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0718 - acc: 0.9614 - val_loss: 1.7867 - val_acc: 0.7483\n",
            "Epoch 75/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0720 - acc: 0.9600 - val_loss: 1.7193 - val_acc: 0.7572\n",
            "Epoch 76/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0707 - acc: 0.9592 - val_loss: 1.7325 - val_acc: 0.7461\n",
            "Epoch 77/100\n",
            "8084/8084 [==============================] - 1s 88us/step - loss: 0.0698 - acc: 0.9612 - val_loss: 1.7052 - val_acc: 0.7494\n",
            "Epoch 78/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0695 - acc: 0.9620 - val_loss: 1.7587 - val_acc: 0.7517\n",
            "Epoch 79/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0684 - acc: 0.9633 - val_loss: 1.7794 - val_acc: 0.7483\n",
            "Epoch 80/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0695 - acc: 0.9609 - val_loss: 1.7438 - val_acc: 0.7472\n",
            "Epoch 81/100\n",
            "8084/8084 [==============================] - 1s 89us/step - loss: 0.0687 - acc: 0.9588 - val_loss: 1.7918 - val_acc: 0.7461\n",
            "Epoch 82/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0686 - acc: 0.9602 - val_loss: 1.7728 - val_acc: 0.7517\n",
            "Epoch 83/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0686 - acc: 0.9603 - val_loss: 1.8558 - val_acc: 0.7428\n",
            "Epoch 84/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0672 - acc: 0.9617 - val_loss: 1.8549 - val_acc: 0.7428\n",
            "Epoch 85/100\n",
            "8084/8084 [==============================] - 1s 90us/step - loss: 0.0691 - acc: 0.9591 - val_loss: 1.8192 - val_acc: 0.7461\n",
            "Epoch 86/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0666 - acc: 0.9607 - val_loss: 1.7839 - val_acc: 0.7483\n",
            "Epoch 87/100\n",
            "8084/8084 [==============================] - 1s 84us/step - loss: 0.0679 - acc: 0.9591 - val_loss: 1.8870 - val_acc: 0.7283\n",
            "Epoch 88/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0668 - acc: 0.9613 - val_loss: 1.8137 - val_acc: 0.7506\n",
            "Epoch 89/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0664 - acc: 0.9621 - val_loss: 1.8330 - val_acc: 0.7439\n",
            "Epoch 90/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0659 - acc: 0.9621 - val_loss: 1.8523 - val_acc: 0.7450\n",
            "Epoch 91/100\n",
            "8084/8084 [==============================] - 1s 86us/step - loss: 0.0664 - acc: 0.9599 - val_loss: 1.8450 - val_acc: 0.7539\n",
            "Epoch 92/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0664 - acc: 0.9608 - val_loss: 1.9037 - val_acc: 0.7327\n",
            "Epoch 93/100\n",
            "8084/8084 [==============================] - 1s 87us/step - loss: 0.0648 - acc: 0.9605 - val_loss: 1.9306 - val_acc: 0.7428\n",
            "Epoch 94/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0659 - acc: 0.9605 - val_loss: 1.9194 - val_acc: 0.7327\n",
            "Epoch 95/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0655 - acc: 0.9605 - val_loss: 1.9177 - val_acc: 0.7283\n",
            "Epoch 96/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0657 - acc: 0.9574 - val_loss: 1.9265 - val_acc: 0.7350\n",
            "Epoch 97/100\n",
            "8084/8084 [==============================] - 1s 82us/step - loss: 0.0643 - acc: 0.9613 - val_loss: 1.8733 - val_acc: 0.7472\n",
            "Epoch 98/100\n",
            "8084/8084 [==============================] - 1s 85us/step - loss: 0.0659 - acc: 0.9587 - val_loss: 1.9060 - val_acc: 0.7428\n",
            "Epoch 99/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0648 - acc: 0.9593 - val_loss: 1.9236 - val_acc: 0.7461\n",
            "Epoch 100/100\n",
            "8084/8084 [==============================] - 1s 83us/step - loss: 0.0641 - acc: 0.9598 - val_loss: 1.9334 - val_acc: 0.7405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X8908muCWdnG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Plotting the training and validation accuracy\n",
        "  * To obtain the validation accuracy at the end of every epoch, just average the performances of all folds."
      ]
    },
    {
      "metadata": {
        "id": "FagMBVf8pWyo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6PJb3RyXXZlC",
        "colab_type": "code",
        "outputId": "2403df0c-4cf4-4744-8641-f590c04aabde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# write a code for plotting the training and validation accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 모든 폴드의 val_acc 평균\n",
        "average_val_acc = [\n",
        "    np.mean([x[i] for x in all_val_acc]) for i in range(num_epochs)]\n",
        "#모든 폴드의 train_acc 평균\n",
        "average_train_acc = [\n",
        "    np.mean([x[i] for x in all_train_acc]) for i in range(num_epochs)]\n",
        "\n",
        "plt.plot(range(1, len(average_train_acc) + 1), average_train_acc, 'bo', label = 'Average Training acc')\n",
        "plt.plot(range(1, len(average_val_acc) + 1), average_val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation Average accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.legend()\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAjy1gzsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "pS0D5_0iXd_y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> ### Inference"
      ]
    },
    {
      "metadata": {
        "id": "r9HNF524XkCy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Find the best performance model by seeing the performance plot.\n",
        "* Calculate the accuracy on test set using the best performance model.\n",
        "  * Here, you should use a majority voting method to get the prediction for a test data point.\n",
        "  * Specifically, given a test data point, get the predicted class from the trained model on each fold, and then decide the final predicted class by majority voting.\n",
        "* **Do not retrain the model.**"
      ]
    },
    {
      "metadata": {
        "id": "-AnZ8nIiq9kC",
        "colab_type": "code",
        "outputId": "85451077-a78b-45bf-95f4-caf552784b6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "cell_type": "code",
      "source": [
        "#  그래프를 좀더 보기 쉽게 만들기 위해서 수업시간에 배운 smooth_curve를 사용한다.\n",
        "def smooth_curve(points, factor=0.9):\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous = smoothed_points[-1]\n",
        "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points\n",
        "\n",
        "smooth_average_train_acc = smooth_curve(average_train_acc[5:])\n",
        "smooth_average_val_acc = smooth_curve(average_val_acc[5:])\n",
        "plt.plot(range(1, len(smooth_average_train_acc) + 1), smooth_average_train_acc, 'bo', label = 'Average Training acc')\n",
        "plt.plot(range(1, len(smooth_average_val_acc) + 1), smooth_average_val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation Average accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.legend()\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFnCAYAAAChL+DqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "6HF3iD-sw3lv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* 그래프의 결과를 보면 10번째 epoch 이후로 과대 적합이 시작 된다. 10번째 epoch 모델을 불러 와서 test 데이터의 정확도를 계산한다."
      ]
    },
    {
      "metadata": {
        "id": "bhDWsnWuY5w-",
        "colab_type": "code",
        "outputId": "1cc66db1-450c-4074-e495-aa0f32d7c193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# write a code here\n",
        "best_model_path = ['/content/gdrive/My Drive/hw2_callbacks/hw2_model.0.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.1.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.2.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.3.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.4.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.5.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.6.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.7.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.8.10.hdf5',\n",
        "                   '/content/gdrive/My Drive/hw2_callbacks/hw2_model.9.10.hdf5']\n",
        "best_model_prediction_all_add = np.zeros(one_hot_test_labels.shape)\n",
        "\n",
        "for i in range(len(best_model_path)):\n",
        "  best_model_i = models.load_model(filepath=best_model_path[i])\n",
        "  best_model_prediction_all_add += best_model_i.predict(x_test)\n",
        "  \n",
        "  \n",
        "\n",
        "best_model_prediction_all_add_argmax = np.array(test_labels)\n",
        "correct = 0\n",
        "for i in range(len(test_labels)):\n",
        "  best_model_prediction_all_add_argmax[i] = np.argmax(best_model_prediction_all_add[i])\n",
        "  if test_labels[i] == best_model_prediction_all_add_argmax[i]:\n",
        "    correct += 1\n",
        "    \n",
        "accuracy = float(correct) / len(test_labels)\n",
        "\n",
        "print(\"Accuracy :\" + str(accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :0.802760463045414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "przPkLp4KzE_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###explain\n",
        "* 10가지의 폴드에 있는 모델들의 predict() 값을 모두 더하고 그 중 가장 큰 확률을 가진 인덱스를 가져오는 방법으로 Majority Voting을 구현하였다.\n",
        "* test_labels의 인덱스 값과 10가지 폴드의 모델의 결과값을 Majority Voting하여 얻은 결과값을 비교하여 일치하면  correct값을 1 더하고 correct/총갯수를 통해 Accuracy를 구현하였다."
      ]
    }
  ]
}